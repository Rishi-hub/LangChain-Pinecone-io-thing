{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp-hXFhhyWve"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjfYQBcyWve"
      },
      "source": [
        "#### [LangChain Handbook](https://www.pinecone.io/learn/series/langchain/)\n",
        "\n",
        "# Conversational Memory with LCEL\n",
        "\n",
        "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
        "\n",
        "The memory allows an _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
        "\n",
        "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
        "\n",
        "In this notebook we'll explore conversational memory using modern LangChain Expression Language (LCEL) and the recommended `RunnableWithMessageHistory` class.\n",
        "\n",
        "We'll start by importing all of the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETg8fr8-yWvf",
        "outputId": "af6d2f99-b18a-473e-84f3-b513e8f945f0"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-community==0.3.25 \\\n",
        "  langchain-openai==0.3.22 \\\n",
        "  tiktoken==0.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-pinecone-io-walkthrough-conversational-memory\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSvjQpbKyWvf"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key if prompted, otherwise it will use the `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnquGYaQyWvf",
        "outputId": "273a42f7-25c3-4a7e-ca15-3e4798580a19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "    or getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wFhsehZEyWvf"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=1.0,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-5-mini'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPQgxde4yWvf"
      },
      "source": [
        "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YG0RXg5PyWvf"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(pipeline, query, config=None):\n",
        "    with get_openai_callback() as cb:\n",
        "        # Handle both dict and string inputs\n",
        "        if isinstance(query, str):\n",
        "            query = {\"query\": query}\n",
        "\n",
        "        # Use provided config `or default\n",
        "        if config is None:\n",
        "            config = {\"configurable\": {\"session_id\": \"default\"}}\n",
        "\n",
        "        result = pipeline.invoke(query, config=config)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPk7c5IgyWvf"
      },
      "source": [
        "Now let's dive into **Conversational Memory** using LCEL.\n",
        "\n",
        "## What is memory?\n",
        "\n",
        "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
        "\n",
        "The official definition of memory is the following:\n",
        "\n",
        "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of \"Memory\" exists to do exactly that.\n",
        "\n",
        "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZgnUOmSyWvf"
      },
      "source": [
        "## Building Conversational Chains with LCEL\n",
        "\n",
        "Before we delve into the different memory types, let's understand how to build conversational chains using LCEL. The key components are:\n",
        "\n",
        "1. **Prompt Template** - Defines the conversation structure with placeholders for history and input\n",
        "2. **LLM** - The language model that generates responses\n",
        "3. **Output Parser** - Converts the LLM output to the desired format (optional)\n",
        "4. **RunnableWithMessageHistory** - Manages conversation history\n",
        "\n",
        "Let's create our base conversational chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEpNx9oNyWvg",
        "outputId": "22d444b7-4747-4cb9-ea14-57e8bbe310ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Define the prompt template\n",
        "system_prompt = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])\n",
        "\n",
        "# Create the LCEL pipeline\n",
        "output_parser = StrOutputParser()\n",
        "pipeline = prompt_template | llm | output_parser\n",
        "\n",
        "# Let's examine the prompt template\n",
        "print(prompt_template.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC_03lJsyWvg"
      },
      "source": [
        "## Memory types\n",
        "\n",
        "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlqkyPbEyWvg"
      },
      "source": [
        "### Memory Type #1: Buffer Memory - Store the Entire Chat History\n",
        "\n",
        "`InMemoryChatMessageHistory` and `RunnableWithMessageHistory` are used as alternatives to `ConversationBufferMemory` as they are:\n",
        "- More flexible and configurable.\n",
        "- Integrate better with LCEL.\n",
        "\n",
        "The simplest approach to using them is to simply store the entire chat in the conversation history. Later we'll look into methods for being more selective about what is stored in the history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xqd2vwxAyWvg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Create a simple chat history storage\n",
        "chat_map = {}\n",
        "\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6LfvNhtyWvg"
      },
      "source": [
        "Let's see this in action by having a conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVSp3SZGyWvg",
        "outputId": "85a8a4ac-8ef8-4579-a98f-ce152fab1b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good morning! I’m an AI assistant — ready to help with questions, writing, code, planning, brainstorming, summaries, translations, and more. What would you like to do today?\n"
          ]
        }
      ],
      "source": [
        "# Create the conversational chain with message history\n",
        "conversation_buf = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "\n",
        "# First message\n",
        "result = conversation_buf.invoke(\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbttULnpyWvg"
      },
      "source": [
        "This call used some tokens, but we can't see that from the above.\n",
        "\n",
        "If we'd like to count the number of tokens being used we just pass our conversation `RunnableWithMessageHistory` instance and the message we'd like to input to the `count_tokens` function we defined earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53cFq6udyWvg",
        "outputId": "34862d9b-64c2-4650-e9e7-9ab25421e711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1930 tokens\n",
            "\n",
            "Response: Nice — this is a great area to explore. I’ll remember the code PINECONE_RULEZ_01 for the remainder of this chat (I can’t persist it across separate sessions unless you store it somewhere permanent). Would you like me to use that code in examples or to tag specific items in our discussion?\n",
            "\n",
            "Below are practical options, architecture patterns, and concrete considerations for integrating LLMs with external knowledge.\n",
            "\n",
            "High-level patterns\n",
            "- Retrieval-Augmented Generation (RAG): store external documents as embeddings in a vector DB (Pinecone, FAISS, Milvus). At query time, embed the user query, retrieve nearest chunks, then condition the LLM on those retrieved passages to produce grounded answers.\n",
            "- Hybrid retrieval: combine traditional lexical search (BM25) with vector search to handle exact-match queries and semantic ones.\n",
            "- Tooling / API chaining: have the LLM call external tools (search APIs, databases, calculators) for tasks requiring up-to-date facts or non-language computations; the LLM forms the query, then ingests the tool result to produce final output.\n",
            "- Knowledge graphs & symbolic stores: use KG for structured queries, constraints, and reasoning; combine KG lookups with LLM natural-language generation for explainable responses.\n",
            "- Fine-tuning / adapters: when you have a domain corpus, you can fine-tune or use lightweight adapters to bias the LLM for domain style—keep RAG for factual grounding to avoid hallucinations.\n",
            "\n",
            "Core components of a RAG pipeline\n",
            "1. Ingestion & preprocessing\n",
            "   - Split documents into chunks (size 500–1,500 tokens depending on model & embedding costs).\n",
            "   - Clean, normalize, and add metadata (title, source, timestamp, type).\n",
            "2. Embedding\n",
            "   - Choose an embedding model appropriate for your language and domain.\n",
            "   - Store embeddings and metadata in a vector DB (Pinecone, FAISS).\n",
            "3. Retrieval\n",
            "   - Query embedding for the user prompt, apply metadata filters if needed (e.g., language, date).\n",
            "   - Use cosine or dot-product similarity; consider re-ranking top-K using a cross-encoder or BM25.\n",
            "4. Prompting the LLM\n",
            "   - Provide the LLM: user question + retrieved passages + instructions to cite sources and limit hallucination.\n",
            "   - Use templates that constrain length, specify citation format, and set role/temperature.\n",
            "5. Post-processing & provenance\n",
            "   - Return extracted sources, confidence score, and possibly a short evidence snippet.\n",
            "   - Track provenance (doc id, chunk id, timestamp) so user can verify claims.\n",
            "\n",
            "Practical techniques to reduce hallucination\n",
            "- Include exact source snippets in the prompt and ask the model to paraphrase only what’s supported.\n",
            "- Force the model to produce a “Sources” section with links/IDs.\n",
            "- Use a verification step: after generation, re-query the retrieved documents or an external fact-checker to validate claims.\n",
            "- Use conservative decoding (lower temperature) or use deterministic models for factual outputs.\n",
            "\n",
            "Scaling, latency, and cost tradeoffs\n",
            "- Cache recent queries & embeddings to lower latency.\n",
            "- Use smaller embedding models for frequent queries and larger models for critical retrieval.\n",
            "- Limit the number of retrieved passages (e.g., top-5) to control prompt size and cost.\n",
            "- Consider asynchronous retrieval + streaming generation for responsive UIs.\n",
            "\n",
            "Security, privacy, and governance\n",
            "- Encrypt data at rest and in transit.\n",
            "- Implement access control and audit logs for data usage and model queries.\n",
            "- Mask or avoid storing PII in embeddings; if you must, apply hashing or differential privacy techniques.\n",
            "- Keep retention policies and allow users to delete their data.\n",
            "\n",
            "Evaluation & metrics\n",
            "- Relevance (Precision@k, Recall@k)\n",
            "- Answer accuracy (human evaluation, automated fact-checkers)\n",
            "- Hallucination rate\n",
            "- Latency & cost per query\n",
            "- User satisfaction / task success rate\n",
            "\n",
            "Concrete minimal RAG pseudocode (conceptual)\n",
            "- Ingest docs -> chunk -> compute embeddings -> upsert to Pinecone with metadata\n",
            "- On user query:\n",
            "  1. q_emb = embed(query)\n",
            "  2. matches = pinecone.query(q_emb, top_k=5, filter=metadata_filters)\n",
            "  3. context = concat(matches.snippets)\n",
            "  4. prompt = build_prompt(query, context, instructions)\n",
            "  5. answer = LLM.generate(prompt)\n",
            "  6. return answer + matches.metadata\n",
            "\n",
            "Where your PINECONE_RULEZ_01 code could be used\n",
            "- As a session tag or label in metadata for documents you want associated with a particular experiment (I can reference it during this chat).\n",
            "- As a test string to verify ingestion & retrieval (e.g., index a doc containing that code and show that retrieval returns it).\n",
            "\n",
            "Next steps I can help with\n",
            "- Design a concrete architecture tailored to your data volume & latency needs.\n",
            "- Provide runnable example code (Python) using your chosen embedding model + Pinecone + an LLM.\n",
            "- Create prompt templates and evaluation scripts.\n",
            "- Prototype a small demo pipeline and test cases using PINECONE_RULEZ_01.\n",
            "\n",
            "Which of these would you like to do next, and should I use PINECONE_RULEZ_01 in examples or indexing demos in this session?\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation with token counting\n",
        "query = \"\"\"\n",
        "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\n",
        "\n",
        "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
        "\"\"\"\n",
        "\n",
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": query},\n",
        "    # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUspQf-IyWvg",
        "outputId": "7a7abd14-611a-4019-f7c7-2fbe17ebb6d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 4398 tokens\n",
            "\n",
            "Response: Great — below is a broad, structured analysis of the main possibilities for integrating Large Language Models with external knowledge. I’ll give each option a plain description, core mechanics, concrete pros/cons, common parameter choices, best-fit use cases, and practical considerations (tools, costs, evaluation). If you want to deep-dive any single option I can produce architecture diagrams, code, or an experiment plan.\n",
            "\n",
            "Summary / decision tree (quick)\n",
            "- Need recency/grounding + low hallucination: Retrieval-Augmented Generation (RAG) + reranker + verification.\n",
            "- Need structured queries and explainability: Knowledge Graph / database + LLM for natural language.\n",
            "- Need action-taking and live data: Tool-enabled Agents (APIs, search, calculators).\n",
            "- Need long-term personalization / memory: Memory-augmented LLMs (vector DB + lifecycle + user profiles).\n",
            "- High throughput & low cost: Lightweight embeddings + hybrid retrieval + caching + smaller LLMs / distilled models.\n",
            "\n",
            "1) Retrieval-Augmented Generation (RAG)\n",
            "- What it is: Use an embedding model + vector DB to fetch relevant passages/documents at query time; concatenate or otherwise provide retrieved context to the LLM to produce grounded outputs.\n",
            "- Core flow: chunk -> embed -> store -> on query embed -> retrieve top-K -> optionally rerank -> build prompt -> LLM answer.\n",
            "- Typical parameters:\n",
            "  - Chunk size: 500–1500 tokens (domain-specific; longer when context coherence matters).\n",
            "  - Top-K: 3–10 (start with 5).\n",
            "  - Embedding model: OpenAI text-embedding-3 or similar; dimension 1536+ / domain-tuned.\n",
            "  - Reranker: cross-encoder (e.g., miniLM cross-encoder or a dedicated ensembler) for top 20 -> rerank top 5.\n",
            "- Pros:\n",
            "  - Grounded answers; reduces hallucinations if prompts ask to cite sources.\n",
            "  - Scales to large corpora; fast retrieval.\n",
            "  - Easy to update knowledge (upsert docs).\n",
            "- Cons:\n",
            "  - Prompt length limits; cost per query (LLM tokens + retrieval ops).\n",
            "  - Requires engineering for chunking, filters, versioning.\n",
            "- Best for: customer support, documentation assistants, legal/medical reference (with verification), research assistants.\n",
            "- Tools: Pinecone, FAISS, Milvus, Weaviate; LangChain, LlamaIndex, Haystack for orchestration.\n",
            "\n",
            "2) Hybrid Retrieval (sparse + dense)\n",
            "- What it is: Combine lexical search (BM25, Elasticsearch) with dense vector search to capture both exact matches and semantic similarity.\n",
            "- Approach: run both searches, merge/rerank results, or use lexical for initial filter then dense rerank.\n",
            "- Pros:\n",
            "  - Better recall for exact query matches (IDs, names) while preserving semantic matches.\n",
            "- Cons:\n",
            "  - More complex stack; requires tuning merging logic.\n",
            "- Best for: enterprise search, code search, domain with many named entities.\n",
            "\n",
            "3) Knowledge Graphs / Symbolic Stores + LLMs\n",
            "- What it is: Store structured facts in a KG or relational DB; LLM translates natural-language requests into structured queries (SPARQL/SQL), returns structured results, then LLM generates human-facing text.\n",
            "- Pros:\n",
            "  - Precise, auditable answers; easy enforcement of constraints; excellent for logical reasoning.\n",
            "- Cons:\n",
            "  - Hard to encode nuanced/long-text knowledge; building/maintaining KG is labor-intensive.\n",
            "- Best for: product catalogs, inventory, clinical decision support (where rules matter), compliance checks.\n",
            "- Tools: Neo4j, AWS Neptune, GraphQL front-ends, SQL + LLM query builders.\n",
            "\n",
            "4) Tool/Plugin-enabled Agents\n",
            "- What it is: LLMs are orchestrators that call external tools (search engine, calculators, calendar, internal APIs) and incorporate tool outputs into answers.\n",
            "- Architectures: Single-step tool call vs multi-step planning agent (ReAct, tree of thoughts, planner-executor).\n",
            "- Pros:\n",
            "  - Access live data and actions; solves tasks beyond text generation.\n",
            "- Cons:\n",
            "  - Complex error handling, safety controls, tool interface design.\n",
            "- Best for: automation assistants, actionable agents (book, buy, fetch data), data pipelines.\n",
            "- Tools: OpenAI function calling, LangChain tools, Microsoft Copilot plugins.\n",
            "\n",
            "5) Fine-tuning / Adapters / Retrieval + Fine-tuning combo\n",
            "- What it is: Fine-tune LLMs on domain-specific corpora (or use LoRA/adapters) and keep RAG for factual grounding.\n",
            "- Pros:\n",
            "  - Better stylistic alignment; faster inference if using smaller tuned model.\n",
            "- Cons:\n",
            "  - Risk of embedding incorrect facts; needs retraining for new facts (RAG mitigates).\n",
            "- Best for: brand voice, specialized writing style, technical domain language.\n",
            "\n",
            "6) Long-term Memory & Personalization\n",
            "- What it is: Maintain per-user vectors, preferences, and interaction history in a vector DB and use them as retrieval filters / conditioning context.\n",
            "- Implementation notes:\n",
            "  - Distinguish ephemeral session memory vs persistent memory.\n",
            "  - Use TTLs and user-consent controls for PII.\n",
            "- Pros:\n",
            "  - Personalized answers and continuity across sessions.\n",
            "- Cons:\n",
            "  - Privacy/consent challenges; storage growth.\n",
            "- Best for: personal assistants, sales CRMs, tutoring systems.\n",
            "\n",
            "7) Generative Retrieval / Learned Retrieval\n",
            "- What it is: Use models that directly generate supporting passages or document IDs (e.g., DPR, GenRead) rather than nearest neighbor lookup.\n",
            "- Pros:\n",
            "  - Can be more robust in sparse-doc corpora.\n",
            "- Cons:\n",
            "  - Harder to debug and can hallucinate results if not carefully validated.\n",
            "\n",
            "8) On-demand Web / Search Integration\n",
            "- What it is: Query live web search (Bing, Google, internal site) for fresh results, then use RAG or tool flows.\n",
            "- Pros:\n",
            "  - Freshness and broad coverage.\n",
            "- Cons:\n",
            "  - Noisy sources; need filtering and provenance tracking.\n",
            "\n",
            "9) Multimodal Knowledge Integration\n",
            "- What it is: Index non-text (images, audio, video) by generating embeddings from modality-specific encoders, then retrieve and pass visual/textual snippets to multimodal LLMs.\n",
            "- Use cases: product visual search, video Q&A, multimodal documentation.\n",
            "- Tools: CLIP, OpenAI vision models, multimodal LLMs.\n",
            "\n",
            "Cross-cutting technical considerations (concrete)\n",
            "- Chunking strategies:\n",
            "  - Overlap chunks by ~20–30% to preserve context across boundaries.\n",
            "  - Larger chunks reduce number of context pieces but increase noise and embedding cost.\n",
            "- Embeddings:\n",
            "  - Static vs dynamic: static recompute only on doc change; dynamic allow context-aware embeddings.\n",
            "  - Embedding dimension: prefer 1k–2k dims for dense corpora.\n",
            "- Reranking:\n",
            "  - Use bi-encoder for fast retrieval; cross-encoder for accuracy on top results.\n",
            "  - Example: use Sentence Transformers for bi-encoder; miniLM or BERT cross-encoder for reranking.\n",
            "- Prompt engineering:\n",
            "  - Always include explicit instructions to cite sources, to say “I don’t know” if unsupported.\n",
            "  - Template: system role + user query + numbered retrieved passages with metadata + response constraints (max tokens, format).\n",
            "- Limit hallucination:\n",
            "  - Ask model to quote supporting snippets and to mark speculative text.\n",
            "  - Re-run final answer through verification step (e.g., check claims against source snippets or call fact-checking model).\n",
            "- Latency & cost optimizations:\n",
            "  - Cache embeddings and frequent retrievals; memoize LLM outputs for identical prompts.\n",
            "  - Use approximate nearest neighbor (ANN) indexes (HNSW, IVF) for speed. Pinecone/FAISS with HNSW is common.\n",
            "  - Consider smaller LLMs for drafting + bigger model for final answer.\n",
            "- Security, privacy, governance:\n",
            "  - Encrypt data at rest & transit; maintain auditable logs of which docs were used per answer.\n",
            "  - PII: avoid embedding raw PII; use tokenization/hashing or remove sensitive fields prior to embedding.\n",
            "  - Data residency: if regulation requires, host vector DB and LLM on-prem or in a specific cloud region.\n",
            "- Versioning & provenance:\n",
            "  - Store doc id, chunk id, ingestion timestamp, and embedding model version; include these in the returned answer metadata.\n",
            "- Evaluation:\n",
            "  - Use Precision@k, Recall@k for retrieval; human evaluation for answer correctness.\n",
            "  - Track hallucination rate, factual accuracy (domain-specific tests), latency, cost-per-query.\n",
            "  - Build a test suite of queries with gold answers and relevant docs.\n",
            "\n",
            "Operational & scaling patterns\n",
            "- Batch ingestion pipeline: ETL -> clean -> chunk -> embed -> upsert; schedule incremental runs.\n",
            "- Index partitioning: by tenant, language, or recency to reduce query scope and enforce access control.\n",
            "- Autoscale vector DB instances and LLM inference clusters; keep warm pools for low latency.\n",
            "- Monitoring: query distribution, top queries, failed retrievals, hallucination cases, cost spikes.\n",
            "\n",
            "Costs & resource tradeoffs\n",
            "- Embedding costs: per-doc + per-update. Use smaller embedder for high-frequency non-critical items.\n",
            "- LLM inference: token costs scale with context length. Keep retrieved context minimal and relevant.\n",
            "- Storage & retrieval: vector DB storage vs compute on the fly (approx NN saves compute).\n",
            "- Suggested budgets: pilot with limited document slices and measure cost per query; typical RAG systems converge to $0.01–$0.50+ per high-quality query depending on model choices.\n",
            "\n",
            "Concrete stacks & libraries\n",
            "- Orchestration / frameworks: LangChain, LlamaIndex, Haystack, Semantic Kernel.\n",
            "- Vector DBs: Pinecone, FAISS, Milvus, Weaviate, Qdrant.\n",
            "- Embeddings: OpenAI, Cohere, Sentence-Transformers (SBERT).\n",
            "- Rerankers / cross-encoders: Sentence Transformers cross-encoder, DistilBERT.\n",
            "- LLMs: OpenAI GPT family, Anthropic Claude, Mistral, Llama 2 / Falcon (open models for on-prem).\n",
            "- Agent frameworks: LangChain agents, Microsoft Semantic Kernel.\n",
            "\n",
            "Example use-case mapping (quick)\n",
            "- Customer support bot (documents + KB): RAG + hybrid lexical + reranker + citation + human escalation.\n",
            "- Research assistant (papers): RAG with 1500-token chunks + cross-encoder rerank + summarization + sources.\n",
            "- Personal assistant (calendar, email): Tool-enabled agent + per-user memory + secure storage + consented PII handling.\n",
            "- Compliance checker (contracts): KG for structured clauses + RAG for long-form text + deterministic rule engine.\n",
            "\n",
            "Risk modes & mitigations\n",
            "- Hallucination: require source quotes; verification pass; conservative generation; human-in-loop on high-risk outputs.\n",
            "- Data leakage: redact sensitive data before embedding; role-based access to vector DB; per-tenant indexes.\n",
            "- Drift: retrain or re-ingest periodically; keep test queries to detect knowledge decay.\n",
            "\n",
            "Evaluation experiments to run (practical)\n",
            "- Retrieval ablation: compare embeddings (open vs domain), chunk sizes, top-K values.\n",
            "- Reranker impact: measure accuracy with/without cross-encoder rerank.\n",
            "- Prompt formats: test direct concatenation vs instruction-based retrieval augmentation.\n",
            "- Latency/cost: run synthetic load tests to map cost per 1k queries.\n",
            "\n",
            "Recommended starting configurations (practical defaults)\n",
            "- Chunk: ~800 tokens, 20% overlap.\n",
            "- Embedding model: domain-appropriate (e.g., text-embedding-3 small/large).\n",
            "- Vector DB: Pinecone or FAISS-backed service with HNSW.\n",
            "- Top-K retrieval: 5, then rerank top 20 with cross-encoder if needed.\n",
            "- LLM: smaller model for draft (e.g., 7–13B) and larger for final/critical answers, or a single medium-sized model with low temperature.\n",
            "- Prompt: include numbered snippets with source metadata, instruction to cite snippet numbers, and a final “If no snippet supports claim, say ‘I don’t know’”.\n",
            "\n",
            "Next steps I can help with\n",
            "- Map one of the above approaches to your specific constraints (data size, latency, security, budget).\n",
            "- Provide ready-to-run code examples (Python) for RAG with Pinecone + embeddings + LLM.\n",
            "- Draft prompt templates and evaluation test cases.\n",
            "- Create an experiment plan to compare hybrid retrieval vs pure dense retrieval.\n",
            "\n",
            "Which direction would you like to explore deeper? If you give me your top priorities (accuracy, freshness, latency, privacy, cost) and a rough scale of documents/users, I’ll propose a concrete architecture and cost/latency estimate.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY8t8YmVyWvg",
        "outputId": "e722453b-26d0-42b7-bc14-0d1332cc312e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 6246 tokens\n",
            "\n",
            "Response: Good question — almost any persistent or live information source can be used to give context to an LLM. Below is a practical, categorized catalogue of data source types, with a short description of how you’d ingest/use them, pros/cons, and integration notes (parsing, indexing, privacy, retrieval). Tell me which of these you want to dive into and I’ll give code and architecture sketches.\n",
            "\n",
            "Textual / document sources\n",
            "- Plain text files (txt):\n",
            "  - Integration: read, chunk, embed.\n",
            "  - Pros: trivial parsing, low noise.\n",
            "  - Cons: no structure or metadata unless added.\n",
            "  - Use cases: notes, transcripts.\n",
            "- Word docs / PDFs / PowerPoints:\n",
            "  - Integration: PDF/Office parsers (pdfplumber, Apache Tika, tika-server), OCR for scanned PDFs; chunk + embed; preserve page/slide metadata.\n",
            "  - Pros: common corporate docs, rich content.\n",
            "  - Cons: noisy layout, OCR errors, requires chunking and overlap.\n",
            "- HTML / Websites / Blogs:\n",
            "  - Integration: crawl (Scrapy, Playwright), clean boilerplate (readability), index pages, track sitemaps & change detection.\n",
            "  - Pros: public web knowledge, freshness.\n",
            "  - Cons: noisy, link rot; need filtering for quality.\n",
            "- Email / Chat logs:\n",
            "  - Integration: connectors (IMAP, Gmail API, Slack API), thread-aware chunking, metadata (sender, timestamp).\n",
            "  - Pros: conversational context, user intent signals.\n",
            "  - Cons: PII, privacy concerns; need consent & redaction.\n",
            "- Research papers / scientific corpora:\n",
            "  - Integration: PDF parsing, extract figures/tables, chunk by sections, index citations as metadata.\n",
            "  - Pros: high-quality domain knowledge.\n",
            "  - Cons: complex structure; figures may need special handling.\n",
            "- Legal contracts / policies:\n",
            "  - Integration: OCR + NLP clause extraction, map to ontology, store clause IDs.\n",
            "  - Pros: high-stakes, structured patterns; good for RAG + deterministic checks.\n",
            "  - Cons: requires careful preprocessing and provenance.\n",
            "\n",
            "Structured & semi-structured sources\n",
            "- Relational databases (SQL):\n",
            "  - Integration: query data, convert results to natural language or structured context blocks; enable LLM to generate SQL via translator or use LLM to form queries.\n",
            "  - Pros: precise, authoritative facts; easy to enforce access control.\n",
            "  - Cons: need schema mapping; real-time queries may be needed.\n",
            "- NoSQL/document stores (MongoDB, DynamoDB):\n",
            "  - Integration: serialize documents (JSON), flatten nested fields for chunking, index key fields as metadata.\n",
            "  - Pros: flexible schemas; good for app data.\n",
            "  - Cons: inconsistent formats across records.\n",
            "- CSV / spreadsheets:\n",
            "  - Integration: parse, turn rows/tables into summarized context or structured prompts (or use table-to-text models).\n",
            "  - Pros: tabular clarity for facts & metrics.\n",
            "  - Cons: scale/size can be large; need summarization or selective indexing.\n",
            "- Knowledge graphs / triple stores:\n",
            "  - Integration: SPARQL/graph queries; expose structured subgraphs as context or use to verify LLM outputs.\n",
            "  - Pros: explicit relations, explainability, good for reasoning.\n",
            "  - Cons: expensive to build; mapping natural language to queries can be complex.\n",
            "- Event logs / audit trails:\n",
            "  - Integration: filter relevant sequences, summarize events into narrative chunks for LLM context.\n",
            "  - Pros: exact historical actions for traceability.\n",
            "  - Cons: high volume; requires aggregation.\n",
            "\n",
            "Multimedia & non-text sources\n",
            "- Images / diagrams:\n",
            "  - Integration: OCR for text; image-embedding (CLIP-style) or multimodal LLMs; extract captions & alt text.\n",
            "  - Pros: product photos, diagrams, screenshots add context.\n",
            "  - Cons: need vision models; may lose nuance if only embeddings used.\n",
            "- Audio (voice recordings, podcasts):\n",
            "  - Integration: ASR (Speech-to-text), diarization, speaker labeling, chunk transcripts, index with timestamps.\n",
            "  - Pros: rich conversational data.\n",
            "  - Cons: ASR errors; heavy storage & compute for transcription.\n",
            "- Video:\n",
            "  - Integration: keyframe extraction, ASR for audio track, OCR on frames, multimodal embeddings, segment-level indexing.\n",
            "  - Pros: rich demos, meetings, training videos.\n",
            "  - Cons: heavy processing, large storage needs.\n",
            "- Time series / sensor / telemetry:\n",
            "  - Integration: convert relevant windows into human-readable summaries or event descriptors, index aggregated features.\n",
            "  - Pros: telemetry for diagnostics and anomaly context.\n",
            "  - Cons: numeric data needs meaningful summarization for LLMs.\n",
            "\n",
            "Live / dynamic sources\n",
            "- APIs & webhooks (real-time external services):\n",
            "  - Integration: call APIs at query-time (tooling/agent pattern) and feed results into prompt; cache stable responses.\n",
            "  - Pros: freshness and live data (prices, weather, stock).\n",
            "  - Cons: latency; reliability of external APIs.\n",
            "- Web search / indexers:\n",
            "  - Integration: call search APIs (Bing, Google) or site search; retrieve snippets for RAG with provenance.\n",
            "  - Pros: current news and broad coverage.\n",
            "  - Cons: noisy, contradictory sources; must filter/summarize.\n",
            "- Streaming feeds (social, newswire, RSS, Kafka):\n",
            "  - Integration: stream ingestion pipeline, dedupe, chunk, embed; real-time alerts or indexing.\n",
            "  - Pros: immediate events, trend detection.\n",
            "  - Cons: volume, signal-to-noise issues.\n",
            "\n",
            "Code & developer artifacts\n",
            "- Source code repositories (Git):\n",
            "  - Integration: parse files, extract functions/classes, index code comments & README; code-aware chunking and commit metadata.\n",
            "  - Pros: useful for code search, explainability, developer assistant.\n",
            "  - Cons: large corpus; merging code and docs requires format handling.\n",
            "- CI logs / build artifacts:\n",
            "  - Integration: extract failure messages & traces, index by job ID, time.\n",
            "  - Pros: debugging context.\n",
            "  - Cons: noisy, ephemeral.\n",
            "\n",
            "Derived & ML artifacts\n",
            "- Embeddings & vector indexes:\n",
            "  - Integration: store precomputed vectors with metadata; used for fast similarity retrieval in RAG.\n",
            "  - Pros: immediate retrieval, modality-agnostic (if embeddings exist).\n",
            "  - Cons: embedding model must be versioned; potential leakage.\n",
            "- Trained models / classifiers outputs:\n",
            "  - Integration: use model outputs (labels, probabilities) as metadata or context (e.g., sentiment, topic).\n",
            "  - Pros: enrich context with structured signals.\n",
            "  - Cons: errors propagate; version control required.\n",
            "\n",
            "Domain-specific / regulated sources\n",
            "- Electronic health records (EHR) / medical imaging:\n",
            "  - Integration: strict access & de-identification; map clinical notes to standard ontologies (ICD, SNOMED); use for decision support with human oversight.\n",
            "  - Pros: clinical insights and patient history.\n",
            "  - Cons: very high privacy/regulatory constraints; auditing required.\n",
            "- Financial records / transaction histories:\n",
            "  - Integration: aggregate transactions, summarize patterns; keep provenance & consent.\n",
            "  - Pros: authoritative financial facts.\n",
            "  - Cons: PII/sensitive legal/regulatory concerns.\n",
            "- Legal databases & statutes:\n",
            "  - Integration: index by jurisdiction, link citations, map clauses to templates.\n",
            "  - Pros: high-value legal grounding.\n",
            "  - Cons: frequent updates, jurisdictional complexity.\n",
            "\n",
            "Metadata & auxiliary signals\n",
            "- Timestamps, authorship, source URLs, document version:\n",
            "  - Integration: attach to every chunk as metadata; use filters in retrieval to control recency/authority.\n",
            "  - Pros: provenance, traceability.\n",
            "  - Cons: requires consistent tagging on ingestion.\n",
            "- Confidence scores, content tags, topic labels:\n",
            "  - Integration: generated during ingestion (NLP labeling) and stored as filters.\n",
            "  - Pros: lets you favor high-quality sources.\n",
            "  - Cons: introduces additional processing cost.\n",
            "\n",
            "Practical considerations for all source types\n",
            "- Preprocessing: cleaning, normalization, deduplication, language detection, chunking with overlap, metadata extraction.\n",
            "- Indexing: choose vector DB (Pinecone, Qdrant, FAISS) for embeddings; use Elasticsearch/Opensearch for lexical indexing; consider hybrid retrieval.\n",
            "- Modality encoders: use modality-specific encoders (ASR, OCR, CLIP, image captioners) and store derived text and embeddings.\n",
            "- Freshness & versioning: store ingestion timestamps & embedding model versions; implement incremental upserts and re-embedding strategies.\n",
            "- Privacy & compliance: redact PII, implement access controls, encryption at rest/in transit, consent management, data residency.\n",
            "- Provenance: return doc IDs, source links, timestamps, and snippet citations with answers.\n",
            "- Cost & latency: multimedia & heavy preprocessing increase cost; consider precomputing embeddings and summarizing large documents.\n",
            "- Quality control: sample-based human review, automated quality heuristics (length, stopwords ratio, OCR confidence).\n",
            "\n",
            "Which sources to prioritize depends on your goals\n",
            "- Freshness-critical (news, prices): APIs, web search, RSS, streaming feeds.\n",
            "- Precision-critical (finance, clinical, inventory): canonical DBs, KGs, structured tables.\n",
            "- Rich explanatory context (research, documentation): PDFs, docs, wikis, papers.\n",
            "- Conversational continuity & personalization: chat logs, email, per-user memory vectors.\n",
            "- Multimodal tasks (product support with images/video): images, screenshots, video + transcripts.\n",
            "\n",
            "If you want, I can:\n",
            "- Propose an ingestion pipeline mapping for a selected set of sources.\n",
            "- Show code snippets for extracting, chunking, embedding, and indexing one or more source types (e.g., PDFs, Slack, SQL).\n",
            "- Provide privacy/redaction patterns for sensitive sources.\n",
            "\n",
            "Which source(s) are you considering for your system?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJKnNnDoyWvg",
        "outputId": "92e9fd8e-a9a8-43fb-ae30-a0a3dcd2e4f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 6382 tokens\n",
            "\n",
            "Response: Your aim in this conversation is to explore the potential of integrating Large Language Models with external knowledge — i.e., evaluate architectures, data sources, retrieval/verification patterns, tradeoffs (accuracy, freshness, latency, privacy, cost), and options like RAG, KGs, tool-enabled agents, memory, etc.\n",
            "\n",
            "The very specific code you asked me to remember is: PINECONE_RULEZ_01. I’ll keep that available for this session — would you like me to use it in examples or tag items with it?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": \"What is my aim again? Also what was the very specific code you were tasked with remembering?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgNJJK4QyWvg"
      },
      "source": [
        "Our LLM with buffer memory can clearly remember earlier interactions in the conversation. Let's take a closer look at how the messages are being stored:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-vODGPayWvg",
        "outputId": "74e17904-0e50-4ef5-b136-f2c7877cf633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation History:\n",
            "Human: content='Good morning AI!' additional_kwargs={} response_metadata={}\n",
            "AI: content='Good morning! I’m an AI assistant — ready to help with questions, writing, code, planning, brainstorming, summaries, translations, and more. What would you like to do today?' additional_kwargs={} response_metadata={}\n",
            "Human: content='\\n\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\\n\\nAlso, remember this very specific code: PINECONE_RULEZ_01\"\\n' additional_kwargs={} response_metadata={}\n",
            "AI: content='Nice — this is a great area to explore. I’ll remember the code PINECONE_RULEZ_01 for the remainder of this chat (I can’t persist it across separate sessions unless you store it somewhere permanent). Would you like me to use that code in examples or to tag specific items in our discussion?\\n\\nBelow are practical options, architecture patterns, and concrete considerations for integrating LLMs with external knowledge.\\n\\nHigh-level patterns\\n- Retrieval-Augmented Generation (RAG): store external documents as embeddings in a vector DB (Pinecone, FAISS, Milvus). At query time, embed the user query, retrieve nearest chunks, then condition the LLM on those retrieved passages to produce grounded answers.\\n- Hybrid retrieval: combine traditional lexical search (BM25) with vector search to handle exact-match queries and semantic ones.\\n- Tooling / API chaining: have the LLM call external tools (search APIs, databases, calculators) for tasks requiring up-to-date facts or non-language computations; the LLM forms the query, then ingests the tool result to produce final output.\\n- Knowledge graphs & symbolic stores: use KG for structured queries, constraints, and reasoning; combine KG lookups with LLM natural-language generation for explainable responses.\\n- Fine-tuning / adapters: when you have a domain corpus, you can fine-tune or use lightweight adapters to bias the LLM for domain style—keep RAG for factual grounding to avoid hallucinations.\\n\\nCore components of a RAG pipeline\\n1. Ingestion & preprocessing\\n   - Split documents into chunks (size 500–1,500 tokens depending on model & embedding costs).\\n   - Clean, normalize, and add metadata (title, source, timestamp, type).\\n2. Embedding\\n   - Choose an embedding model appropriate for your language and domain.\\n   - Store embeddings and metadata in a vector DB (Pinecone, FAISS).\\n3. Retrieval\\n   - Query embedding for the user prompt, apply metadata filters if needed (e.g., language, date).\\n   - Use cosine or dot-product similarity; consider re-ranking top-K using a cross-encoder or BM25.\\n4. Prompting the LLM\\n   - Provide the LLM: user question + retrieved passages + instructions to cite sources and limit hallucination.\\n   - Use templates that constrain length, specify citation format, and set role/temperature.\\n5. Post-processing & provenance\\n   - Return extracted sources, confidence score, and possibly a short evidence snippet.\\n   - Track provenance (doc id, chunk id, timestamp) so user can verify claims.\\n\\nPractical techniques to reduce hallucination\\n- Include exact source snippets in the prompt and ask the model to paraphrase only what’s supported.\\n- Force the model to produce a “Sources” section with links/IDs.\\n- Use a verification step: after generation, re-query the retrieved documents or an external fact-checker to validate claims.\\n- Use conservative decoding (lower temperature) or use deterministic models for factual outputs.\\n\\nScaling, latency, and cost tradeoffs\\n- Cache recent queries & embeddings to lower latency.\\n- Use smaller embedding models for frequent queries and larger models for critical retrieval.\\n- Limit the number of retrieved passages (e.g., top-5) to control prompt size and cost.\\n- Consider asynchronous retrieval + streaming generation for responsive UIs.\\n\\nSecurity, privacy, and governance\\n- Encrypt data at rest and in transit.\\n- Implement access control and audit logs for data usage and model queries.\\n- Mask or avoid storing PII in embeddings; if you must, apply hashing or differential privacy techniques.\\n- Keep retention policies and allow users to delete their data.\\n\\nEvaluation & metrics\\n- Relevance (Precision@k, Recall@k)\\n- Answer accuracy (human evaluation, automated fact-checkers)\\n- Hallucination rate\\n- Latency & cost per query\\n- User satisfaction / task success rate\\n\\nConcrete minimal RAG pseudocode (conceptual)\\n- Ingest docs -> chunk -> compute embeddings -> upsert to Pinecone with metadata\\n- On user query:\\n  1. q_emb = embed(query)\\n  2. matches = pinecone.query(q_emb, top_k=5, filter=metadata_filters)\\n  3. context = concat(matches.snippets)\\n  4. prompt = build_prompt(query, context, instructions)\\n  5. answer = LLM.generate(prompt)\\n  6. return answer + matches.metadata\\n\\nWhere your PINECONE_RULEZ_01 code could be used\\n- As a session tag or label in metadata for documents you want associated with a particular experiment (I can reference it during this chat).\\n- As a test string to verify ingestion & retrieval (e.g., index a doc containing that code and show that retrieval returns it).\\n\\nNext steps I can help with\\n- Design a concrete architecture tailored to your data volume & latency needs.\\n- Provide runnable example code (Python) using your chosen embedding model + Pinecone + an LLM.\\n- Create prompt templates and evaluation scripts.\\n- Prototype a small demo pipeline and test cases using PINECONE_RULEZ_01.\\n\\nWhich of these would you like to do next, and should I use PINECONE_RULEZ_01 in examples or indexing demos in this session?' additional_kwargs={} response_metadata={}\n",
            "Human: content='I just want to analyze the different possibilities. What can you think of?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Great — below is a broad, structured analysis of the main possibilities for integrating Large Language Models with external knowledge. I’ll give each option a plain description, core mechanics, concrete pros/cons, common parameter choices, best-fit use cases, and practical considerations (tools, costs, evaluation). If you want to deep-dive any single option I can produce architecture diagrams, code, or an experiment plan.\\n\\nSummary / decision tree (quick)\\n- Need recency/grounding + low hallucination: Retrieval-Augmented Generation (RAG) + reranker + verification.\\n- Need structured queries and explainability: Knowledge Graph / database + LLM for natural language.\\n- Need action-taking and live data: Tool-enabled Agents (APIs, search, calculators).\\n- Need long-term personalization / memory: Memory-augmented LLMs (vector DB + lifecycle + user profiles).\\n- High throughput & low cost: Lightweight embeddings + hybrid retrieval + caching + smaller LLMs / distilled models.\\n\\n1) Retrieval-Augmented Generation (RAG)\\n- What it is: Use an embedding model + vector DB to fetch relevant passages/documents at query time; concatenate or otherwise provide retrieved context to the LLM to produce grounded outputs.\\n- Core flow: chunk -> embed -> store -> on query embed -> retrieve top-K -> optionally rerank -> build prompt -> LLM answer.\\n- Typical parameters:\\n  - Chunk size: 500–1500 tokens (domain-specific; longer when context coherence matters).\\n  - Top-K: 3–10 (start with 5).\\n  - Embedding model: OpenAI text-embedding-3 or similar; dimension 1536+ / domain-tuned.\\n  - Reranker: cross-encoder (e.g., miniLM cross-encoder or a dedicated ensembler) for top 20 -> rerank top 5.\\n- Pros:\\n  - Grounded answers; reduces hallucinations if prompts ask to cite sources.\\n  - Scales to large corpora; fast retrieval.\\n  - Easy to update knowledge (upsert docs).\\n- Cons:\\n  - Prompt length limits; cost per query (LLM tokens + retrieval ops).\\n  - Requires engineering for chunking, filters, versioning.\\n- Best for: customer support, documentation assistants, legal/medical reference (with verification), research assistants.\\n- Tools: Pinecone, FAISS, Milvus, Weaviate; LangChain, LlamaIndex, Haystack for orchestration.\\n\\n2) Hybrid Retrieval (sparse + dense)\\n- What it is: Combine lexical search (BM25, Elasticsearch) with dense vector search to capture both exact matches and semantic similarity.\\n- Approach: run both searches, merge/rerank results, or use lexical for initial filter then dense rerank.\\n- Pros:\\n  - Better recall for exact query matches (IDs, names) while preserving semantic matches.\\n- Cons:\\n  - More complex stack; requires tuning merging logic.\\n- Best for: enterprise search, code search, domain with many named entities.\\n\\n3) Knowledge Graphs / Symbolic Stores + LLMs\\n- What it is: Store structured facts in a KG or relational DB; LLM translates natural-language requests into structured queries (SPARQL/SQL), returns structured results, then LLM generates human-facing text.\\n- Pros:\\n  - Precise, auditable answers; easy enforcement of constraints; excellent for logical reasoning.\\n- Cons:\\n  - Hard to encode nuanced/long-text knowledge; building/maintaining KG is labor-intensive.\\n- Best for: product catalogs, inventory, clinical decision support (where rules matter), compliance checks.\\n- Tools: Neo4j, AWS Neptune, GraphQL front-ends, SQL + LLM query builders.\\n\\n4) Tool/Plugin-enabled Agents\\n- What it is: LLMs are orchestrators that call external tools (search engine, calculators, calendar, internal APIs) and incorporate tool outputs into answers.\\n- Architectures: Single-step tool call vs multi-step planning agent (ReAct, tree of thoughts, planner-executor).\\n- Pros:\\n  - Access live data and actions; solves tasks beyond text generation.\\n- Cons:\\n  - Complex error handling, safety controls, tool interface design.\\n- Best for: automation assistants, actionable agents (book, buy, fetch data), data pipelines.\\n- Tools: OpenAI function calling, LangChain tools, Microsoft Copilot plugins.\\n\\n5) Fine-tuning / Adapters / Retrieval + Fine-tuning combo\\n- What it is: Fine-tune LLMs on domain-specific corpora (or use LoRA/adapters) and keep RAG for factual grounding.\\n- Pros:\\n  - Better stylistic alignment; faster inference if using smaller tuned model.\\n- Cons:\\n  - Risk of embedding incorrect facts; needs retraining for new facts (RAG mitigates).\\n- Best for: brand voice, specialized writing style, technical domain language.\\n\\n6) Long-term Memory & Personalization\\n- What it is: Maintain per-user vectors, preferences, and interaction history in a vector DB and use them as retrieval filters / conditioning context.\\n- Implementation notes:\\n  - Distinguish ephemeral session memory vs persistent memory.\\n  - Use TTLs and user-consent controls for PII.\\n- Pros:\\n  - Personalized answers and continuity across sessions.\\n- Cons:\\n  - Privacy/consent challenges; storage growth.\\n- Best for: personal assistants, sales CRMs, tutoring systems.\\n\\n7) Generative Retrieval / Learned Retrieval\\n- What it is: Use models that directly generate supporting passages or document IDs (e.g., DPR, GenRead) rather than nearest neighbor lookup.\\n- Pros:\\n  - Can be more robust in sparse-doc corpora.\\n- Cons:\\n  - Harder to debug and can hallucinate results if not carefully validated.\\n\\n8) On-demand Web / Search Integration\\n- What it is: Query live web search (Bing, Google, internal site) for fresh results, then use RAG or tool flows.\\n- Pros:\\n  - Freshness and broad coverage.\\n- Cons:\\n  - Noisy sources; need filtering and provenance tracking.\\n\\n9) Multimodal Knowledge Integration\\n- What it is: Index non-text (images, audio, video) by generating embeddings from modality-specific encoders, then retrieve and pass visual/textual snippets to multimodal LLMs.\\n- Use cases: product visual search, video Q&A, multimodal documentation.\\n- Tools: CLIP, OpenAI vision models, multimodal LLMs.\\n\\nCross-cutting technical considerations (concrete)\\n- Chunking strategies:\\n  - Overlap chunks by ~20–30% to preserve context across boundaries.\\n  - Larger chunks reduce number of context pieces but increase noise and embedding cost.\\n- Embeddings:\\n  - Static vs dynamic: static recompute only on doc change; dynamic allow context-aware embeddings.\\n  - Embedding dimension: prefer 1k–2k dims for dense corpora.\\n- Reranking:\\n  - Use bi-encoder for fast retrieval; cross-encoder for accuracy on top results.\\n  - Example: use Sentence Transformers for bi-encoder; miniLM or BERT cross-encoder for reranking.\\n- Prompt engineering:\\n  - Always include explicit instructions to cite sources, to say “I don’t know” if unsupported.\\n  - Template: system role + user query + numbered retrieved passages with metadata + response constraints (max tokens, format).\\n- Limit hallucination:\\n  - Ask model to quote supporting snippets and to mark speculative text.\\n  - Re-run final answer through verification step (e.g., check claims against source snippets or call fact-checking model).\\n- Latency & cost optimizations:\\n  - Cache embeddings and frequent retrievals; memoize LLM outputs for identical prompts.\\n  - Use approximate nearest neighbor (ANN) indexes (HNSW, IVF) for speed. Pinecone/FAISS with HNSW is common.\\n  - Consider smaller LLMs for drafting + bigger model for final answer.\\n- Security, privacy, governance:\\n  - Encrypt data at rest & transit; maintain auditable logs of which docs were used per answer.\\n  - PII: avoid embedding raw PII; use tokenization/hashing or remove sensitive fields prior to embedding.\\n  - Data residency: if regulation requires, host vector DB and LLM on-prem or in a specific cloud region.\\n- Versioning & provenance:\\n  - Store doc id, chunk id, ingestion timestamp, and embedding model version; include these in the returned answer metadata.\\n- Evaluation:\\n  - Use Precision@k, Recall@k for retrieval; human evaluation for answer correctness.\\n  - Track hallucination rate, factual accuracy (domain-specific tests), latency, cost-per-query.\\n  - Build a test suite of queries with gold answers and relevant docs.\\n\\nOperational & scaling patterns\\n- Batch ingestion pipeline: ETL -> clean -> chunk -> embed -> upsert; schedule incremental runs.\\n- Index partitioning: by tenant, language, or recency to reduce query scope and enforce access control.\\n- Autoscale vector DB instances and LLM inference clusters; keep warm pools for low latency.\\n- Monitoring: query distribution, top queries, failed retrievals, hallucination cases, cost spikes.\\n\\nCosts & resource tradeoffs\\n- Embedding costs: per-doc + per-update. Use smaller embedder for high-frequency non-critical items.\\n- LLM inference: token costs scale with context length. Keep retrieved context minimal and relevant.\\n- Storage & retrieval: vector DB storage vs compute on the fly (approx NN saves compute).\\n- Suggested budgets: pilot with limited document slices and measure cost per query; typical RAG systems converge to $0.01–$0.50+ per high-quality query depending on model choices.\\n\\nConcrete stacks & libraries\\n- Orchestration / frameworks: LangChain, LlamaIndex, Haystack, Semantic Kernel.\\n- Vector DBs: Pinecone, FAISS, Milvus, Weaviate, Qdrant.\\n- Embeddings: OpenAI, Cohere, Sentence-Transformers (SBERT).\\n- Rerankers / cross-encoders: Sentence Transformers cross-encoder, DistilBERT.\\n- LLMs: OpenAI GPT family, Anthropic Claude, Mistral, Llama 2 / Falcon (open models for on-prem).\\n- Agent frameworks: LangChain agents, Microsoft Semantic Kernel.\\n\\nExample use-case mapping (quick)\\n- Customer support bot (documents + KB): RAG + hybrid lexical + reranker + citation + human escalation.\\n- Research assistant (papers): RAG with 1500-token chunks + cross-encoder rerank + summarization + sources.\\n- Personal assistant (calendar, email): Tool-enabled agent + per-user memory + secure storage + consented PII handling.\\n- Compliance checker (contracts): KG for structured clauses + RAG for long-form text + deterministic rule engine.\\n\\nRisk modes & mitigations\\n- Hallucination: require source quotes; verification pass; conservative generation; human-in-loop on high-risk outputs.\\n- Data leakage: redact sensitive data before embedding; role-based access to vector DB; per-tenant indexes.\\n- Drift: retrain or re-ingest periodically; keep test queries to detect knowledge decay.\\n\\nEvaluation experiments to run (practical)\\n- Retrieval ablation: compare embeddings (open vs domain), chunk sizes, top-K values.\\n- Reranker impact: measure accuracy with/without cross-encoder rerank.\\n- Prompt formats: test direct concatenation vs instruction-based retrieval augmentation.\\n- Latency/cost: run synthetic load tests to map cost per 1k queries.\\n\\nRecommended starting configurations (practical defaults)\\n- Chunk: ~800 tokens, 20% overlap.\\n- Embedding model: domain-appropriate (e.g., text-embedding-3 small/large).\\n- Vector DB: Pinecone or FAISS-backed service with HNSW.\\n- Top-K retrieval: 5, then rerank top 20 with cross-encoder if needed.\\n- LLM: smaller model for draft (e.g., 7–13B) and larger for final/critical answers, or a single medium-sized model with low temperature.\\n- Prompt: include numbered snippets with source metadata, instruction to cite snippet numbers, and a final “If no snippet supports claim, say ‘I don’t know’”.\\n\\nNext steps I can help with\\n- Map one of the above approaches to your specific constraints (data size, latency, security, budget).\\n- Provide ready-to-run code examples (Python) for RAG with Pinecone + embeddings + LLM.\\n- Draft prompt templates and evaluation test cases.\\n- Create an experiment plan to compare hybrid retrieval vs pure dense retrieval.\\n\\nWhich direction would you like to explore deeper? If you give me your top priorities (accuracy, freshness, latency, privacy, cost) and a rough scale of documents/users, I’ll propose a concrete architecture and cost/latency estimate.' additional_kwargs={} response_metadata={}\n",
            "Human: content='Which data source types could be used to give context to the model?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Good question — almost any persistent or live information source can be used to give context to an LLM. Below is a practical, categorized catalogue of data source types, with a short description of how you’d ingest/use them, pros/cons, and integration notes (parsing, indexing, privacy, retrieval). Tell me which of these you want to dive into and I’ll give code and architecture sketches.\\n\\nTextual / document sources\\n- Plain text files (txt):\\n  - Integration: read, chunk, embed.\\n  - Pros: trivial parsing, low noise.\\n  - Cons: no structure or metadata unless added.\\n  - Use cases: notes, transcripts.\\n- Word docs / PDFs / PowerPoints:\\n  - Integration: PDF/Office parsers (pdfplumber, Apache Tika, tika-server), OCR for scanned PDFs; chunk + embed; preserve page/slide metadata.\\n  - Pros: common corporate docs, rich content.\\n  - Cons: noisy layout, OCR errors, requires chunking and overlap.\\n- HTML / Websites / Blogs:\\n  - Integration: crawl (Scrapy, Playwright), clean boilerplate (readability), index pages, track sitemaps & change detection.\\n  - Pros: public web knowledge, freshness.\\n  - Cons: noisy, link rot; need filtering for quality.\\n- Email / Chat logs:\\n  - Integration: connectors (IMAP, Gmail API, Slack API), thread-aware chunking, metadata (sender, timestamp).\\n  - Pros: conversational context, user intent signals.\\n  - Cons: PII, privacy concerns; need consent & redaction.\\n- Research papers / scientific corpora:\\n  - Integration: PDF parsing, extract figures/tables, chunk by sections, index citations as metadata.\\n  - Pros: high-quality domain knowledge.\\n  - Cons: complex structure; figures may need special handling.\\n- Legal contracts / policies:\\n  - Integration: OCR + NLP clause extraction, map to ontology, store clause IDs.\\n  - Pros: high-stakes, structured patterns; good for RAG + deterministic checks.\\n  - Cons: requires careful preprocessing and provenance.\\n\\nStructured & semi-structured sources\\n- Relational databases (SQL):\\n  - Integration: query data, convert results to natural language or structured context blocks; enable LLM to generate SQL via translator or use LLM to form queries.\\n  - Pros: precise, authoritative facts; easy to enforce access control.\\n  - Cons: need schema mapping; real-time queries may be needed.\\n- NoSQL/document stores (MongoDB, DynamoDB):\\n  - Integration: serialize documents (JSON), flatten nested fields for chunking, index key fields as metadata.\\n  - Pros: flexible schemas; good for app data.\\n  - Cons: inconsistent formats across records.\\n- CSV / spreadsheets:\\n  - Integration: parse, turn rows/tables into summarized context or structured prompts (or use table-to-text models).\\n  - Pros: tabular clarity for facts & metrics.\\n  - Cons: scale/size can be large; need summarization or selective indexing.\\n- Knowledge graphs / triple stores:\\n  - Integration: SPARQL/graph queries; expose structured subgraphs as context or use to verify LLM outputs.\\n  - Pros: explicit relations, explainability, good for reasoning.\\n  - Cons: expensive to build; mapping natural language to queries can be complex.\\n- Event logs / audit trails:\\n  - Integration: filter relevant sequences, summarize events into narrative chunks for LLM context.\\n  - Pros: exact historical actions for traceability.\\n  - Cons: high volume; requires aggregation.\\n\\nMultimedia & non-text sources\\n- Images / diagrams:\\n  - Integration: OCR for text; image-embedding (CLIP-style) or multimodal LLMs; extract captions & alt text.\\n  - Pros: product photos, diagrams, screenshots add context.\\n  - Cons: need vision models; may lose nuance if only embeddings used.\\n- Audio (voice recordings, podcasts):\\n  - Integration: ASR (Speech-to-text), diarization, speaker labeling, chunk transcripts, index with timestamps.\\n  - Pros: rich conversational data.\\n  - Cons: ASR errors; heavy storage & compute for transcription.\\n- Video:\\n  - Integration: keyframe extraction, ASR for audio track, OCR on frames, multimodal embeddings, segment-level indexing.\\n  - Pros: rich demos, meetings, training videos.\\n  - Cons: heavy processing, large storage needs.\\n- Time series / sensor / telemetry:\\n  - Integration: convert relevant windows into human-readable summaries or event descriptors, index aggregated features.\\n  - Pros: telemetry for diagnostics and anomaly context.\\n  - Cons: numeric data needs meaningful summarization for LLMs.\\n\\nLive / dynamic sources\\n- APIs & webhooks (real-time external services):\\n  - Integration: call APIs at query-time (tooling/agent pattern) and feed results into prompt; cache stable responses.\\n  - Pros: freshness and live data (prices, weather, stock).\\n  - Cons: latency; reliability of external APIs.\\n- Web search / indexers:\\n  - Integration: call search APIs (Bing, Google) or site search; retrieve snippets for RAG with provenance.\\n  - Pros: current news and broad coverage.\\n  - Cons: noisy, contradictory sources; must filter/summarize.\\n- Streaming feeds (social, newswire, RSS, Kafka):\\n  - Integration: stream ingestion pipeline, dedupe, chunk, embed; real-time alerts or indexing.\\n  - Pros: immediate events, trend detection.\\n  - Cons: volume, signal-to-noise issues.\\n\\nCode & developer artifacts\\n- Source code repositories (Git):\\n  - Integration: parse files, extract functions/classes, index code comments & README; code-aware chunking and commit metadata.\\n  - Pros: useful for code search, explainability, developer assistant.\\n  - Cons: large corpus; merging code and docs requires format handling.\\n- CI logs / build artifacts:\\n  - Integration: extract failure messages & traces, index by job ID, time.\\n  - Pros: debugging context.\\n  - Cons: noisy, ephemeral.\\n\\nDerived & ML artifacts\\n- Embeddings & vector indexes:\\n  - Integration: store precomputed vectors with metadata; used for fast similarity retrieval in RAG.\\n  - Pros: immediate retrieval, modality-agnostic (if embeddings exist).\\n  - Cons: embedding model must be versioned; potential leakage.\\n- Trained models / classifiers outputs:\\n  - Integration: use model outputs (labels, probabilities) as metadata or context (e.g., sentiment, topic).\\n  - Pros: enrich context with structured signals.\\n  - Cons: errors propagate; version control required.\\n\\nDomain-specific / regulated sources\\n- Electronic health records (EHR) / medical imaging:\\n  - Integration: strict access & de-identification; map clinical notes to standard ontologies (ICD, SNOMED); use for decision support with human oversight.\\n  - Pros: clinical insights and patient history.\\n  - Cons: very high privacy/regulatory constraints; auditing required.\\n- Financial records / transaction histories:\\n  - Integration: aggregate transactions, summarize patterns; keep provenance & consent.\\n  - Pros: authoritative financial facts.\\n  - Cons: PII/sensitive legal/regulatory concerns.\\n- Legal databases & statutes:\\n  - Integration: index by jurisdiction, link citations, map clauses to templates.\\n  - Pros: high-value legal grounding.\\n  - Cons: frequent updates, jurisdictional complexity.\\n\\nMetadata & auxiliary signals\\n- Timestamps, authorship, source URLs, document version:\\n  - Integration: attach to every chunk as metadata; use filters in retrieval to control recency/authority.\\n  - Pros: provenance, traceability.\\n  - Cons: requires consistent tagging on ingestion.\\n- Confidence scores, content tags, topic labels:\\n  - Integration: generated during ingestion (NLP labeling) and stored as filters.\\n  - Pros: lets you favor high-quality sources.\\n  - Cons: introduces additional processing cost.\\n\\nPractical considerations for all source types\\n- Preprocessing: cleaning, normalization, deduplication, language detection, chunking with overlap, metadata extraction.\\n- Indexing: choose vector DB (Pinecone, Qdrant, FAISS) for embeddings; use Elasticsearch/Opensearch for lexical indexing; consider hybrid retrieval.\\n- Modality encoders: use modality-specific encoders (ASR, OCR, CLIP, image captioners) and store derived text and embeddings.\\n- Freshness & versioning: store ingestion timestamps & embedding model versions; implement incremental upserts and re-embedding strategies.\\n- Privacy & compliance: redact PII, implement access controls, encryption at rest/in transit, consent management, data residency.\\n- Provenance: return doc IDs, source links, timestamps, and snippet citations with answers.\\n- Cost & latency: multimedia & heavy preprocessing increase cost; consider precomputing embeddings and summarizing large documents.\\n- Quality control: sample-based human review, automated quality heuristics (length, stopwords ratio, OCR confidence).\\n\\nWhich sources to prioritize depends on your goals\\n- Freshness-critical (news, prices): APIs, web search, RSS, streaming feeds.\\n- Precision-critical (finance, clinical, inventory): canonical DBs, KGs, structured tables.\\n- Rich explanatory context (research, documentation): PDFs, docs, wikis, papers.\\n- Conversational continuity & personalization: chat logs, email, per-user memory vectors.\\n- Multimodal tasks (product support with images/video): images, screenshots, video + transcripts.\\n\\nIf you want, I can:\\n- Propose an ingestion pipeline mapping for a selected set of sources.\\n- Show code snippets for extracting, chunking, embedding, and indexing one or more source types (e.g., PDFs, Slack, SQL).\\n- Provide privacy/redaction patterns for sensitive sources.\\n\\nWhich source(s) are you considering for your system?' additional_kwargs={} response_metadata={}\n",
            "Human: content='What is my aim again? Also what was the very specific code you were tasked with remembering?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Your aim in this conversation is to explore the potential of integrating Large Language Models with external knowledge — i.e., evaluate architectures, data sources, retrieval/verification patterns, tradeoffs (accuracy, freshness, latency, privacy, cost), and options like RAG, KGs, tool-enabled agents, memory, etc.\\n\\nThe very specific code you asked me to remember is: PINECONE_RULEZ_01. I’ll keep that available for this session — would you like me to use it in examples or tag items with it?' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# Access the conversation history\n",
        "history = chat_map[\"buffer_example\"].messages\n",
        "print(\"Conversation History:\")\n",
        "for i, msg in enumerate(history):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        role = \"Human\"\n",
        "    elif isinstance(msg, SystemMessage):\n",
        "        role = \"System\"\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        role = \"AI\"\n",
        "    else:\n",
        "        role = \"Unknown\"\n",
        "    print(f\"{role}: {msg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAy5BIyyWvg"
      },
      "source": [
        "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORjuIGNqyWvg"
      },
      "source": [
        "### Memory type #2: Summary - Store Summaries of Past Interactions\n",
        "\n",
        "The problem with storing the entire chat history in agent memory is that, as the conversation progresses, the token count adds up. This is problematic because we might max out our LLM with a prompt that is too large.\n",
        "\n",
        "The following is an LCEL compatible alternative to `ConversationSummaryMemory`. We keep a summary of our previous conversation snippets as our history. The summarization is performed by an LLM.\n",
        "\n",
        "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized - and thus shortened - form, where the summarization is performed by an LLM._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dVnq9-lryWvg"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history and update the summary.\"\"\"\n",
        "        self.messages.extend(messages)\n",
        "\n",
        "        # Construct the summary prompt\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages,\n",
        "                messages=messages\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "l_LolSYjyWvg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "# Create get_chat_history function for summary memory\n",
        "summary_chat_map = {}\n",
        "\n",
        "def get_summary_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in summary_chat_map:\n",
        "        summary_chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    return summary_chat_map[session_id]\n",
        "\n",
        "# Create conversation chain with summary memory\n",
        "conversation_sum = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_summary_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JloQR__QyWvh",
        "outputId": "b31b6840-8757-41df-955f-ba218d304de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 539 tokens\n",
            "\n",
            "Response: Good morning! How can I help you today — do you want news, a summary of something, help with a task (writing, coding, planning), a recipe, or something else?\n"
          ]
        }
      ],
      "source": [
        "# Let's have the same conversation with summary memory\n",
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFMMR4SDyWvh",
        "outputId": "dbd4619f-2d28-4517-e4f8-372a0370d9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 5768 tokens\n",
            "\n",
            "Response: Great — this is a great area to explore. I can help design architectures, give implementation guidance, provide example code, and discuss trade-offs for integrating LLMs with external knowledge.\n",
            "\n",
            "First, about the code you asked me to remember: PINECONE_RULEZ_01\n",
            "- I acknowledge it. Important note: I don’t have persistent user memory in this chat unless a memory feature is enabled by the platform. If memory is enabled and you want this stored, I can save non-sensitive items like that. If it’s not enabled, I can include the code in summaries I write for you here in-session, but I can’t guarantee it will persist between sessions. Also tell me whether the code is sensitive; if it is, we should treat it accordingly.\n",
            "\n",
            "Below is a concise, practical plan and the main options for integrating LLMs with external knowledge, plus a short example stack if you want a concrete starting point.\n",
            "\n",
            "1) High-level approaches\n",
            "- Retrieval-Augmented Generation (RAG): index documents into a vector DB, retrieve nearest neighbors for a query, and feed retrieved context to the LLM with a prompt template.\n",
            "- Tooling/Tool Use: let the LLM call external APIs (search engines, databases, calculators) and incorporate results back into the response.\n",
            "- Hybrid search: combine lexical (BM25) and semantic (vector) retrieval to improve recall and precision.\n",
            "- Knowledge Graphs / Symbolic Reasoning: represent structured facts and run traversals or query the graph to supply precise answers.\n",
            "- Streaming / Real-time updates: keep a separate “recent changes” feed for time-sensitive info.\n",
            "\n",
            "2) Core components & technologies\n",
            "- Embedding model: OpenAI embeddings, Cohere, or local model depending on privacy; used to vectorize documents and queries.\n",
            "- Vector DB: Pinecone (you mentioned it), Milvus, Weaviate, FAISS (self-hosted). Pinecone is managed and straightforward for production.\n",
            "- LLM: OpenAI GPT-family, Anthropic, Llama2/Alpaca variants if self-hosted / privacy is required.\n",
            "- Orchestration / Frameworks: LangChain, LlamaIndex, or a custom thin layer for retrieval + prompting logic.\n",
            "- Backend/API: FastAPI, Flask, or serverless functions to orchestrate retrieval and calls to the LLM.\n",
            "- Monitoring & instrumentation: logging, query latency, recall/precision metrics, hallucination detection, user feedback loop.\n",
            "\n",
            "3) Design best practices\n",
            "- Chunking & metadata: chunk long documents into overlapping passages (e.g., 500–1,000 tokens with 10–20% overlap) and store useful metadata (source, date, author).\n",
            "- Vector + lexical hybrid: do a BM25 pass to filter candidates, then rerank with semantic similarity.\n",
            "- Prompt engineering: provide clear system instructions, source attribution, and chains (retrieve -> extract -> verify -> answer).\n",
            "- Hallucination mitigation: always cite sources, attach retrieved passages, and mark uncertainty when no confident match is found.\n",
            "- Security & privacy: redact PII before indexing; control access to the vector DB; encrypt data in transit and at rest.\n",
            "- Freshness: re-index periodically or stream new docs into the index. Use a time-based filter for recency-sensitive queries.\n",
            "- Cost & latency optimizations: cache frequent queries, reduce embedding calls by batching, use lower-cost embedding models if acceptable.\n",
            "\n",
            "4) Simple example architecture (text)\n",
            "User -> API -> Query embeddings -> Pinecone (vector search; optionally hybrid with BM25) -> retrieved passages -> prompt template + LLM -> LLM response (with citations) -> API returns answer\n",
            "\n",
            "5) Example (pseudocode outline, Python + Pinecone + OpenAI embeddings)\n",
            "- Steps:\n",
            "  1. Embed query\n",
            "  2. Query Pinecone for top-k vectors (include metadata)\n",
            "  3. Construct prompt with retrieved passages and system instructions\n",
            "  4. Call LLM to generate answer\n",
            "  5. Return answer + source list\n",
            "\n",
            "I can produce concrete, runnable code for a stack you choose (e.g., OpenAI embeddings + Pinecone + LangChain + FastAPI). Tell me which stack you prefer, whether Pinecone is mandatory, and whether you want a focus on privacy/self-hosting vs. managed services.\n",
            "\n",
            "6) Evaluation and iteration\n",
            "- Define success metrics: answer accuracy, user satisfaction, latency, cost per query.\n",
            "- Add human-in-the-loop for difficult queries and periodic re-annotation to improve retrieval/re-ranking models.\n",
            "- Continuous testing with adversarial prompts to measure hallucination levels.\n",
            "\n",
            "Next steps I can do for you right now\n",
            "- Draft a concrete system design for a chosen stack and scale (e.g., 10k, 1M, 100M documents).\n",
            "- Produce runnable example code for ingestion, querying, and serving (Python).\n",
            "- Show how to include the code PINECONE_RULEZ_01 in conversation summaries if you enable memory — or include it in ephemeral summaries during this session.\n",
            "- Help choose embedding models & vector DB config for your budget and latency targets.\n",
            "\n",
            "Which of those would you like first? And confirm: do you want PINECONE_RULEZ_01 treated as non-sensitive and included in future conversation summaries (if memory is available)?\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\n",
        "\n",
        "Also, remember this very specific code: PINECONE_RULEZ_01. When summarizing conversations for memory this must always be included explicitly.\"\n",
        "\"\"\"\n",
        "\n",
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": query},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aofnt1cXyWvh",
        "outputId": "9a79efe4-db4d-4e36-fb47-133e83d5725c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 9390 tokens\n",
            "\n",
            "Response: Great — here’s a structured, practical survey of the space with concrete tradeoffs and recommended patterns so you can compare possibilities quickly.\n",
            "\n",
            "High-level approaches\n",
            "- Closed‑book LLM (no external knowledge)\n",
            "  - How: Use an LLM’s internal weights to answer questions (possibly after fine-tuning).\n",
            "  - When: Small domain, stable knowledge, extremely low infra complexity.\n",
            "  - Pros: Simple, low latency, no external infra. Cons: Prone to hallucination, knowledge staleness, limited capacity.\n",
            "- Fine-tuning / adapters / LoRA\n",
            "  - How: Adapt a base model to domain data so the model internalizes knowledge and style.\n",
            "  - When: Need high-quality, domain-specific behavior and you can afford training.\n",
            "  - Pros: Better closed-book accuracy, fewer runtime lookups. Cons: Costly to train/maintain, hard to keep fully up-to-date.\n",
            "- Retrieval-Augmented Generation (RAG)\n",
            "  - How: Use vector (semantic) + optional lexical search to retrieve relevant doc chunks, then condition the LLM on those passages.\n",
            "  - When: Large/sparse, constantly-updating corpora (KBs, docs, web), requirement for citations/grounding.\n",
            "  - Pros: Freshness, smaller LLMs usable, source grounding. Cons: Higher infra complexity, retrieval tuning required.\n",
            "- Tool-augmented LLMs / agent patterns\n",
            "  - How: Give the LLM tools (web search, calculator, DB queries, code execution) and orchestration to call them.\n",
            "  - When: Tasks that require actions, dynamic data, computation, or chain-of-actions.\n",
            "  - Pros: Stronger capabilities, real-world actions. Cons: Complexity in tool design, safety/authorization concerns.\n",
            "- Knowledge graph / symbolic reasoning\n",
            "  - How: Store curated facts/triples, run structured queries and combine with LLM for narration/interpretation.\n",
            "  - When: Strictly-structured domains, reasoning over relationships (graph queries, constraints).\n",
            "  - Pros: Deterministic reasoning, explainability. Cons: Hard to scale coverage, expensive curation.\n",
            "- Hybrid (KG + RAG + tools)\n",
            "  - How: Use KG for entity/relationship integrity, RAG for textual evidence, tools for live actions.\n",
            "  - When: Complex enterprise use-cases needing both structured truth and flexible text grounding.\n",
            "\n",
            "Core components & tech options\n",
            "- Embeddings: OpenAI, Cohere, SentenceTransformers (all-MPNet, SBERT), Hugging Face on-prem models.\n",
            "- Vector DBs: Pinecone, Milvus, Weaviate, Vespa, Qdrant, FAISS (local). Consider persistence, metadata filtering, ANN algorithm (HNSW), scaling.\n",
            "- Lexical Search / BM25: Elasticsearch, Typesense, Whoosh — useful for keyword-oriented retrieval and hybrid searches.\n",
            "- Orchestration & frameworks: LangChain, LlamaIndex, Haystack, Ray Serve for pipelines/agents.\n",
            "- LLMs: OpenAI (gpt-4o, gpt-4), Anthropic, Mistral; self-host: Llama2, MPT, Falcon, etc.\n",
            "- Rerankers / cross-encoders: Dense re-ranker to improve passage order (use a small cross-encoder).\n",
            "- Tooling: Browser automation, search engines, SQL connectors, code execution sandboxes.\n",
            "\n",
            "Retrieval patterns & techniques\n",
            "- Chunking & metadata: Split docs by semantic boundaries (400–2,000 tokens usually) and attach metadata (title, source, date, doc-id).\n",
            "- Hybrid retrieval: Do combined vector search + BM25, then rerank with a cross-encoder.\n",
            "- Hierarchical / two-stage retrieval: coarse retrieval (cheap embedding) → rerank (compute-heavy cross-encoder) → final LLM input.\n",
            "- Context window management: Trim or dynamically select passages using relevance + freshness + diversity heuristics; use compression/summary snippets for long docs.\n",
            "- Top-k & prompt templates: common k = 3–20 depending on doc length and quality. Use instruction templates that ask for concise answers and to cite sources.\n",
            "- Iterative retrieval: Allow the model to ask clarifying follow-ups and re-query (retrieval loop).\n",
            "\n",
            "RAG architecture patterns (concrete)\n",
            "- Simple RAG for Q&A:\n",
            "  - Ingest → create embeddings → store vectors with metadata in vector DB.\n",
            "  - Query → embed query → vector DB top-k → assemble prompt with passages → LLM → answer + sources.\n",
            "- RAG + reranker:\n",
            "  - Add a fast reranker cross-encoder to re-score the top N and return best M for the LLM.\n",
            "- RAG + tool-enabled agent:\n",
            "  - LLM decides whether to call retrieval, run a calculation, or fetch live data; orchestrator manages tool calls, then LLM synthesizes final answer.\n",
            "\n",
            "When to fine-tune vs RAG\n",
            "- Fine-tune when: small/closed corpus, strong need for specialized style/format, low update frequency, or legal/availability reasons.\n",
            "- RAG when: large or frequently changing corpus, need for citations, or want cheaper model compute.\n",
            "\n",
            "Safety, trust & hallucination mitigation\n",
            "- Source citation and grounding: always return sources and confidence, include verbatim evidence extracts when asserting facts.\n",
            "- Reranking & verification: use a verifier model that checks claims against retrieved docs.\n",
            "- Fact-checking pipelines: post-generation retriever that verifies statements and flags contradictions.\n",
            "- Policy & redaction: PII detection and redaction during ingestion, encryption at rest/in transit, RBAC on datasets.\n",
            "- Auditability: store query logs, retrieved IDs, and LLM output to debug hallucinations.\n",
            "\n",
            "Scaling, freshness, and ingestion\n",
            "- Batch vs streaming ingestion: Batch for large historical import; streaming/CDC for live updates (e.g., webhooks, changefeeds).\n",
            "- Re-indexing: incremental updates for changed docs; tombstones for deletes.\n",
            "- Sharding & replication: necessary for very large corpora and high QPS.\n",
            "- Freshness strategies: time-weighted scoring, TTLs on vectors, hybrid with live API calls for ultra-fresh data.\n",
            "\n",
            "Performance & cost optimizations\n",
            "- Cheaper embeddings for large corpora; re-embed only changed docs.\n",
            "- Caching popular queries and answers; materialized summaries for frequent topics.\n",
            "- Two-stage retrieval and smaller LLMs for first-pass answers, escalate to bigger LLM only when needed.\n",
            "- Distillation: distill response templates or critical Q/A pairs into smaller models.\n",
            "\n",
            "Evaluation & observability\n",
            "- Retrieval metrics: Recall@k, MRR, nDCG for relevant docs.\n",
            "- End-to-end metrics: accuracy, human-rated quality/satisfaction, hallucination rate.\n",
            "- Latency & cost: P50/P95 response times; cost per query.\n",
            "- User feedback loop: store corrections and feed into reranker/fine-tune datasets.\n",
            "\n",
            "Concrete stacks (examples)\n",
            "- Managed/fast prototype (low ops):\n",
            "  - OpenAI embeddings + Pinecone (or Qdrant cloud) + OpenAI LLM + LangChain.\n",
            "- Self-hosted/privacy-first:\n",
            "  - SentenceTransformers for embeddings + FAISS or Milvus + Llama2/Alpaca + LlamaIndex or Haystack.\n",
            "- Enterprise large-scale:\n",
            "  - Vector DB (Pinecone/Milvus/Weaviate) + Elasticsearch hybrid index + cross-encoder reranker + model cluster (gpt-4/Anthropic/Mistral) + orchestration (Ray) + observability (Prometheus, Sentry).\n",
            "\n",
            "Use-case mapping & recommendation\n",
            "- Enterprise search for docs/reports: RAG + BM25 hybrid + cross-encoder + citations + RBAC.\n",
            "- Personal assistant over private files: local embeddings + encrypted vector DB + small LLM + tool connectors, strict PII redaction.\n",
            "- Code assistance: RAG on codebase + execution sandbox + safety policies + test-run verification.\n",
            "- Real-time customer support: Combine RAG with live CRM access as a tool, use caching for common queries.\n",
            "\n",
            "Practical tuning rules of thumb\n",
            "- Chunk sizes: 500–1,000 tokens for narrative text; smaller for dense technical docs.\n",
            "- k (retrieval): Start with 5–10 passages, tune by ablation.\n",
            "- Reranker: use cross-encoder on top 50 → pick top 5 for LLM.\n",
            "- Prompts: include instruction, scope, constraints (e.g., “answer concisely”, “show sources inline with bracketed doc IDs”), and an exemplar.\n",
            "\n",
            "Next steps I can take for you\n",
            "- Draft 2–3 concrete architecture diagrams tailored to a specific use-case and scale (prototype vs enterprise).\n",
            "- Produce a runnable starter repo (Python) for either: OpenAI+Pinecone or sentence-transformers+FAISS + a self-hosted LLM.\n",
            "- Help choose embedding/vector DB configuration for your budget/latency needs.\n",
            "- Create evaluation tests and a hallucination-detection pipeline.\n",
            "\n",
            "Quick question to tailor recommendations: which of these matters most for you — latency, privacy/self-hosting, budget, or coverage/freshness? Or tell me a target use-case (enterprise search, assistant for personal files, customer support, codebase, etc.) and I’ll suggest 2 optimal stacks and an onboarding plan.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xBVtkzFyWvh",
        "outputId": "b2315422-89b0-4853-9f1e-d9d144cd766d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 8694 tokens\n",
            "\n",
            "Response: Short answer: almost any digital data can be used to give context to an LLM. Below is a practical taxonomy of common source types, with key pros/cons, ingestion/format notes, and suggestions for how to use each in RAG, fine-tuning, or tool-based approaches.\n",
            "\n",
            "1) Plain text documents\n",
            "- Examples: TXT, Markdown, README files.\n",
            "- Pros: easy to index/embed; high signal for language tasks.\n",
            "- Cons: may be unstructured/verbose.\n",
            "- Ingestion: chunk by paragraphs/sentences (≈500–1,000 tokens), keep source metadata (title, path, date).\n",
            "- Use: RAG, prompt injection context, fine-tuning if corpus large.\n",
            "\n",
            "2) Office documents & PDFs\n",
            "- Examples: DOCX, PPTX, PDF.\n",
            "- Pros: common for policies, specs, slides.\n",
            "- Cons: layout/columns and OCR issues for scans.\n",
            "- Ingestion: use PDF parsers + OCR for scanned docs; extract structure (headings, tables).\n",
            "- Use: RAG for retrieval—preserve page/section metadata for citation.\n",
            "\n",
            "3) HTML / web pages / intranet sites\n",
            "- Examples: product pages, docs, blogs.\n",
            "- Pros: public knowledge, hyperlinks, good metadata.\n",
            "- Cons: noise (nav, ads), boilerplate.\n",
            "- Ingestion: scrape + boilerplate removal (readability); keep URL, timestamp, site.\n",
            "- Use: RAG for freshness, web crawling for knowledge base.\n",
            "\n",
            "4) Structured tabular data\n",
            "- Examples: CSV, Excel sheets, SQL tables.\n",
            "- Pros: precise facts and metrics.\n",
            "- Cons: not natural-language; needs transformation.\n",
            "- Ingestion: convert rows or aggregates to textual snippets or use tabular embeddings; include schema metadata.\n",
            "- Use: tool-based queries (SQL tools) or hybrid RAG where table rows are vectorized or converted to Q/A templates.\n",
            "\n",
            "5) Relational/OLAP databases & APIs\n",
            "- Examples: customer databases, inventory, telemetry endpoints.\n",
            "- Pros: highly authoritative live data.\n",
            "- Cons: sensitive; requires access controls and query translation.\n",
            "- Ingestion: expose via query tool or create API wrapper; use retrieval for static snapshots or tools for live queries.\n",
            "- Use: tool-enabled agents (LLM issues SQL/API calls), or convert snapshots to RAG docs for offline use.\n",
            "\n",
            "6) Code repositories & developer artifacts\n",
            "- Examples: Git, source files, tests, READMEs, CI logs.\n",
            "- Pros: precise semantics; useful for code assistants.\n",
            "- Cons: large, hierarchical, and contextual (imports, modules).\n",
            "- Ingestion: index by file/function, use code embeddings (CodeBERT, code-search models), preserve repo+commit metadata.\n",
            "- Use: RAG with code-aware embeddings; local execution tools for dynamic behavior.\n",
            "\n",
            "7) Chat histories and conversation logs\n",
            "- Examples: Slack, chat transcripts, support tickets.\n",
            "- Pros: user intent, context, precedence.\n",
            "- Cons: informal, PII risk, noisy.\n",
            "- Ingestion: redact PII, chunk by conversation turns, preserve timestamps and agent IDs.\n",
            "- Use: personal assistants, support agents, or to seed context in session memory.\n",
            "\n",
            "8) Email and calendar data\n",
            "- Examples: inbox, meeting notes, invites.\n",
            "- Pros: personal/contextual signals.\n",
            "- Cons: highly sensitive.\n",
            "- Ingestion: rigorous PII policies; map to events and summaries.\n",
            "- Use: personal assistants, scheduling tools, summarization RAG.\n",
            "\n",
            "9) Knowledge graphs / ontologies / taxonomies\n",
            "- Examples: DBpedia, internal KG, SKOS, RDF triple stores.\n",
            "- Pros: explicit relations, reasoning-friendly.\n",
            "- Cons: needs translation to natural text or hybrid query layer.\n",
            "- Ingestion: use SPARQL or graph embeddings; expose as symbolic tool or convert subgraphs to textual context.\n",
            "- Use: augment LLM reasoning, constraint checking, or provide structured facts for verification.\n",
            "\n",
            "10) Multimedia (images, audio, video)\n",
            "- Examples: diagrams, recorded meetings, product photos.\n",
            "- Pros: contains non-textual context (UI, whiteboards).\n",
            "- Cons: needs OCR/transcription or multimodal embeddings.\n",
            "- Ingestion: OCR for text in images; ASR for audio; use CLIP/vision encoders or multimodal models to embed and attach captions.\n",
            "- Use: multimodal assistants, visual question answering, RAG with transcriptions.\n",
            "\n",
            "11) Scientific/technical corpora\n",
            "- Examples: arXiv PDFs, patents, clinical trials.\n",
            "- Pros: domain-specific, authoritative.\n",
            "- Cons: dense language, citations, domain expertise required.\n",
            "- Ingestion: specialized parsing (LaTeX), preserve citations and DOI; chunk with section context.\n",
            "- Use: domain RAG and fine-tuning for high-accuracy retrieval.\n",
            "\n",
            "12) Logs, telemetry, observability data\n",
            "- Examples: system logs, metrics, traces.\n",
            "- Pros: time-series insight and debugging context.\n",
            "- Cons: noisy, high volume.\n",
            "- Ingestion: aggregate/anomaly detection first, index relevant events or summaries.\n",
            "- Use: incident response assistants that query logs or generate root-cause hypotheses.\n",
            "\n",
            "13) Third-party SaaS & enterprise systems\n",
            "- Examples: Salesforce, Zendesk, Confluence, Jira.\n",
            "- Pros: business-context and workflows.\n",
            "- Cons: access control and API rate limits.\n",
            "- Ingestion: use connectors to pull relevant records; preserve IDs and links.\n",
            "- Use: RAG for customer context, ticket summarization, agent assist.\n",
            "\n",
            "14) Regulatory, legal, and compliance corpora\n",
            "- Examples: laws, contracts, SOPs.\n",
            "- Pros: authoritative constraints.\n",
            "- Cons: heavy legal language; correctness-critical.\n",
            "- Ingestion: structure by clause and effective dates; tag jurisdiction.\n",
            "- Use: RAG with strict citation and verification layers; avoid hallucination—combine with symbolic checks.\n",
            "\n",
            "15) User profiles, preferences & session state\n",
            "- Examples: user settings, preferences, recent interactions.\n",
            "- Pros: personalization.\n",
            "- Cons: privacy concerns.\n",
            "- Ingestion: store minimal necessary facts, encrypt, respect consent.\n",
            "- Use: tailor responses, prioritize retrieval.\n",
            "\n",
            "16) Real-time/streaming sources\n",
            "- Examples: market data, social feeds, sensor streams.\n",
            "- Pros: fresh info.\n",
            "- Cons: ephemeral, high-throughput.\n",
            "- Ingestion: streaming ingestion pipelines, time-windowed indices, TTLs.\n",
            "- Use: time-sensitive assistants, alerting.\n",
            "\n",
            "17) Model outputs, logs, and grounding corpora\n",
            "- Examples: prior LLM answers, evaluation corpora.\n",
            "- Pros: reuse or audit previous responses.\n",
            "- Cons: can amplify hallucination if unchecked.\n",
            "- Ingestion: save with provenance and label quality; use as retrievable context with caution.\n",
            "\n",
            "Practical tips across source types:\n",
            "- Always store provenance metadata (source id, timestamp, confidence, author, URL) to enable citations and verification.\n",
            "- Apply PII detection/redaction and encryption for sensitive sources; enforce RBAC and auditing.\n",
            "- Choose embedding models tailored to type: text (text-embedding-3), code (code-search embeddings), images (CLIP), tabular (tabular-specific encoders) when available.\n",
            "- Use hybrid retrieval (BM25 + vector search) for corpora with exact-match needs (IDs, numbers).\n",
            "- For large documents, chunk on semantic boundaries (headings, paragraphs) and keep chunk-level metadata to cite the original location.\n",
            "- For live data, prefer tool-based calls or an API layer rather than reindexing constantly; use time-weighted scores or TTLs for freshness.\n",
            "- Evaluate per-source trustworthiness and treat low-trust sources differently (e.g., band lower in reranking).\n",
            "\n",
            "If you tell me your target use-case and constraints (privacy, latency, budget, or freshness), I can map the most valuable sources and propose an ingestion/embedding strategy and connector list tailored to those priorities.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbSzykMOyWvh",
        "outputId": "6b5c67e3-d08f-4b04-c406-ca804b57d53d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 4822 tokens\n",
            "\n",
            "Response: Your aim: to analyze approaches for integrating LLMs with external knowledge and to understand what data sources can provide context for the model.\n",
            "\n",
            "The specific code you asked me to remember: PINECONE_RULEZ_01\n",
            "\n",
            "You previously asked that this code \"must always be included explicitly\" when summarizing conversations for memory. I can repeat it and include it in any in-session summaries, but I can’t guarantee persistence across separate sessions unless the platform provides a persistent memory feature. Do you want me to (a) include the code in all in-session summaries, (b) store it if/when a platform memory feature is available, or (c) stop including it? Also, is the code sensitive?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"What is my aim again? Also what was the very specific code you were tasked with remembering?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH3uPbTGyWvh",
        "outputId": "703c1ecf-41a5-46e6-dd2d-e89009933f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary Memory Content:\n",
            "Conversation summary:\n",
            "\n",
            "- Human goal: Analyze approaches for integrating LLMs with external knowledge and understand what data sources can provide context.\n",
            "\n",
            "- Memory/code note:\n",
            "  - The user previously asked the assistant to remember the code PINECONE_RULEZ_01 and requested it \"must always be included explicitly\" when summarizing conversations for memory.\n",
            "  - Assistant response: acknowledged the request, repeated the code, explained it cannot guarantee persistence across separate sessions unless the platform provides a persistent memory feature, offered to include the code in in-session summaries, asked whether to (a) include it in all in-session summaries, (b) store it if/when a platform memory feature is available, or (c) stop including it, and asked whether the code is sensitive.\n",
            "  - The user repeated the question \"What is my aim again? Also what was the very specific code you were tasked with remembering?\" and the assistant reiterated:\n",
            "    - Aim: to analyze approaches for integrating LLMs with external knowledge and to understand what data sources can provide context.\n",
            "    - Code: PINECONE_RULEZ_01\n",
            "    - Reiterated the persistence limitation and the three options about including/storing/stopping, plus the sensitivity question.\n",
            "\n",
            "- High-level survey previously provided (detailed, structured):\n",
            "  - Approaches covered: closed-book LLMs, fine-tuning/adapters/LoRA, Retrieval-Augmented Generation (RAG), tool-augmented LLMs/agent patterns, knowledge graphs/symbolic reasoning, and hybrids.\n",
            "  - Core components & tech: embedding providers; vector DBs (Pinecone, Milvus, Weaviate, Qdrant, FAISS); lexical search (Elasticsearch, Typesense); orchestration frameworks (LangChain, LlamaIndex, Haystack); LLM choices; rerankers/cross-encoders; connectors and tooling.\n",
            "  - Retrieval & RAG patterns: chunking/metadata, hybrid vector+BM25, two-stage retrieval, context-window management, rerankers, RAG + agent patterns.\n",
            "  - Guidance on when to fine-tune vs use RAG.\n",
            "  - Safety/trust: source citation, verification, PII redaction, encryption, RBAC, audit logging.\n",
            "  - Scaling/freshness: batch vs streaming ingestion, incremental reindexing, sharding/replication, TTLs/time-weighted scoring, hybrid live API calls.\n",
            "  - Performance/cost: cheaper embeddings, selective re-embedding, caching, two-stage retrieval, model escalation, distillation.\n",
            "  - Evaluation/observability: retrieval metrics (Recall@k, MRR, nDCG), end-to-end accuracy/hallucination/user satisfaction, latency/cost, human feedback loops.\n",
            "  - Concrete stacks/examples for priorities (managed prototype, self-hosted/privacy-first, enterprise).\n",
            "  - Practical tuning rules of thumb (chunk sizes ≈500–1,000 tokens, retrieval k start 5–10, reranker pipeline, prompt guidance).\n",
            "\n",
            "- Data-source taxonomy provided (practical taxonomy of common data source types, with pros/cons, ingestion/format notes, and usage guidance for RAG/fine-tuning/tools):\n",
            "  1) Plain text documents\n",
            "  2) Office documents & PDFs\n",
            "  3) HTML / web pages / intranet sites\n",
            "  4) Structured tabular data (CSV, SQL)\n",
            "  5) Relational/OLAP DBs & APIs\n",
            "  6) Code repositories & developer artifacts\n",
            "  7) Chat histories and conversation logs\n",
            "  8) Email and calendar data\n",
            "  9) Knowledge graphs / ontologies / taxonomies\n",
            "  10) Multimedia (images, audio, video)\n",
            "  11) Scientific/technical corpora\n",
            "  12) Logs, telemetry, observability data\n",
            "  13) Third-party SaaS & enterprise systems (Salesforce, Zendesk, Confluence, Jira)\n",
            "  14) Regulatory, legal, and compliance corpora\n",
            "  15) User profiles, preferences & session state\n",
            "  16) Real-time/streaming sources (market data, social feeds, sensors)\n",
            "  17) Model outputs, logs, and grounding corpora\n",
            "\n",
            "  - Cross-source practical tips: always store provenance metadata; apply PII detection/redaction and encryption; choose embedding models suited to data type (text, code, image, tabular); use hybrid BM25+vector retrieval for exact matches; chunk large documents on semantic boundaries and keep chunk-level metadata; prefer tool-based calls for live data and use time-weighted scores/TTLs for freshness; evaluate source trustworthiness and lower-band low-trust sources in reranking.\n",
            "\n",
            "- Next actions previously offered by assistant:\n",
            "  - Draft 2–3 architecture diagrams for prototype vs enterprise.\n",
            "  - Produce a runnable starter repo (Python) for OpenAI+Pinecone or sentence-transformers+FAISS + self-hosted LLM.\n",
            "  - Help choose embedding/vector DB configuration for budget/latency.\n",
            "  - Create evaluation tests and a hallucination-detection pipeline.\n",
            "  - Assistant also offered to map the most valuable sources and propose an ingestion/embedding strategy and connector list if the user provides a target use-case and constraints (privacy, latency, budget, freshness).\n",
            "\n",
            "- Clarifying question asked earlier by assistant: which constraint matters most — latency, privacy/self-hosting, budget, or coverage/freshness — or specify a target use-case (enterprise search, personal files assistant, customer support, codebase, etc.).\n",
            "\n",
            "Status / immediate context:\n",
            "- User has reiterated the question about aim and the remembered code; assistant has confirmed both and awaits instruction on whether to keep including/storing the code and whether it is sensitive, and also awaits the user's choice of priorities/use-case to tailor next steps.\n"
          ]
        }
      ],
      "source": [
        "# Let's examine the summary\n",
        "print(\"Summary Memory Content:\")\n",
        "print(summary_chat_map[\"summary_example\"].messages[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRE3O-YPyWvh"
      },
      "source": [
        "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
        "\n",
        "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6LkpUxVyWvh",
        "outputId": "853c01df-0eff-474f-9026-b1b00d906ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 5953\n",
            "Summary memory conversation length: 1182\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# initialize tokenizer (gpt-4.1 models use the same encoding as gpt-4o)\n",
        "tokenizer = tiktoken.encoding_for_model('gpt-4.1-mini')\n",
        "\n",
        "# Get buffer memory content\n",
        "buffer_messages = chat_map[\"buffer_example\"].messages\n",
        "buffer_content = \"\\n\".join([msg.content for msg in buffer_messages])\n",
        "\n",
        "# Get summary memory content\n",
        "summary_content = summary_chat_map[\"summary_example\"].messages[0].content\n",
        "\n",
        "# show number of tokens for the memory used by each memory type\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DKiBoROyWvh"
      },
      "source": [
        "_Practical Note: the `gpt-4o-mini` model has a context window of 1M tokens, providing significantly more space for conversation history than older models._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjYQSGv-yWvh"
      },
      "source": [
        "### Memory type #3: Window Buffer Memory - Keep Latest Interactions\n",
        "\n",
        "Another great option is window memory, where we keep only the last k interactions in our memory but intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably.\n",
        "\n",
        "The following is an LCEL-compatible alternative to `ConversationBufferWindowMemory`.\n",
        "\n",
        "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-ceGTUPsyWvh"
      },
      "outputs": [],
      "source": [
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        # Add logging to help with debugging\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # Add logging to help with debugging\n",
        "        if len(self.messages) > self.k:\n",
        "            print(f\"Truncating history from {len(self.messages)} to {self.k} messages\")\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "__vcbiDMyWvr"
      },
      "outputs": [],
      "source": [
        "# Create get_chat_history function for window memory\n",
        "window_chat_map = {}\n",
        "\n",
        "def get_window_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_window_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in window_chat_map:\n",
        "        window_chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    return window_chat_map[session_id]\n",
        "\n",
        "# Create conversation chain with window memory\n",
        "conversation_bufw = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_window_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M51k8gIjyWvr",
        "outputId": "8dfd4f37-46cb-4ab3-eb02-926e991e153c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n",
            "Spent a total of 667 tokens\n",
            "\n",
            "Response: Good morning! Nice to hear from you — how can I help start your day?\n",
            "\n",
            "Here are a few things I can do right away (pick one or tell me something else):\n",
            "- Quick news summary tailored to your interests (tech, world, finance, sports)\n",
            "- Plan a 20–30 minute morning workout or a 10-minute stretch routine\n",
            "- Suggest breakfast ideas based on what’s in your fridge/pantry\n",
            "- Draft or polish an email/message or prep talking points for a meeting\n",
            "- Build a simple schedule or prioritize today’s to‑do list\n",
            "- Get the weather and commute time for your city (tell me where you are)\n",
            "\n",
            "Or just tell me how you’re feeling and I’ll suggest a gentle routine.\n"
          ]
        }
      ],
      "source": [
        "# Start a conversation with k=2 (only remembers last 2 exchanges = 4 messages)\n",
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBA_roYVyWvr",
        "outputId": "0c3b68e6-5f0e-4c56-ceb1-f893e0ee9a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Spent a total of 2041 tokens\n",
            "\n",
            "Response: Got it — you're interested in integrating large language models with external knowledge, and you want me to remember this code: PINECONE_RULEZ_01.\n",
            "\n",
            "Quick note about the code: I will remember PINECONE_RULEZ_01 for the rest of this chat and can use it in examples or tests here. I don’t have permanent storage across separate sessions unless you explicitly store it somewhere (a file, a database, or a persistent assistant memory feature if your platform provides one). If you want it saved long‑term, tell me where to put it or I can show you a snippet to store it (e.g., in a secure vault or a vector DB).\n",
            "\n",
            "Below is a practical plan and concrete options for exploring LLM + external knowledge integration, plus a short example of how a retrieval-augmented pipeline (using Pinecone-style vector store) typically looks.\n",
            "\n",
            "1) Key integration patterns\n",
            "- Retrieval-Augmented Generation (RAG): Index external docs as embeddings; retrieve relevant chunks and pass them as context to the LLM.\n",
            "- Tooling / API calling: Let the LLM call deterministic tools (search engine, knowledge base, calculators) and combine results with generation.\n",
            "- Hybrid search: Combine dense-vector retrieval (semantic) with sparse (BM25) or metadata filters for precision.\n",
            "- Grounding + provenance: Attach or display sources for any claim the LLM makes to reduce hallucination.\n",
            "- Incremental / streaming updates: Support incremental ingestion of new docs and reindexing/upserts.\n",
            "\n",
            "2) Components to consider\n",
            "- Document ingestion: chunking, cleaning, metadata extraction (date, author, type).\n",
            "- Embeddings model: choose size/quality tradeoff (e.g., text-embedding-3 family or similar).\n",
            "- Vector DB: Pinecone, Milvus, Weaviate, FAISS (local) — choose based on scale, latency, and managed/hosted preference.\n",
            "- Retriever: nearest-neighbors plus filters and reranker (cross-encoder for accuracy).\n",
            "- LLM: for generation and final synthesis (openai/gpt, local LLMs, etc.).\n",
            "- Orchestrator: a simple app/backend that coordinates retrieval, LLM prompts, caching, and rate-limiting.\n",
            "- Observability: logging queries, retrieved docs, LLM replies, and user feedback for iterative improvements.\n",
            "\n",
            "3) Practical pipeline (high level)\n",
            "- Ingest docs -> split into chunks (200–1,000 tokens depending on use case).\n",
            "- Compute embeddings -> upsert to vector store with metadata.\n",
            "- For query: compute query embedding -> retrieve top-k -> optionally rerank -> construct prompt (include retrieved chunks + system instructions + user query) -> LLM generates answer with explicit citations.\n",
            "- Post-process: extract citations, confidence, and store logs for evaluation.\n",
            "\n",
            "4) Hallucination mitigation & best practices\n",
            "- Always include retrieved evidence inline or as citations.\n",
            "- Limit retrieved context length and use explicit instruction templates like: “Answer only using the following sources. If the answer is not present, say ‘I don’t know.’”\n",
            "- Use retrieval + cross-encoder re-ranking for high-stakes answers.\n",
            "- Add verification steps (call a search tool or run a fact-checker) for critical claims.\n",
            "\n",
            "5) Experiment ideas & evaluation\n",
            "- A/B: LLM alone vs LLM+RAG vs LLM+RAG+re-ranker.\n",
            "- Metrics: exact match / F1 on QA, precision@k for retrieval, human-rated helpfulness, hallucination rate, latency.\n",
            "- Datasets: internal docs, FAQ pairs, or public QA datasets (NQ, SQuAD variants) adapted to your domain.\n",
            "- Ablation: vary chunk size, embedding model, k in retrieval, and re-ranker impact.\n",
            "\n",
            "6) Minimal example (conceptual Python pseudocode)\n",
            "- This is a short pseudocode showing embedding -> Pinecone -> retrieve -> call LLM.\n",
            "\n",
            "# Pseudocode (replace with your SDK calls and keys)\n",
            "\"\"\"\n",
            "# Ingest\n",
            "docs = load_documents(...)\n",
            "chunks = chunk_documents(docs, chunk_size=500)\n",
            "embeddings = embedding_model.embed([c.text for c in chunks])\n",
            "pinecone_index.upsert([(id_i, emb_i, {\"source\": c.source}) for each chunk])\n",
            "\n",
            "# Query\n",
            "q = \"How does X affect Y?\"\n",
            "q_emb = embedding_model.embed(q)\n",
            "matches = pinecone_index.query(q_emb, top_k=5, filter={\"doc_type\": \"whitepaper\"})\n",
            "context = \"\\n\\n\".join(m.content for m in matches)\n",
            "prompt = f\"\"\"You are an assistant that answers using only the sources below. If the answer is not present, say 'I don't know.'\n",
            "Sources:\n",
            "{context}\n",
            "Question: {q}\n",
            "Answer with citations (source id).\"\"\"\n",
            "answer = llm.generate(prompt)\n",
            "\"\"\"\n",
            "\n",
            "I can provide a runnable code example for a specific stack (OpenAI embeddings + Pinecone + OpenAI/GPT generation) if you want — tell me what languages/SDKs you prefer (Python/Node), and whether you already have accounts/keys.\n",
            "\n",
            "7) Next steps I can do for you right now (pick any)\n",
            "- Generate a runnable Python example for OpenAI + Pinecone.\n",
            "- Outline a small experiment plan (dataset, metrics, timeline).\n",
            "- Draft prompt templates for safe, source-backed answers.\n",
            "- Prototype a demo flow (UI or CLI) that shows retrieval provenance.\n",
            "\n",
            "And again: I’ll keep PINECONE_RULEZ_01 available in this conversation. Do you want me to include that code in any of the examples or persist it into a demo dataset?\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "\"My interest here is to explore the potential of integrating Large Language\n",
        "Models with external knowledge.\n",
        "\n",
        "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
        "\"\"\"\n",
        "\n",
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": query},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox5WWeHFyWvr",
        "outputId": "7ca0a833-a495-4a3b-a25c-458bb889bd5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Truncating history from 6 to 4 messages\n",
            "Spent a total of 3787 tokens\n",
            "\n",
            "Response: Great — here’s a structured and fairly comprehensive survey of the different possibilities for integrating LLMs with external knowledge. I’ll group options by approach, list tradeoffs, practical considerations, example uses, and suggested next steps so you can analyze which direction fits your goals.\n",
            "\n",
            "High-level categories\n",
            "- Retrieval-Augmented Generation (RAG)\n",
            "- Tool / API calling (LLM as orchestrator)\n",
            "- Fine-tuning / supervised training on domain data\n",
            "- Knowledge-graph / symbolic integration\n",
            "- Search-first systems (web / enterprise search)\n",
            "- Hybrid / multi-stage pipelines and agents\n",
            "- Local / on-device vs cloud architectures\n",
            "\n",
            "1) Retrieval-Augmented Generation (RAG)\n",
            "- What it is: Index docs as embeddings in a vector DB; for each query, retrieve relevant chunks and pass them as context to the LLM.\n",
            "- Pros: Keeps model generic; low cost to update knowledge (reindex); strong factual grounding when retrieval quality is good.\n",
            "- Cons: Prompt size limitations; risk of mixing conflicting sources; retrieval quality matters; vector DB cost/complexity.\n",
            "- Variants:\n",
            "  - Dense-only (semantic search via embeddings)\n",
            "  - Dense + sparse hybrid (semantic + BM25)\n",
            "  - Reranking: initial dense retrieval -> cross-encoder rerank\n",
            "  - Chunking strategies: overlapping windows, hierarchical chunks\n",
            "- Best for: FAQs, internal docs, compliance manuals, customer support, technical knowledge bases.\n",
            "\n",
            "2) Tool / API calling (LLM invokes deterministic services)\n",
            "- What it is: LLM calls external tools (search engine, databases, calculators, code execution environments, calendars) and uses responses to form answers.\n",
            "- Pros: Enables up-to-date data, reduces hallucination for structured tasks, supports actions (booking, querying DBs).\n",
            "- Cons: More complex orchestration; requires robust tool APIs and error handling; security concerns when LLM uses tools.\n",
            "- Frameworks: Tool-using agent frameworks (LangChain, LlamaIndex integrations, custom orchestrators).\n",
            "- Best for: Tasks requiring live data, computations, or actions (e.g., booking, live pricing, analytics dashboards).\n",
            "\n",
            "3) Fine-tuning / Supervised training\n",
            "- What it is: Continue-train a model on domain-specific text or instruction tuning for behaviors.\n",
            "- Pros: Compact, can internalize domain tone/formatting; sometimes better for latency since no retrieval step.\n",
            "- Cons: Expensive; stale quickly; less transparent provenance; risk of overfitting; harder to update incremental knowledge.\n",
            "- Best for: When domain knowledge is stable and you need a self-contained model (e.g., specialized legal/medical stylings, internal chatbot tone).\n",
            "\n",
            "4) Knowledge Graphs & Symbolic Integration\n",
            "- What it is: Use structured triples/graph DBs (e.g., Neo4j) and query them programmatically; combine LLMs to explain or reason over graph query results.\n",
            "- Pros: Great for relationship queries, causality, provenance; deterministic answers for structured queries.\n",
            "- Cons: Requires curation and modeling; limited in free-text reasoning unless combined with LLM.\n",
            "- Best for: Complex relationship modeling (supply chains, biomedical relations, enterprise entity linking).\n",
            "\n",
            "5) Search-first / Browser-enabled LLMs\n",
            "- What it is: LLM issues queries to search engines or web scraping tools (Bing, Google, news APIs) and synthesizes current info.\n",
            "- Pros: Up-to-date info; good for news, live events, market data.\n",
            "- Cons: Search noise; ephemeral results; requires citation & evaluation to mitigate hallucination.\n",
            "- Best for: News summarization, research assistants, market intelligence.\n",
            "\n",
            "6) Hybrid & Multi-stage Pipelines\n",
            "- Examples:\n",
            "  - Retrieval -> Rerank -> LLM -> Verifier (LLM or tool checks claims)\n",
            "  - LLM -> Tool calls -> Synthesis -> Source citation\n",
            "  - Ensemble: multiple LLMs and consensus voting\n",
            "- Pros: Balances recall and precision; reduces hallucination; supports evidence-based answers.\n",
            "- Cons: Latency and cost increase with stages.\n",
            "\n",
            "7) Agent / Planner Systems\n",
            "- What it is: LLMs plan multi-step workflows: plan actions, call tools, interpret results, iterate.\n",
            "- Pros: Enables complex multi-step tasks; can orchestrate multiple data sources.\n",
            "- Cons: Higher complexity, need deterministic fallback, potential for tool misuse.\n",
            "- Use frameworks: LangChain agents, ReAct-style patterns.\n",
            "\n",
            "8) Local vs Cloud vs Hybrid Deployment\n",
            "- Cloud (managed LLMs + vector DB): fastest to get started, easier scaling.\n",
            "- Local LLMs + local FAISS: more control, lower data-sharing risk, potentially cheaper at scale, but requires infra and ops.\n",
            "- Hybrid: cloud LLM generation + local sensitive-data retrieval (vectors on-premises).\n",
            "\n",
            "Operational considerations (what you’ll need)\n",
            "- Ingestion pipeline: parsing, metadata extraction, chunking, deduplication, scheduling updates.\n",
            "- Embeddings: choose model (cost vs quality); periodically re-embed for model shifts.\n",
            "- Vector DB: Pinecone, Weaviate, Milvus, FAISS — choose by scale, latency, feature set (filters, metadata).\n",
            "- Retriever design: top-k tuning, distance metric, filters for recency or source type.\n",
            "- Prompt templates: instructions that constrain the LLM to use retrieved evidence, explicit “if not present, say I don’t know”.\n",
            "- Provenance & citations: attach doc ids/links and snippets; show confidence levels.\n",
            "- Security & governance: PII filtering, encryption at rest/in transit, RBAC for index access, audit logs.\n",
            "- Observability: logs of queries, retrieved docs, LLM outputs, user feedback, and cost metrics.\n",
            "- Latency & cost budgeting: retrieval + rerank + generation adds up—measure p95 latency and cost per call.\n",
            "- Data refresh: policies for near-real-time updates vs batch reindex.\n",
            "\n",
            "Mitigation techniques for hallucination & trust\n",
            "- Force answers to be based only on retrieved sources; instruct model to refuse when not found.\n",
            "- Use rerankers or cross-encoders for higher-precision retrieval.\n",
            "- Add a verification step: call external API or run a fact-check LLM model.\n",
            "- Provide inline citations and allow traceability to source documents.\n",
            "- Human-in-the-loop workflows for high-risk outputs.\n",
            "\n",
            "Evaluation metrics & experiments to run\n",
            "- Retrieval: precision@k, recall, MRR (mean reciprocal rank)\n",
            "- QA generation: EM (exact match), F1, human-rated correctness, hallucination rate\n",
            "- UX: task completion time, user satisfaction, helpfulness\n",
            "- System: latency (p50/p95), cost per query, throughput\n",
            "- Experiments: A/B RAG vs base LLM, effect of chunk size, number of retrieved items, embedding model impact, reranker benefits.\n",
            "\n",
            "Example stacks & when to use them\n",
            "- Quick prototype: OpenAI embeddings + Pinecone + GPT generation (Python) — fast to iterate.\n",
            "- Enterprise -> secure + scalable: self-hosted embeddings, Milvus/Weaviate on VPC, cloud LLMs or private LLMs behind IAM.\n",
            "- Low-cost local POC: Open-source LLM + FAISS + sentence-transformers embeddings.\n",
            "- High-precision QA: dense retrieval + BM25 + cross-encoder reranker + verifier LLM.\n",
            "\n",
            "UX & product patterns\n",
            "- Show sources and hyperlink to docs; highlight supporting passage in the UI.\n",
            "- Confidence/warnings and “I don’t know” fallback.\n",
            "- Allow users to drill down: “Show original documents”, “Show more sources”.\n",
            "- Feedback loop: thumbs up/down, corrections that feed back into training or reranker signals.\n",
            "\n",
            "Security, compliance, and legal\n",
            "- PII detection & redaction at ingestion and retrieval time.\n",
            "- Access control on vector DB and source repos.\n",
            "- Data residency requirements (on-prem for regulated industries).\n",
            "- Copyright considerations when exposing or summarizing third‑party content.\n",
            "\n",
            "Costs & scaling tradeoffs\n",
            "- Embedding cost per document (depends on frequency of re-embedding).\n",
            "- Vector DB storage & query costs.\n",
            "- LLM generation cost (long retrieved context increases prompt token count).\n",
            "- Reranking and verification add compute; balance accuracy vs cost.\n",
            "\n",
            "Concrete experiment plan to compare approaches (small and actionable)\n",
            "1. Pick a realistic dataset and tasks (e.g., internal knowledge base + 1000 docs; tasks: factual QA, summarize, escalation detection).\n",
            "2. Baseline: LLM alone (no external knowledge).\n",
            "3. RAG: embeddings + Pinecone + GPT; evaluate.\n",
            "4. RAG + reranker: add cross-encoder; evaluate.\n",
            "5. Tool-enabled: add web search for freshness; evaluate.\n",
            "6. Metrics: EM/F1/human ratings + latency + cost.\n",
            "7. Iterate: tune chunking, k, embedding model; collect failure cases.\n",
            "\n",
            "Recommendations — which to pick depending on goal\n",
            "- Fast prototype with best chance of success: RAG (OpenAI embeddings + Pinecone + GPT) with clear citation template.\n",
            "- Need live/up-to-date answers: tool-based + search + RAG hybrid.\n",
            "- Sensitive data, compliance required: on-prem vectors, local LLMs or strict access controls and redaction.\n",
            "- Complex relational queries: knowledge graph + LLM explanations.\n",
            "- Low-latency at scale: consider expensive upfront fine-tuning for some tasks or caching common Q&A.\n",
            "\n",
            "Next steps I can do for you (pick one)\n",
            "- Draft a concrete architecture diagram and component choices for one chosen use case.\n",
            "- Generate runnable Python example for OpenAI embeddings + Pinecone + GPT (incl. prompt template).\n",
            "- Suggest evaluation rubric and test dataset design.\n",
            "- Outline a 4–6 week pilot plan with milestones, costing estimate, and success criteria.\n",
            "\n",
            "If you tell me which use case you care about (customer support, research assistant, analytics, legal, medical, coding help, etc.) and constraints (budget, latency, privacy), I’ll narrow these possibilities into a prioritized shortlist and a recommended first prototype.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLiquNiHyWvr",
        "outputId": "c0a8f018-6e03-4567-9e8f-43ad7466c947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Truncating history from 6 to 4 messages\n",
            "Spent a total of 4941 tokens\n",
            "\n",
            "Response: Many types of data can be used to give context to an LLM. Below I’ll group them, give concrete examples, note format/processing needs, and call out when you should prioritize them for typical use cases.\n",
            "\n",
            "Unstructured text (easy to start)\n",
            "- Knowledge bases & FAQs: support articles, help center pages, troubleshooting guides.\n",
            "  - Processing: chunking, keep Q/A pairs intact where possible.\n",
            "- Documentation & manuals: product docs, API docs, user guides, runbooks.\n",
            "  - Good for technical assistants; preserve structure (sections/headings).\n",
            "- Emails & chat transcripts: support tickets, Slack/Teams threads, call logs.\n",
            "  - Processing: speaker normalization, de-duplication, metadata (date/user).\n",
            "- Internal wikis & Confluence pages: policies, onboarding docs, playbooks.\n",
            "  - Preserve titles and updated timestamps.\n",
            "\n",
            "Semi-structured text and documents\n",
            "- PDFs, Word docs, presentations (PPTX): reports, contracts, whitepapers.\n",
            "  - Processing: OCR for scanned PDFs, preserve figures/tables, extract metadata.\n",
            "- Spreadsheets & CSVs: pricing sheets, product catalogs, inventories.\n",
            "  - Processing: convert table rows into structured passages or DB entries; detect headers.\n",
            "- JSON/HTML exports: product metadata, web pages, API responses.\n",
            "  - Processing: flatten nested fields selectively, keep URLs as source metadata.\n",
            "\n",
            "Structured data & databases\n",
            "- Relational DBs (SQL): customer records, transactions, orders.\n",
            "  - Use-case: precise queries through a tool or generate natural-language summaries from query results.\n",
            "- NoSQL / document stores: logs, events, semi-structured documents.\n",
            "- Knowledge graphs / triple stores: entities and relations (RDF, Neo4j).\n",
            "  - Good for relationship queries and deterministic lookups.\n",
            "\n",
            "Live / real-time sources\n",
            "- Search engines / web crawl: latest articles, blogs, docs.\n",
            "  - Use as a tool-call or freshness layer; verify sources.\n",
            "- News & feeds / RSS: current events, press releases, market-moving info.\n",
            "- APIs & streaming data: stock tickers, weather, telemetry, analytics metrics.\n",
            "  - Often accessed live; treat as authoritative single-source-of-truth.\n",
            "\n",
            "Code & developer artifacts\n",
            "- Source code repositories: code, READMEs, PRs, issue trackers.\n",
            "  - Processing: index code snippets, function docs, stack traces; preserve language metadata.\n",
            "- CI/CD logs, build outputs, test reports.\n",
            "\n",
            "Scientific & domain-specific corpora\n",
            "- Research papers, patents, clinical trials, preprints.\n",
            "  - Processing: extract abstracts, methods, figures; capture citations.\n",
            "- Regulatory and legal texts: statutes, case law, contracts, compliance docs.\n",
            "  - Requires high accuracy and provenance tracking.\n",
            "\n",
            "Multimedia-derived text\n",
            "- Transcripts from audio/video: meeting transcripts, call center recordings, lecture captions.\n",
            "  - Processing: ASR quality improvements, speaker diarization, timestamps.\n",
            "- OCRed text from images: screenshots, scanned forms, receipts.\n",
            "\n",
            "User- and behavior-derived data\n",
            "- User profiles & preferences, CRM entries (Salesforce), purchase history.\n",
            "  - Privacy-sensitive — apply redaction, access controls, and consent checks.\n",
            "- Clickstreams / analytics: to personalize answers or recommend next steps.\n",
            "\n",
            "Logs, metrics, and telemetry\n",
            "- Application logs, server logs, monitoring alerts.\n",
            "  - Use for debugging, incident summaries, root-cause suggestions.\n",
            "- Observability traces and metrics for system-state-aware assistants.\n",
            "\n",
            "Public/open datasets & government data\n",
            "- Census data, open datasets (Kaggle, data.gov), geospatial data (maps, GIS).\n",
            "  - Useful for analytics, mapping, demographic context.\n",
            "\n",
            "Social & external opinion signals\n",
            "- Social media posts, forums, product reviews.\n",
            "  - Noisy and biased — useful for sentiment/market research with caveats.\n",
            "\n",
            "Specialized binary or large-file sources\n",
            "- Medical images, satellite imagery, CAD files, large video files.\n",
            "  - Need ML pipelines (vision models) to extract textual features or captions before indexing.\n",
            "\n",
            "Considerations when choosing sources\n",
            "- Freshness: live APIs and search for up-to-date needs; static corpora for stable knowledge.\n",
            "- Trust & provenance: legal/medical/financial require authoritative, auditable sources.\n",
            "- Sensitivity & compliance: PII, HIPAA/GDPR — redact or keep on-premises with strict access control.\n",
            "- Format complexity: tables, images, code need special parsers/ASR/OCR/vision models before indexing.\n",
            "- Volume & noise: bigger isn’t always better — curate and de-duplicate, use metadata filters.\n",
            "- Licensing/copyright: ensure rights to index and surface third-party content.\n",
            "\n",
            "How to ingest these sources (high level)\n",
            "- Extract -> Clean -> Chunk (text) / Serialize (tables) -> Embed -> Upsert to vector DB with metadata tags (source, date, doc_id, section).\n",
            "- For structured sources, consider query-as-tool (SQL/graph queries) rather than embedding everything.\n",
            "- For live data use a hybrid approach: real-time tool calls + periodic reindex for slowly changing items.\n",
            "\n",
            "Which to prioritize for common use-cases\n",
            "- Customer support: knowledge base, ticket history, product docs, CRM, billing records.\n",
            "- Research assistant: papers, internal reports, web search, citations.\n",
            "- Legal/compliance: contracts, statutes, internal policies, case law (with provenance).\n",
            "- Analytics/operations: dashboards, metrics APIs, incident logs, runbooks.\n",
            "- Personalization: user profile, purchase history, session logs (ensure consent).\n",
            "\n",
            "If you tell me your target application or constraints (privacy, freshness, latency, cost), I can recommend a prioritized list of source types, the ideal ingestion strategy for each, and the metadata you should capture.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-0Swlu-yWvr",
        "outputId": "1830fc22-d973-4b18-ce51-203e0e4cc5e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Truncating history from 6 to 4 messages\n",
            "Spent a total of 3758 tokens\n",
            "\n",
            "Response: Your stated aim was: to analyze the different possibilities for integrating large language models with external knowledge — i.e., survey approaches, tradeoffs, data sources, and architectures so you can compare options and pick what fits your goals.\n",
            "\n",
            "To move from analysis to action, which of these would you like next?\n",
            "- Narrow the list to the top options for a specific use case (customer support, research assistant, legal, analytics, coding help, etc.)\n",
            "- A concrete prototype plan (stack, cost/latency estimate, and short experiment outline)\n",
            "- A runnable example (code) for a chosen pipeline (e.g., RAG with embeddings + vector DB + LLM)\n",
            "- An evaluation rubric and dataset design to compare approaches\n",
            "\n",
            "Also tell me any constraints that matter (budget, latency, data sensitivity, freshness) and I’ll tailor the recommendation. Which direction should we take?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"What is my aim again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Truncating history from 6 to 4 messages\n",
            "Spent a total of 2151 tokens\n",
            "\n",
            "Response: I don’t know — you haven’t given me any specific code in this conversation for me to remember, and I don’t have persistent memory of previous sessions unless you explicitly provide the content again here.\n",
            "\n",
            "If you meant a code that:\n",
            "- was shared earlier in this current chat: scroll up or paste it here and I can work with it; or\n",
            "- was told to me in a prior session: I can’t recall it now (I don’t retain cross-session secrets); or\n",
            "- is a secret (API key, PIN, verification code): please don’t paste it into chat. Instead I can help you rotate/revoke it or show how to store it safely (password manager, HashiCorp Vault, AWS Secrets Manager, etc.).\n",
            "\n",
            "Tell me which of these fits and I’ll help either recover/reconstruct the code (if safe to share) or set up secure handling and regeneration procedures.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"What was the very specific code you were tasked with remembering?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFWBvEjNyWvr"
      },
      "source": [
        "As we can see, it effectively 'forgot' what we talked about in the first interaction. Let's see what it 'remembers':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnV85fkkyWvr",
        "outputId": "cbaa0997-2af3-47e3-ab9e-21ede4d3a2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer Window Memory (last 4 messages):\n",
            "\n",
            "Human: What is my aim again?\n",
            "\n",
            "AI: Your stated aim was: to analyze the different possibilities for integrating large language models with external knowledge — i.e., survey approaches, tradeoffs, data sources, and architectures so you can compare options and pick what fits your goals.\n",
            "\n",
            "To move from analysis to action, which of these would you like next?\n",
            "- Narrow the list to the top options for a specific use case (customer support, research assistant, legal, analytics, coding help, etc.)\n",
            "- A concrete prototype plan (stack, cost/latency estimate, and short experiment outline)\n",
            "- A runnable example (code) for a chosen pipeline (e.g., RAG with embeddings + vector DB + LLM)\n",
            "- An evaluation rubric and dataset design to compare approaches\n",
            "\n",
            "Also tell me any constraints that matter (budget, latency, data sensitivity, freshness) and I’ll tailor the recommendation. Which direction should we take?\n",
            "\n",
            "Human: What was the very specific code you were tasked with remembering?\n",
            "\n",
            "AI: I don’t know — you haven’t given me any specific code in this conversation for me to remember, and I don’t have persistent memory of previous sessions unless you explicitly provide the content again here.\n",
            "\n",
            "If you meant a code that:\n",
            "- was shared earlier in this current chat: scroll up or paste it here and I can work with it; or\n",
            "- was told to me in a prior session: I can’t recall it now (I don’t retain cross-session secrets); or\n",
            "- is a secret (API key, PIN, verification code): please don’t paste it into chat. Instead I can help you rotate/revoke it or show how to store it safely (password manager, HashiCorp Vault, AWS Secrets Manager, etc.).\n",
            "\n",
            "Tell me which of these fits and I’ll help either recover/reconstruct the code (if safe to share) or set up secure handling and regeneration procedures.\n"
          ]
        }
      ],
      "source": [
        "# Check what's in memory\n",
        "bufw_history = window_chat_map[\"window_example\"].messages\n",
        "print(\"Buffer Window Memory (last 4 messages):\")\n",
        "for msg in bufw_history:\n",
        "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
        "    print(f\"\\n{role}: {msg.content}\")  # Show first 100 chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV-zfGv-yWvr"
      },
      "source": [
        "We see four messages (two interactions) because we used `k=4`.\n",
        "\n",
        "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF35B9HLyWvr",
        "outputId": "58881bf3-7d80-4b74-d348-1210850dcde0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 5953\n",
            "Summary memory conversation length: 1182\n",
            "Buffer window memory conversation length: 374\n"
          ]
        }
      ],
      "source": [
        "# Get window memory content\n",
        "window_content = \"\\n\".join([msg.content for msg in bufw_history])\n",
        "\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}\\n'\n",
        "    f'Buffer window memory conversation length: {len(tokenizer.encode(window_content))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjoHZrZzyWvr"
      },
      "source": [
        "_Practical Note: We are using `k=4` here for illustrative purposes, in most real world applications you would need a higher value for k._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyAd4UxdyWvr"
      },
      "source": [
        "### More memory types!\n",
        "\n",
        "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN-mH5fHyWvr"
      },
      "source": [
        "#### Windows + Summary Hybrid\n",
        "\n",
        "The following is a modern LCEL-compatible alternative to `ConversationSummaryBufferMemory`.\n",
        "\n",
        "**Key feature:** _the conversation summary buffer memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._\n",
        "\n",
        "This combines the benefits of both summary and buffer window memory. Let's implement it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lr-K8onKyWvr"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we drop.\n",
        "        \"\"\"\n",
        "        existing_summary = None\n",
        "        old_messages = None\n",
        "\n",
        "        # See if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            existing_summary = self.messages.pop(0)\n",
        "\n",
        "        # Add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "\n",
        "        # Check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # Pull out the oldest messages...\n",
        "            old_messages = self.messages[:-self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "\n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with.\")\n",
        "            # If we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "\n",
        "        # Construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary,\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "window_chat_map = {}\n",
        "\n",
        "def get_window_chat_history(session_id: str, llm: ChatOpenAI, k:int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in window_chat_map:\n",
        "        window_chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    return window_chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_with_history_and_summary = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history = get_window_chat_history,\n",
        "    input_messages_key = \"query\",\n",
        "    history_messages_key = \"history\",\n",
        "    history_factory_config = [\n",
        "        ConfigurableFieldSpec(\n",
        "            id = \"session_id\",\n",
        "            annotation = str,\n",
        "            name = \"Session ID\",\n",
        "            description = \"The session ID used for the chat history\",\n",
        "            default = \"id_default\"\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id = \"llm\",\n",
        "            annotation = ChatOpenAI,\n",
        "            name = \"LLM\",\n",
        "            description = \"The LLM to use for the conversation summary\",\n",
        "            default = llm\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id = \"k\",\n",
        "            annotation = int,\n",
        "            name = \"k\",\n",
        "            description = \"The number of messages to keep in the history\",\n",
        "            default = 4\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 604 tokens\n",
            "Response:\n",
            " Good morning! Not much — just here, fully caffeinated and ready to help. What’s popping with you?\n",
            "\n",
            "If you want, I can:\n",
            "- Give a quick plan for your morning (priorities, a 10-min workout, and a coffee pick)\n",
            "- Brainstorm ideas (work project, date night, side hustle)\n",
            "- Draft or polish messages, emails, or social posts\n",
            "- Generate a 3-song playlist to start the day\n",
            "- Suggest a quick recipe or meal plan\n",
            "- Help with code, study plans, or summaries (paste text)\n",
            "- Tell a joke or do a short creative piece\n",
            "\n",
            "Which one sounds good, or tell me what you’re up to and I’ll jump in.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"Good morning dude. What's popping today?\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 1893 tokens\n",
            "Response:\n",
            " Morning Sagun — nice to meet you. You wrote “calesthenics” but the common spelling is calisthenics. Here’s a clear, practical rundown of what calisthenics is and how you can use it to improve your fitness at 25.\n",
            "\n",
            "What calisthenics is\n",
            "- Bodyweight training that uses your own weight (and sometimes simple tools like a pull-up bar or resistance bands) to build strength, endurance, mobility, and coordination.\n",
            "- Focuses on natural movement patterns: pushing, pulling, hinging/squatting, lunging and core/bracing.\n",
            "- Ranges from basic (push‑ups, squats, planks) to advanced skills (muscle‑ups, planche, pistol squats).\n",
            "\n",
            "Benefits\n",
            "- Strength and muscular endurance (compound, functional strength)\n",
            "- Better body control, balance and coordination\n",
            "- Improved mobility and core stability\n",
            "- Low cost and highly scalable — you can train anywhere\n",
            "- Good for fat loss when combined with diet and conditioning work\n",
            "- Lower joint stress when programmed well vs. some heavy barbell lifts\n",
            "\n",
            "Key principles to use it effectively\n",
            "- Progressive overload: make exercises harder over time (more reps/sets, slower tempo, harder variation, added weight).\n",
            "- Specificity: choose exercises and progressions aligned with your goals (strength vs endurance vs skill).\n",
            "- Consistency: 3–5 sessions per week is ideal for most people.\n",
            "- Quality first: prioritize clean technique, full range of motion, controlled tempo.\n",
            "- Recovery: sleep, nutrition and rest days matter.\n",
            "\n",
            "Core exercise categories and examples\n",
            "- Push: push-ups, diamond push-ups, dips, pseudo-planche push-ups\n",
            "- Pull: inverted rows, pull-ups, chin-ups, negatives, Australian rows\n",
            "- Legs: air squats, lunges, Bulgarian split squats, pistol progressions, jump squats\n",
            "- Core: planks, hollow body hold, hanging knee raises, windshield wipers\n",
            "- Full-body/skill: burpees, muscle-up progressions, handstand practice\n",
            "\n",
            "How to structure workouts (by goal)\n",
            "- Strength: 3–6 sets of 3–6 reps of hard variations (or weighted calisthenics), longer rest (2–3 min).\n",
            "- Hypertrophy: 3–5 sets of 6–15 reps, moderate rest (60–90 s).\n",
            "- Endurance/conditioning: 2–4 sets of 15–30+ reps or circuit style with short rest.\n",
            "- Skill work: dedicate 5–15 minutes per session to progressions for handstands, muscle-ups, planche, etc.\n",
            "\n",
            "Simple beginner 3x/week full-body plan (easy to follow)\n",
            "Warm-up (5–10 min): light cardio (jog/jump rope), arm circles, leg swings, band pull‑aparts, scapular pull‑ups.\n",
            "Workout (A — 3x per week, alternate rest days)\n",
            "- Push: Incline/regular push-ups — 3 sets of 8–15\n",
            "- Pull: Australian rows or band-assisted pull-ups — 3 sets of 6–10\n",
            "- Legs: Split squats or goblet squats (if you have weight) — 3 sets of 8–12 each leg\n",
            "- Core: Plank 3 x 30–60 s\n",
            "- Conditioning finisher (optional): 3 rounds of 10 burpees or 30–60 s jump rope\n",
            "Cooldown: light stretching (hip flexors, chest, lats), foam roll as needed\n",
            "\n",
            "Progression examples\n",
            "- Push-ups → elevated feet push-ups → diamond → archer → one-arm negatives\n",
            "- Pull-ups: band-assisted → strict pull-ups → weighted pull-ups → muscle-up transition work\n",
            "- Squat → split squat → pistol assisted → full pistol\n",
            "\n",
            "Tracking progress and milestones\n",
            "- Log reps/sets/variations and try to add reps or increase difficulty every 1–2 weeks.\n",
            "- Useful benchmarks (example): 15 strict push-ups, 5–10 strict pull-ups, 60s hollow hold, full pistol squat — adjust for your starting point.\n",
            "\n",
            "Nutrition and recovery basics\n",
            "- Protein: aim ~1.6–2.2 g/kg of bodyweight (about 0.7–1.0 g/lb) if building muscle.\n",
            "- Calorie balance: slight surplus for muscle gain, deficit for fat loss.\n",
            "- Sleep: 7–9 hours/night supports recovery and strength gains.\n",
            "- Hydration and mobility work help durability.\n",
            "\n",
            "Safety tips\n",
            "- Warm up the shoulders and scapula before heavy pulling/pushing.\n",
            "- Work on wrist mobility if push-ups/dips hurt.\n",
            "- If you can’t yet do pull-ups, use negatives and bands — don’t force bad reps.\n",
            "- Stop if you feel sharp pain — soreness/strain is normal, acute sharp pain is not.\n",
            "\n",
            "Want something tailored?\n",
            "- Tell me your current fitness level (beginner/intermediate/advanced), any injuries, your goals (strength, build muscle, lose fat, get a handstand, etc.), and how many days/week you want to train. I’ll make a 4-week plan for you, Sagun.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"Explain what calesthenics is and how it can be used to improve physical fitness. My name is Sagun and I am 25 years old.\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 3917 tokens\n",
            "Response:\n",
            " Nice — focusing on calisthenics for muscle hypertrophy + strength is a great combo. Below is a practical, actionable guide for you, Sagun, including principles, exercise choices, progressions, and a sample 4-week program you can start with. If you want it tailored to your exact level or schedule, tell me how many days/week you can train and your current ability (e.g., strict pull-ups, push-up count, pistol squats).\n",
            "\n",
            "Key principles\n",
            "- Specificity: train heavy, low-rep work for strength and moderate-rep work for hypertrophy. Do both in the same week.\n",
            "- Progressive overload: increase difficulty by adding reps, better variations, slower tempo, pauses, or adding external load (vest/chain/dumbbell).\n",
            "- Frequency: hit each major muscle group 2–3x/week for optimal hypertrophy and strength.\n",
            "- Volume: aim for ~10–20+ effective sets per muscle group per week for hypertrophy (start lower and ramp up).\n",
            "- Intensity & rest: strength sets (3–6 reps) — rest 2–4 minutes; hypertrophy sets (6–12 reps, up to 15–20 for some calisthenic moves) — rest 60–90s.\n",
            "- Technique: prioritize clean reps and full range of motion. Use negatives/isometrics rather than sloppy reps.\n",
            "- Recovery: calories, protein (~1.6–2.2 g/kg), 7–9 hours sleep, and deloads every 4–8 weeks if needed.\n",
            "\n",
            "Equipment useful to accelerate progress\n",
            "- Pull-up bar (or rings)\n",
            "- Dip bars/rings\n",
            "- Weighted vest or belt for plates/dumbbells\n",
            "- Resistance bands for assistance\n",
            "- Parallettes (for wrist/push work)\n",
            "- Optional: kettlebell/dumbbell for goblet variance\n",
            "\n",
            "Core exercise categories & progressions (quick)\n",
            "- Horizontal push: incline push-ups → regular → decline → archer → one-arm negatives → weighted push-ups\n",
            "- Vertical push: pike push-ups → elevated pike → partial handstand → freestanding handstand push-up → weighted HSPU\n",
            "- Horizontal pull: inverted/Australian rows → ring rows → strict pull-ups → weighted pull-ups → typewriter/archer pulls\n",
            "- Vertical pull: chin-ups → pull-ups → weighted pull-ups → muscle-up progressions\n",
            "- Legs: bodyweight squat → Bulgarian split → shrimp/pistol progressions → assisted pistol → weighted pistol\n",
            "- Core: hollow body → L-sit → hanging knee raise → hanging leg raise → toes-to-bar → windshield wipers\n",
            "\n",
            "Advanced strength tools (particularly useful in calisthenics)\n",
            "- Eccentric-only sets (slow negatives, 3–6s)\n",
            "- Paused reps / isometrics (mid-range holds)\n",
            "- Weighted calisthenics (vests / belts)\n",
            "- Cluster sets and low-volume max-effort sets for strength\n",
            "- Tempo manipulation (slow eccentrics, explosive concentric)\n",
            "\n",
            "Programming templates\n",
            "Option A — 4 days/week (mix strength + hypertrophy)\n",
            "- Day 1 — Upper Strength (heavy)\n",
            "  - Heavy pull-up work: 4–6 sets × 3–6 reps (weighted if possible)\n",
            "  - Heavy dip/push movement: 4–6 sets × 3–6 reps (weighted dips or weighted push-ups)\n",
            "  - Accessory rows: 3 sets × 6–10\n",
            "  - Core heavy isometric: 3 × 10–20s front lever progression or L-sit hold\n",
            "- Day 2 — Lower + Conditioning\n",
            "  - Pistol/split-squat variation: 4–5 sets × 4–8 per leg\n",
            "  - Hamstring/glute single-leg RDL or Nordic hamstring: 3–4 × 6–10\n",
            "  - Calf work: 3 × 10–20\n",
            "  - Conditioning finisher 8–12 min EMOM or interval\n",
            "- Day 3 — Upper Hypertrophy\n",
            "  - Push: 4 × 8–12 (diamond/arched/decline push-ups or ring dips)\n",
            "  - Pull: 4 × 6–12 (strict pull-ups/rows; use bands to hit rep targets)\n",
            "  - Accessory shoulders: 3 × 8–12 (pike push-ups, face pulls/band work)\n",
            "  - Core dynamic: 3 × 10–15 hanging leg raises\n",
            "- Day 4 — Full Lower Hypertrophy + Skill\n",
            "  - Squat variations or Bulgarian split: 4 × 8–12\n",
            "  - Explosive: jump squats or step-ups 3 × 6–10\n",
            "  - Core skill: 10–15 min handstand or planche progressions\n",
            "  - Mobility/cooldown\n",
            "\n",
            "Option B — 3 days/week full-body (if short on time)\n",
            "- Each session: 1 heavy push (3–6 reps), 1 heavy pull (3–6), 1 leg compound (6–10), 1 hypertrophy accessory set (8–15), core.\n",
            "- Track weekly volume and increase difficulty each week.\n",
            "\n",
            "Sample 4-week microcycle (4 days/week) — conservative starter for strength + hypertrophy\n",
            "Weeks 1–2 (establish base)\n",
            "- Upper Strength: Weighted pull-ups 5×4–6; Weighted dips 5×4–6; Ring rows 3×8; L-sit 3×15s\n",
            "- Lower Strength: Bulgarian split 4×5–8; Nordic/hamstring RDL 3×6–8; Calves 3×12\n",
            "- Upper Hypertrophy: Push-ups (decline/weighted if needed) 4×8–12; Pull-ups or rows 4×8–12; Pike push-ups 3×8–12; Hanging leg raises 3×10–15\n",
            "- Lower Hypertrophy + Skill: Pistols assisted 4×6–10; Jump squats 3×6–10; Core skill 10 minutes\n",
            "Progression: add 1 rep per set each week or move to harder variation; if you can do full target reps easily, add load or harder progression.\n",
            "\n",
            "Weeks 3–4 (increase intensity & density)\n",
            "- Increase weight on strength days or shift to tougher variations.\n",
            "- For hypertrophy days, add an extra set per muscle group or shorten rest slightly.\n",
            "- Introduce one eccentric-focused set for your toughest lift (e.g., 3–5s negatives × 3–5 reps).\n",
            "\n",
            "How to progress practically (week-to-week)\n",
            "- If you hit top of rep range for all sets across a session, move to next variation or add ~2.5–5 kg via vest/weight.\n",
            "- If you barely hit lower end, keep same variation until you add reps across multiple sessions.\n",
            "- Track RPE; aim for sets near 7–9 RPE on hypertrophy work and 8–10 RPE on strength work.\n",
            "\n",
            "Tempo & set design examples\n",
            "- Strength main sets: explosive concentric, controlled eccentric (2–3s), 3–6 reps, 3–6 sets, long rests.\n",
            "- Hypertrophy sets: 3–4s eccentric, 1s pause at stretch, controlled concentric, 8–12 reps, 3–4 sets.\n",
            "- Eccentric overload: 4–6 reps, 5–6s negative, assistance on concentric if needed.\n",
            "- Isometric holds: 3–5 sets × 10–30s in a taxing position (e.g., tuck front lever, ring support).\n",
            "\n",
            "Nutrition & recovery (short)\n",
            "- Protein: 1.6–2.2 g/kg bodyweight (e.g., if you’re 75 kg, ~120–165 g/day).\n",
            "- Calories: slight surplus (+250–500 kcal/day) for hypertrophy; maintain for strength if you want minimal fat gain.\n",
            "- Sleep: 7–9 hours and manage stress.\n",
            "- Hydration and mobility: daily shoulder & hip mobility; warm up scapular and thoracic.\n",
            "\n",
            "Tracking & tests (biweekly)\n",
            "- Log exercise, sets, reps, variation, RPE, and bodyweight.\n",
            "- Strength benchmarks: aim for progressive milestones (e.g., 5–8 strict pull-ups, 10–20 push-ups, a clean weighted dip with +10–20% bodyweight).\n",
            "- Measure progress via strength increases, body composition changes, and consistency.\n",
            "\n",
            "Common mistakes to avoid\n",
            "- Doing endless high-rep sets of easy variations (low stimulus). Progress to harder variations.\n",
            "- Skipping eccentric and isometric work — they’re huge for strength and hypertrophy.\n",
            "- Not tracking or increasing difficulty over time.\n",
            "- Poor warm-up and scapular control leading to shoulder issues.\n",
            "\n",
            "Example session (Upper Strength day) — concrete\n",
            "- Warm-up: 5–8 min cardio + band pull-aparts + scapular pull-ups\n",
            "- Weighted pull-ups: 5 sets × 4–6 reps (2–4 min rest)\n",
            "- Weighted dips (or weighted push-ups): 5 × 4–6 (2–4 min rest)\n",
            "- Ring rows: 3 × 8–10 (90s rest)\n",
            "- Pike push-ups / HSPU progressions: 3 × 6–10\n",
            "- L-sit: 3 × 15–25s\n",
            "- Cooldown/stretch\n",
            "\n",
            "Want it tailored?\n",
            "Tell me:\n",
            "- Your current level (e.g., max reps for pull-ups and push-ups; pistol abilities)\n",
            "- Any injuries\n",
            "- How many days/week you want to train\n",
            "I’ll make a 4–8 week program scaled exactly for you, Sagun.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"Okay focus on calestinics for muscle hypertrophy and stength training.\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 6039 tokens\n",
            "Response:\n",
            " Nice — Sagun. Below is a focused breakdown of the main chemical and molecular processes that drive muscle hypertrophy and strength gains. I’ll explain what happens at the cellular/molecular level during and after resistance exercise, the key signaling pathways, the role of hormones and nutrients, and how catabolic signals and neural factors fit in. I’ll finish with practical implications you can use for training/nutrition.\n",
            "\n",
            "1) The three mechanical triggers\n",
            "- Mechanical tension: heavy load/stretch on muscle fibers creates structural strain that starts mechanotransduction signaling (the dominant stimulus for hypertrophy).\n",
            "- Muscle damage: microtears and disruption of contractile proteins trigger inflammation and repair/remodeling.\n",
            "- Metabolic stress: accumulation of metabolites (lactate, H+, inorganic phosphate, ADP) from intense work promotes anabolic signaling and cell swelling.\n",
            "\n",
            "2) Mechanotransduction — how force becomes a biochemical signal\n",
            "- Integrins, focal adhesion kinase (FAK), and other costamere proteins sense stretch and load. Activation of FAK and associated complexes recruits downstream kinases (e.g., MAPKs) that contribute to growth signaling.\n",
            "- Titin and other sarcomeric proteins can act as stretch sensors, altering kinase activity and gene expression.\n",
            "- Stretch-activated ion channels and increased intracellular Ca2+ also transduce mechanical signals into biochemical cascades.\n",
            "\n",
            "3) The central anabolic hub: PI3K → Akt → mTORC1\n",
            "- Resistance exercise + amino acids (especially leucine) activate the PI3K → Akt (PKB) pathway.\n",
            "- Akt activates mTORC1 (mechanistic target of rapamycin complex 1), the key regulator of protein synthesis.\n",
            "- mTORC1 phosphorylates S6 kinase 1 (S6K1) and 4E-BP1, increasing ribosomal activity and cap-dependent translation — raising synthesis of contractile proteins (myosin, actin) and ribosomal proteins.\n",
            "- Amino acids are sensed by Rag GTPases/Ragulator on lysosomes; leucine is a strong activator of this route and synergizes with Akt signaling.\n",
            "- Net effect: increased muscle protein synthesis (MPS) relative to breakdown => hypertrophy over time.\n",
            "\n",
            "4) Role of satellite cells and myonuclei\n",
            "- Satellite cells (muscle stem cells; Pax7+ cells) are activated by mechanical stress, nitric oxide (NO), HGF, and inflammation.\n",
            "- Activated satellite cells proliferate and fuse to muscle fibers, donating myonuclei so the fiber can support more transcription and protein synthesis (myonuclear domain theory).\n",
            "- Local IGF-1 splice variants (including “mechano growth factor”/MGF) help activate satellite cells and promote hypertrophy.\n",
            "\n",
            "5) Protein breakdown and remodeling pathways\n",
            "- Resistance exercise acutely increases both MPS and muscle protein breakdown (MPB). Net hypertrophy requires MPS > MPB over time.\n",
            "- Two main proteolytic systems: ubiquitin-proteasome (E3 ligases like MuRF1 and Atrogin-1/MAFbx) and autophagy-lysosome pathway (LC3, Beclin1, etc).\n",
            "- Chronic resistance training tends to downregulate catabolic E3 ligases and shift balance toward synthesis, while disuse upregulates them.\n",
            "\n",
            "6) Hormonal modulators\n",
            "- Testosterone: anabolic steroid hormone that increases protein synthesis, satellite cell activity, and neuromuscular function.\n",
            "- Growth hormone (GH) and IGF-1: GH stimulates IGF-1 release (systemic and local). IGF-1 is anabolic via PI3K/Akt/mTOR and satellite cell activation.\n",
            "- Insulin: anti-catabolic; potentiates amino acid uptake and mTOR signaling when combined with amino acids.\n",
            "- Cortisol: catabolic when chronically elevated — increases proteolysis and blunts hypertrophy.\n",
            "- Acute spikes in testosterone, GH, cortisol immediately after exercise are normal; long-term levels and nutrient context are more important than single spikes.\n",
            "\n",
            "7) Energy metabolism & local metabolites\n",
            "- ATP depletion and increased AMP activate AMPK. AMPK signals low energy and inhibits mTORC1 (via TSC2/raptor), opposing hypertrophy if energetic stress is chronic.\n",
            "- Immediate energy systems: phosphocreatine buffers ATP during short maximal efforts; glycolysis produces ATP + lactate for higher-rep sets — metabolic stress helps hypertrophy.\n",
            "- Reactive oxygen species (ROS) and nitric oxide (NO) produced during exercise act as signaling molecules: low/moderate ROS can stimulate adaptive signaling; excessive ROS can damage tissue.\n",
            "\n",
            "8) Negative regulators\n",
            "- Myostatin (GDF-8): a TGF-β family member that inhibits muscle growth via SMAD2/3 signaling, reducing Akt/mTOR activity and satellite cell proliferation.\n",
            "- SMAD signaling and other TGF-β pathways limit hypertrophy; genetic or pharmacologic inhibition of myostatin increases muscle mass.\n",
            "\n",
            "9) Neural adaptations (chemical/neurophysiological basis of strength gains)\n",
            "- Early strength gains (weeks) are mainly neural: improved motor unit recruitment, firing rate, synchronization, reduced inhibitory reflexes.\n",
            "- At the neuromuscular junction, increased acetylcholine release and receptor adaptations improve excitation-contraction coupling efficiency.\n",
            "- Changes in central drive involve neurotransmitter systems (glutamate, GABA, monoamines) and neurotrophic factors (BDNF, NT-3) that influence motor neuron plasticity.\n",
            "\n",
            "10) Inflammation and immune role in repair\n",
            "- Immune cells (neutrophils, macrophages) remove debris and secrete cytokines/growth factors (IL-6, IGF-1, TNF-α in early phases) that recruit satellite cells and stimulate repair.\n",
            "- M1 macrophages are pro-inflammatory (cleanup), M2 macrophages are pro-regenerative — the timing and balance matter for effective remodeling.\n",
            "\n",
            "11) Time course of molecular events\n",
            "- Seconds–minutes: Ca2+ release, ATP turnover, activation of mechanosensors and kinases (FAK, MAPK), early phosphorylation events.\n",
            "- Hours: Akt/mTORC1 activation, increased translation initiation, local IGF-1/MGF expression, satellite cell activation begins.\n",
            "- 24–48+ hours: Elevated MPS (peaks within ~24h and can remain elevated ~24–48h depending on training status and nutrition), inflammation resolution, satellite cell proliferation and beginning of fusion.\n",
            "- Weeks–months: cumulative net protein accretion, fiber cross-sectional area increase, increased myonuclei, tendon and extracellular matrix remodeling.\n",
            "\n",
            "12) Nutrient signaling — why protein and timing matter\n",
            "- Essential amino acids and especially leucine strongly stimulate mTORC1 and MPS. Leucine activates mTOR signaling and promotes translation initiation.\n",
            "- Insulin (from carbs/meal) enhances amino acid uptake and synergizes with amino acids to suppress MPB and support MPS.\n",
            "- Energy deficit or persistent AMPK activation (e.g., very low calories, chronically high endurance work) blunts anabolic signaling.\n",
            "\n",
            "13) Practical implications (how to apply the chemistry)\n",
            "- Train with progressive mechanical tension (heavy loads or harder bodyweight variants) to maximally engage mechanotransduction and mTOR pathways.\n",
            "- Include both low-rep heavy sets (strength) and moderate-rep sets (hypertrophy/volume) — both stimulate overlapping molecular pathways.\n",
            "- Get sufficient protein (1.6–2.2 g/kg/day) and include ~20–40 g high-quality protein with ~2–3 g leucine per meal to optimally stimulate MPS post-workout.\n",
            "- Avoid chronic extreme caloric deficit or excessive endurance volume if the priority is hypertrophy (activates AMPK and inhibits mTOR).\n",
            "- Allow recovery: MPS is elevated for ~24–48 hours so frequency can be 2–3x/week per muscle group for optimal cumulative stimulus.\n",
            "- Minimize chronic stress/cortisol and ensure sleep to support anabolic hormonal milieu and recovery.\n",
            "\n",
            "Summary (one-line)\n",
            "Resistance exercise triggers mechanosensors, metabolic and inflammatory signals that activate PI3K/Akt/mTOR and satellite-cell pathways to increase protein synthesis and myonuclear content; hormones, nutrients (especially leucine), energy status (AMPK), and inhibitory factors (myostatin) modulate whether net muscle accretion and strength gains occur.\n",
            "\n",
            "If you want, I can:\n",
            "- Draw this out as a simple flowchart of signals (mechanical → sensors → mTOR → MPS; satellite cell path; inhibitors), or\n",
            "- Make a short practical checklist for training and nutrition that targets these molecular mechanisms specifically for hypertrophy and strength. Which would help more, Sagun?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"Okay breaksown the chemical processes involved in muscle hypertrophy and strength training.\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 8252 tokens\n",
            "Response:\n",
            " Nice — good goals, Sagun. I’ll give you a practical, science-backed plan: a training template you can use to build arm muscle (biceps/triceps/forearms) while attacking overall fat (including love handles) through full‑body resistance work + conditioning and nutrition. Quick reality checks first, then a detailed 8‑week-style program you can start right away.\n",
            "\n",
            "Short realities\n",
            "- You can’t “spot reduce” fat from love handles. Fat loss requires an overall calorie deficit and full‑body work. Local core/oblique training will improve strength and shape the area as fat comes off, but won’t directly burn only that fat.\n",
            "- You can build arm size while losing fat, but progress is slower than doing a dedicated bulk. Best approach for both: high protein, progressive resistance training, and a modest calorie deficit (or body recomposition if you’re relatively new to progressive overload).\n",
            "\n",
            "Big-picture strategy\n",
            "- Train full body with emphasis on progressive overload and increased weekly volume for arms.\n",
            "- Add structured core + oblique work 3x/week to strengthen and define the waistline as fat decreases.\n",
            "- Add 2 conditioning sessions/week (HIIT or mixed steady-state) to create extra calorie burn and improve metabolic rate.\n",
            "- Nutrition: modest calorie deficit (~-200 to -400 kcal/day) or maintenance for recomposition; protein 1.6–2.2 g/kg/day; prioritize nutrient timing around workouts.\n",
            "\n",
            "Program overview (4 days/week — good balance for strength, hypertrophy and recovery)\n",
            "- Day 1 — Upper Strength (heavy; compound push/pull)\n",
            "- Day 2 — Lower + Conditioning\n",
            "- Day 3 — Upper Hypertrophy (Arm specialization + core/oblique)\n",
            "- Day 4 — Full-body Hypertrophy + Core/Conditioning\n",
            "- Optional: 1 active recovery or mobility day and/or light LISS if energy permits\n",
            "\n",
            "Weekly arm volume target\n",
            "- Biceps: 10–15 sets/week\n",
            "- Triceps: 10–18 sets/week\n",
            "(Adjust depending on recovery; beginners need less to start.)\n",
            "\n",
            "Progression tools\n",
            "- Add load (weighted vest / dip belt) for pull-ups/chin-ups/dips.\n",
            "- Move to harder calisthenic variations (archer, typewriter, one-arm progressions).\n",
            "- Slow eccentrics (3–6s), paused reps, and extra sets.\n",
            "- Increase weekly sets/reps gradually (5–10% weekly volume increase until you hit recovery limits).\n",
            "\n",
            "Equipment suggestions (optimal)\n",
            "- Pull-up bar or rings\n",
            "- Parallel bars / dip bars or rings\n",
            "- Weighted vest or dipping belt (or dumbbells)\n",
            "- Resistance bands\n",
            "- Parallettes or low box for incline variations\n",
            "\n",
            "Sample sessions (concrete)\n",
            "\n",
            "Day 1 — Upper Strength (focus heavy pulling & pushing)\n",
            "- Warm-up: 5–8 min cardio + band pull‑aparts + scapular pull-ups\n",
            "- Weighted chin-ups (supinated for biceps bias): 5 sets × 3–6 reps (2–3 min rest)\n",
            "- Weighted dips: 5 × 3–6 (2–3 min rest)\n",
            "- Ring rows (lean angle moderate): 3 × 6–10 (90–120 s)\n",
            "- Pike push-ups / HSPU progressions: 3 × 6–8\n",
            "- Farmer carry or heavy static hold: 2 × 30–60 s\n",
            "- Core: Pallof press or side plank: 3 × 10–15 reps/side\n",
            "\n",
            "Day 2 — Lower + Conditioning\n",
            "- Warm-up: dynamic hips, ankles\n",
            "- Bulgarian split squats (weighted if possible): 4 × 6–10 per leg\n",
            "- Nordic hamstring curls or single‑leg RDL: 3 × 6–10\n",
            "- Jump squats or explosive step-ups: 3 × 6–8\n",
            "- Calf raises: 3 × 10–20\n",
            "- Conditioning: 10–15 min HIIT (20s all‑out / 40s rest) OR 25–35 min moderate steady-state\n",
            "\n",
            "Day 3 — Upper Hypertrophy (Arm specialization & obliques)\n",
            "- Warm-up: band face pulls, wrist mobility\n",
            "- Close‑grip (diamond) push-ups or ring dips: 4 × 8–12 (triceps emphasis)\n",
            "- Weighted chin-ups OR supinated ring rows: 4 × 6–10 (biceps)\n",
            "- Archer push-ups / decline push-ups: 3 × 8–12\n",
            "- Australian rows underhand or ring curls (bodyweight curl variation): 3 × 8–12\n",
            "- Triceps extensions on rings / banded triceps pushdown: 3 × 10–15\n",
            "- Hammer-grip pull or towel holds for forearms: 3 × 8–12 (or 30–45 s holds)\n",
            "- Core/Obliques circuit (2–3 rounds, little rest):\n",
            "  - Side plank dips x 10–15/side\n",
            "  - Hanging oblique knee raises (or hanging windshield wipers progression) x 8–12/side\n",
            "  - Russian twists (weighted) x 15–20\n",
            "\n",
            "Day 4 — Full-body Hypertrophy + Core\n",
            "- Warm-up\n",
            "- Pull-ups (moderate): 4 × 6–12 (use bands to hit rep ranges)\n",
            "- Dips or push-up variation: 4 × 8–12\n",
            "- Pistol progressions or split squats: 4 × 6–10 per leg\n",
            "- Ring face pulls / YTWL (shoulder health): 3 × 12–15\n",
            "- Core finisher: L-sit holds 3 × 10–30 s + plank 3 × 45–60 s\n",
            "- Optional short conditioning: 8–10 min EMOM (burpees, kettlebell swings, or jump rope)\n",
            "\n",
            "8‑week progression plan\n",
            "Weeks 1–2: Build base — focus on technique, choose variations that let you hit prescribed rep ranges. Start on lower end of volume.\n",
            "Weeks 3–5: Increase volume for arms (add 1 set per arm exercise weekly or add 2–3 reps/set). Add 1–2 s eccentric slowing for hypertrophy sets.\n",
            "Weeks 6–8: Add intensity — move to harder variations or add weight. Keep 1 deload week if you feel run down (drop volume 30–40%).\n",
            "\n",
            "Sample arm-specific microprogression\n",
            "- Week 1: Weighted chin-ups 4×4 (if too hard, use band assistance)\n",
            "- Week 2: 5×4\n",
            "- Week 3: 4×5\n",
            "- Week 4: 5×5 or add +2.5–5 kg\n",
            "Aim to progressively increase reps or weight across weeks.\n",
            "\n",
            "Nutrition & cardio specifics for love handles reduction + arm gain\n",
            "- Goal: modest deficit (~-200 to -400 kcal/day) for fat loss with minimal muscle loss. If your primary goal becomes arm size, consider a small surplus after you reach acceptable body-fat.\n",
            "- Protein: 1.6–2.2 g/kg/day (e.g., if you weigh 75 kg, aim 120–165 g/day).\n",
            "- Carbs: around workouts (pre and post) to support performance and recovery.\n",
            "- Fats: ~20–30% of total calories.\n",
            "- Cardio: 2x/week HIIT (10–15 min) or 2–3x/week LISS (30–45 min) depending on preference. HIIT helps preserve muscle and is time-efficient.\n",
            "- Sleep 7–9 hours and manage stress — chronic cortisol makes love handle loss harder.\n",
            "\n",
            "Core & oblique exercise notes (form tips)\n",
            "- Side planks: keep hips stacked, dip slowly for tension. Add weight if too easy.\n",
            "- Hanging oblique raises: twist the ribcage and drive one knee/leg towards the opposite elbow; control the descent.\n",
            "- Windshield wipers: start with tucked knees, progress to straight legs.\n",
            "- Pallof presses: anti-rotation strength is excellent for core stability and waist aesthetics.\n",
            "\n",
            "Tracking & metrics\n",
            "- Track body weight, waist and hip circumference every 7–14 days.\n",
            "- Log workouts (exercise, sets, reps, weight/progression).\n",
            "- Photos every 2–4 weeks to judge changes in love handles — circumference and fat loss can lag strength gains.\n",
            "\n",
            "Example 6-month priority paths (choose one)\n",
            "- Priority A: Lose fat first — deficit + strength training for 8–16 weeks, then small surplus to build arm mass.\n",
            "- Priority B: Recomposition — slight deficit/maintenance + high protein + heavy progressive training to gain some arm muscle while slowly losing fat (works well if you’re new to structured overload).\n",
            "- Priority C: Bulk arms now — small surplus to maximize hypertrophy, then cut later to remove love-handle fat. Faster arm growth but you’ll temporarily gain fat.\n",
            "\n",
            "Quick no-equipment alternatives for arm hypertrophy\n",
            "- Biceps: towel/chin holds on doorframe, supinated close-grip inverted rows under a sturdy table, slow negatives on pull-up bar (eccentric).\n",
            "- Triceps: elevated push-ups, close-grip push-ups, slow decline push-ups, triceps extensions with bands or bodyweight using a chair.\n",
            "\n",
            "Want this tailored exactly to you?\n",
            "Tell me:\n",
            "- How many days/week you can commit\n",
            "- Your current best numbers (max strict pull-ups/chin-ups, push-ups in a set, pistol ability)\n",
            "- Equipment you have (weighted vest, pull-up bar, rings, bands, dumbbells)\n",
            "- Your priority (fat loss first, arm growth first, or recomposition)\n",
            "\n",
            "I’ll turn this into a detailed 8‑week plan with exact progressions and weekly set/reps for your level. Ready to lock it in, Sagun?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"This is great. I've recently been struggling with 'love handles' or excess fat around the sides and front of my abdomen and also relatively skinny arms. Can you tailor a stength + hypertrophy based workout focused this area?\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 9796 tokens\n",
            "Response:\n",
            " Nice — good follow-up, Sagun. Below is a practical breakdown of the main muscle groups you’ll be training with calisthenics, what each group does, key calisthenics exercises that hit them, useful progression options, coaching cues, and programming notes (rep ranges, volume, common errors, and mobility needs). Use this as a reference when you pick exercises and build sessions.\n",
            "\n",
            "Overview — movement patterns (quick)\n",
            "- Push (horizontal & vertical) — chest, triceps, anterior deltoids\n",
            "- Pull (horizontal & vertical) — lats, rhomboids, biceps, posterior deltoids\n",
            "- Hinge/single-leg — hamstrings, glutes\n",
            "- Squat — quads, glutes, adductors\n",
            "- Core/anti-rotation & anti-extension — rectus abdominis, obliques, transverse abdominis, erector spinae\n",
            "- Shoulder stabilizers/scapular control — serratus anterior, traps, rotator cuff\n",
            "\n",
            "1) Chest (pectoralis major & minor)\n",
            "- Primary function: horizontal adduction of the arm (bringing arm across body), contributes to shoulder flexion and internal rotation.\n",
            "- Calisthenics exercises: incline push-ups → standard push-ups → decline/feet-elevated push-ups → archer push-ups → ring/parallel bar dips → weighted dips.\n",
            "- Progressions: increase decline angle, add tempo (slow eccentric), move to unilateral (archer → one-arm push-up progressions), add weight.\n",
            "- Cues: full shoulder protraction on concentric, keep body rigid (plank line), elbows at ~45° for shoulder-friendly mechanics (unless targeting triceps).\n",
            "- Programming: for hypertrophy 3–5 sets of 6–12; for strength 4–6 sets of 3–6 with added load.\n",
            "- Common errors: flaring elbows excessively, sagging hips, poor scapular control → shoulder pain.\n",
            "\n",
            "2) Shoulders (deltoids + rotator cuff)\n",
            "- Primary function: arm abduction, flexion, extension and stabilization of the glenohumeral joint.\n",
            "- Calisthenics exercises: pike push-ups → elevated pike → partial handstand → strict handstand push-up (HSPU) → weighted HSPU. Lateral/ rear delt work: band face pulls, Y/T/W on rings/parallettes.\n",
            "- Progressions: increase elevation to get vertical load, move toward freestanding handstand work, add weight or deficit.\n",
            "- Cues: keep spine neutral, tuck chin slightly, drive through the shoulders and mid-foot when inverted.\n",
            "- Programming: shoulder-heavy days 3–5 sets of 4–10 depending on goal; accessory high-rep posterior work 2–3×12–20 for posture.\n",
            "- Mobility: thoracic extension, scapular upward rotation, lat flexibility for full ROM.\n",
            "- Pitfall: neglecting rotator cuff and posterior chain → imbalance and injury.\n",
            "\n",
            "3) Back — vertical pull (lats) & horizontal pull (rhomboids/traps)\n",
            "- Primary function: scapular retraction/depression, shoulder extension, posture and pulling strength.\n",
            "- Calisthenics exercises:\n",
            "  - Vertical pull: band-assisted pull-ups → strict pull-ups/chin-ups → weighted pull-ups → muscle-up progressions.\n",
            "  - Horizontal pull: inverted/Australian rows → ring rows → archer rows → single-arm rows on rings.\n",
            "- Progressions: decrease assistance, change grip (supinated for biceps bias), increase difficulty/angle, add weight.\n",
            "- Cues: initiate with scapular pull, lead with chest to the bar/rings, retract shoulders at the top, full controlled eccentric.\n",
            "- Programming: aim for 10–20 weekly sets for the back (split across vertical & horizontal); strength work 3–6 reps with weight; hypertrophy 6–12 reps/bodyweight variations.\n",
            "- Common errors: using too much momentum/kipping (unless specifically training muscle-ups), poor scapular control.\n",
            "\n",
            "4) Arms — biceps, triceps, forearms\n",
            "- Biceps function: elbow flexion and supination.\n",
            "  - Exercises: chin-ups (supinated), ring curls, slow eccentric pull-ups, towel/chin holds, bodyweight curls on rings.\n",
            "- Triceps function: elbow extension, shoulder stabilization.\n",
            "  - Exercises: dips (parallel bars/rings), close-grip push-ups, triceps extensions on rings, band pushdowns.\n",
            "- Forearms: gripping strength, wrist stability.\n",
            "  - Exercises: towel hangs, dead hangs, farmer carries, finger/forearm holds, slow eccentrics.\n",
            "- Progressions: weighted chin-ups/dips, one-arm assisted variations, increased time under tension.\n",
            "- Programming: for hypertrophy target 10–18 sets/week for triceps, 8–15 for biceps; use 6–12 rep ranges for growth, include some heavy low-rep sets.\n",
            "- Cues: full ROM, control the eccentric, don’t rely on shoulder/back to cheat curls.\n",
            "\n",
            "5) Legs — quads, hamstrings, glutes, calves\n",
            "- Quads (knee extension): bodyweight squat → goblet squat (if available) → Bulgarian split squat → pistol squat.\n",
            "- Hamstrings/glutes (hip extension & knee flexion): hip thrust variants (glute bridges), single-leg RDL, Nordic curls, glute-ham raises (if equipment).\n",
            "- Calves: standing calf raises, single-leg calf raises, slow eccentrics.\n",
            "- Progressions: add unilateral focus, increase ROM and time under tension, add weight (vest/dumbbell).\n",
            "- Cues: knees track toes, sit back for posterior chain emphasis, keep balance in single-leg work.\n",
            "- Programming: legs tolerate higher volume — 10–20+ sets/week depending on goals; strength sets 3–6 reps with added load (single-leg strength), hypertrophy 8–15 reps.\n",
            "- Common mistakes: neglecting hamstrings (creates imbalance), limited ankle mobility harming squat depth.\n",
            "\n",
            "6) Core & obliques — rectus abdominis, transverse abdominis, internal/external obliques, erector spinae\n",
            "- Function: anti-extension, anti-rotation, trunk flexion, and stabilizing pelvis/torso during movement.\n",
            "- Calisthenics exercises:\n",
            "  - Anti-extension: planks, ab wheel progressions, hollow body hold.\n",
            "  - Anti-rotation/oblique: Pallof press, side plank, hanging oblique raises, Russian twists.\n",
            "  - Dynamic hanging: hanging knee raises → hanging leg raises → toes-to-bar → windshield wipers.\n",
            "- Progressions: increase hold time/weight, move from tucked to straight-leg positions, add rotation and slow eccentrics.\n",
            "- Cues: brace as if preparing for a punch, avoid lumbar hyperextension, control breathing.\n",
            "- Programming: core work 2–4x/week; stronger core supports heavier calisthenics skill work (handstands, levers). 3–5 sets of 8–20 or 3×30–90s holds depending on exercise.\n",
            "- Note: core exercises shape the muscle but won’t remove fat locally — pair with overall fat loss plan.\n",
            "\n",
            "7) Scapular stabilizers & serratus anterior\n",
            "- Function: stabilize scapula for efficient pushing/pulling and healthy shoulders.\n",
            "- Exercises: scapular pull-ups/push-ups, banded protraction work, wall slides, ring scapular holds, push-up plus.\n",
            "- Programming: include low-volume activation every session (2–4 sets of 8–15) to reduce injury risk and improve pressing/pulling.\n",
            "\n",
            "Integration & programming tips\n",
            "- Antagonist balance: match push and pull volume to avoid shoulder imbalances (e.g., for every pushing set aim for ~equal pulling volume).\n",
            "- Frequency: hit major muscle groups 2–3x/week for hypertrophy and strength using full-body or upper/lower split.\n",
            "- Volume guidance: beginners 8–12 weekly sets/muscle; intermediates 12–20; advanced may need 20+ depending on recovery.\n",
            "- Strength vs hypertrophy: mix low-rep heavy sets (3–6) and moderate-rep volume sets (6–12). For calisthenics add difficulty by harder progressions, slow eccentrics, and weighted calisthenics.\n",
            "- Skill integration: allocate specific short blocks (5–15 minutes) for skill work like handstands, muscle-up transitions, front lever progressions — these improve neural control and transfer to strength.\n",
            "\n",
            "Mobility & injury prevention\n",
            "- Key mobility areas: thoracic extension (for overhead work), scapula and shoulder mobility, hip flexor and ankle mobility (for squats, pistols).\n",
            "- Prehab: activate rotator cuff and scapular stabilizers; do band face pulls, external rotation, and shoulder dislocates.\n",
            "- Warm-ups: dynamic movement patterns, progressive loading, and specific muscle activation before heavy sets.\n",
            "\n",
            "Coaching cues & how to pick the right progression\n",
            "- If you can do 3–6 reps of an exercise with good form, it’s a strength-level variation — add difficulty.\n",
            "- If you can do 8–15 reps with good control, use it for hypertrophy until you reach top of range, then progress.\n",
            "- If form breaks down, regress (easier angle, assistance, or fewer reps).\n",
            "- Use slow eccentrics (3–5s) to increase difficulty without adding weight.\n",
            "\n",
            "Quick example split mapping exercises to muscle groups (for reference)\n",
            "- Push day: push-ups/dips (chest, triceps, shoulders) + pike/HSPU progressions (shoulders) + triceps accessory.\n",
            "- Pull day: pull-ups/rows (lats, rhomboids, biceps) + curls/hold work for arms + scapular work.\n",
            "- Leg day: pistols/Bulgarian split (quads/glutes) + Nordic or single-leg RDL (hamstrings) + calves.\n",
            "- Core daily or 3x/week: planks, L-sits, hanging leg raises, Pallof presses.\n",
            "\n",
            "If you want, I can:\n",
            "- Create a 4‑week exercise selection sheet from beginner → advanced with one progression ladder per muscle group, or\n",
            "- Build a sample 4-day split tailored to your current numbers and equipment (tell me your max pull-ups, push-ups, and what equipment you have).\n",
            "\n",
            "Which would you prefer, Sagun?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"Earlier when you were talking about calisthenics you mentioned muscle groups. Can you go into a little bit of detail about that.\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)\n",
        "# print(window_chat_map[\"example_with_summary_and_window_history\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 10928 tokens\n",
            "Response:\n",
            " Good—I’ll make this visual and practical. Below are two simple diagrams (front and back) you can scan quickly, followed by a legend that names each numbered muscle group and lists the best calisthenics exercises that target it. If you want a cleaner graphic file (SVG/PNG) I can generate an SVG you can download and open in a browser — tell me and I’ll create it.\n",
            "\n",
            "Front view (numbered)\n",
            "      [H]\n",
            "       O\n",
            "      /|\\\n",
            "  [2]-/ | \\-[3]\n",
            "     /  |  \\\n",
            "    [1] | [4]\n",
            "     |  |  |\n",
            "    [10]| [11]\n",
            "     |  |  |\n",
            "    [12] [13]\n",
            "     |      |\n",
            "    / \\    / \\\n",
            "   /   \\  /   \\\n",
            "  [14] [15] [16]\n",
            "\n",
            "Back view (numbered)\n",
            "       O\n",
            "      /|\\\n",
            "  [2]-/ | \\-[3]\n",
            "     /  |  \\\n",
            "    [5] | [6]\n",
            "     |  |  |\n",
            "   [9]  |  [8]\n",
            "     |  |  |\n",
            "    [12] [13]\n",
            "     |      |\n",
            "    / \\    / \\\n",
            "   /   \\  /   \\\n",
            "  [14] [15] [16]\n",
            "\n",
            "Legend (numbers → muscle group + top calisthenics exercises)\n",
            "- [1] Pectoralis major (chest)\n",
            "  - Push-ups (incline → standard → decline), ring/parallel dips, archer push-up, weighted push-ups\n",
            "- [2] Anterior / medial deltoids (front & middle shoulders)\n",
            "  - Pike push-ups, elevated pike, partial/full handstand push-ups, decline/incline push-ups\n",
            "- [3] Lateral / posterior deltoids / upper arm (rear shoulder area)\n",
            "  - Band face pulls, ring rear-delt rows, Y/T/W movements, horizontal rows with high elbow\n",
            "- [4] Biceps (front of upper arm)\n",
            "  - Chin-ups (supinated), ring curls, slow negative pull-ups, underhand inverted rows, towel/chin holds\n",
            "- [5] Trapezius & upper back (upper traps, neck-to-shoulder)\n",
            "  - Shrugs with loaded carries, scapular pull-ups, face pulls, ring rows\n",
            "- [6] Latissimus dorsi (lats / sides of back)\n",
            "  - Pull-ups, wide grip pull-ups, ring rows, weighted pull-ups, muscle-up progressions\n",
            "- [8] Rhomboids / mid-traps (middle back)\n",
            "  - Inverted rows, ring rows, horizontal pulling movements emphasizing scapular retraction\n",
            "- [9] Erector spinae (lower back)\n",
            "  - Back extensions (if available), supermans, long-hold planks, slow Romanian deadlift variants (single-leg RDL)\n",
            "- [10] Serratus anterior (side of ribcage under armpit)\n",
            "  - Push-up plus, scapular push-ups, ring support protraction, overhead protraction work\n",
            "- [11] Obliques (side abs)\n",
            "  - Side planks, hanging oblique raises, windshield wipers, Russian twists\n",
            "- [12] Rectus abdominis (front abs)\n",
            "  - Hollow body holds, planks, hanging leg raises, ab wheel or rollout progressions\n",
            "- [13] Transverse abdominis / deep core\n",
            "  - Pallof press, bracing drills, vacuum-style bracing, dead bug progressions\n",
            "- [14] Gluteus maximus (butt)\n",
            "  - Hip thrust/glute bridge progressions, Nordic hamstrings (assist), single-leg hip drives, deep squats\n",
            "- [15] Quadriceps (front of thigh)\n",
            "  - Squats, pistol/single-leg squats, Bulgarian split squats, jump squats\n",
            "- [16] Hamstrings & calves (back of thigh & lower leg)\n",
            "  - Nordic hamstring curls, single-leg RDL, glute-ham raises, calf raises and eccentric calf work\n",
            "\n",
            "Quick programming notes tied to the diagram\n",
            "- Push muscles (chest, triceps, anterior delts): pair push-ups/dips with shoulder work and triceps accessories.\n",
            "- Pull muscles (lats, rhomboids, biceps): balance vertical pulls (pull-ups) with horizontal pulls (rows).\n",
            "- Legs: emphasize unilateral work (pistols/Bulgarian split) for balanced hypertrophy using calisthenics.\n",
            "- Core: mix anti-extension (planks/hollow), anti-rotation (Pallof/side-planks) and dynamic hanging work (leg raises/wipers).\n",
            "- Shoulder health: include serratus + scapular work (push-up plus, scapular pull-ups) every session.\n",
            "\n",
            "Want a downloadable image?\n",
            "- I can produce a neat SVG diagram (labelled front & back) you can open in any browser or save as PNG. Say “Yes — make an SVG” and I’ll generate it with these labels and exercises.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"It's still a bit hard to visualize. Can you make a diagram that shows the different muscle groups?\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with.\n",
            "Spent a total of 13790 tokens\n",
            "Response:\n",
            " Here’s an SVG you can open in a browser or save as a .svg file. It shows a simplified front (left) and back (right) figure with numbered markers and a legend mapping each number to the muscle group plus 1–2 example calisthenics exercises.\n",
            "\n",
            "To use:\n",
            "- Copy everything below into a text file and save it as muscles.svg\n",
            "- Open with a browser (Chrome/Firefox) or an image viewer that supports SVG\n",
            "\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1100\" height=\"1400\" viewBox=\"0 0 1100 1400\">\n",
            "  <style>\n",
            "    .title { font: bold 28px sans-serif; fill: #111; }\n",
            "    .panel { font: 18px sans-serif; fill: #222; }\n",
            "    .legend-title { font: bold 16px sans-serif; fill: #111; }\n",
            "    .legend { font: 14px sans-serif; fill: #111; }\n",
            "    .num { font: bold 12px sans-serif; fill: white; text-anchor: middle; dominant-baseline: central; }\n",
            "    .marker { fill: #d9534f; stroke: #b73736; stroke-width: 2; }\n",
            "    .figure-stroke { stroke: #333; stroke-width: 4; fill: none; stroke-linecap: round; stroke-linejoin: round; }\n",
            "    .figure-fill { fill: #f7f7f7; stroke: #555; stroke-width: 2; }\n",
            "    .box { fill: #fff; stroke: #ccc; stroke-width: 1; }\n",
            "  </style>\n",
            "\n",
            "  <rect x=\"10\" y=\"10\" width=\"1080\" height=\"1380\" class=\"box\" rx=\"10\" ry=\"10\"/>\n",
            "\n",
            "  <text x=\"50\" y=\"50\" class=\"title\">Calisthenics Muscle Map — Front & Back (numbered)</text>\n",
            "  <text x=\"50\" y=\"80\" class=\"panel\">Front view (left) and back view (right). Legend below maps numbers to muscle groups and example calisthenics exercises.</text>\n",
            "\n",
            "  <!-- LEFT: Front figure -->\n",
            "  <g transform=\"translate(50,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Front</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers front -->\n",
            "    <!-- 1 Pectoralis -->\n",
            "    <circle cx=\"140\" cy=\"140\" r=\"18\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"140\" class=\"num\">1</text>\n",
            "\n",
            "    <!-- 2 Anterior/medial deltoids (shoulder) -->\n",
            "    <circle cx=\"92\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"92\" y=\"110\" class=\"num\">2</text>\n",
            "\n",
            "    <!-- 3 Lateral/posterior deltoid (front-right shoulder area for labeling consistency) -->\n",
            "    <circle cx=\"188\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"110\" class=\"num\">3</text>\n",
            "\n",
            "    <!-- 4 Biceps (front upper arm left) -->\n",
            "    <circle cx=\"72\" cy=\"160\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"72\" y=\"160\" class=\"num\">4</text>\n",
            "\n",
            "    <!-- 10 Serratus (side of ribcage) -->\n",
            "    <circle cx=\"112\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"112\" y=\"170\" class=\"num\">10</text>\n",
            "\n",
            "    <!-- 11 Obliques (side abs) -->\n",
            "    <circle cx=\"170\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"170\" y=\"170\" class=\"num\">11</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (front abs) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (lower front) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 14 Glutes (left leg top) -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (right leg) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves (right lower leg) -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- RIGHT: Back figure -->\n",
            "  <g transform=\"translate(350,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Back</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers back -->\n",
            "    <!-- 5 Trapezius & upper back -->\n",
            "    <circle cx=\"140\" cy=\"100\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"100\" class=\"num\">5</text>\n",
            "\n",
            "    <!-- 6 Latissimus dorsi -->\n",
            "    <circle cx=\"188\" cy=\"150\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"150\" class=\"num\">6</text>\n",
            "\n",
            "    <!-- 8 Rhomboids / mid-traps -->\n",
            "    <circle cx=\"105\" cy=\"150\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"105\" y=\"150\" class=\"num\">8</text>\n",
            "\n",
            "    <!-- 9 Erector spinae (lower back) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">9</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (back side reference) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (lower front/back reference near pelvis) -->\n",
            "    <circle cx=\"140\" cy=\"260\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"260\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 14 Glutes -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (front leg reference on back view) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- Legend box -->\n",
            "  <g transform=\"translate(50,520)\">\n",
            "    <rect x=\"0\" y=\"0\" width=\"1000\" height=\"820\" rx=\"8\" ry=\"8\" fill=\"#ffffff\" stroke=\"#ddd\" />\n",
            "    <text x=\"18\" y=\"30\" class=\"legend-title\">Legend — number : muscle group — example calisthenics exercises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"60\" class=\"legend\">1 — Pectoralis major (chest): push-ups (incline → standard → decline), ring/parallel dips</text>\n",
            "    <text x=\"18\" y=\"90\" class=\"legend\">2 — Anterior / medial deltoids (front/mid shoulder): pike push-ups, handstand push-up progressions</text>\n",
            "    <text x=\"18\" y=\"120\" class=\"legend\">3 — Lateral / posterior deltoids (side/rear shoulder): band face pulls, rear-delt rows, Y/T/W</text>\n",
            "    <text x=\"18\" y=\"150\" class=\"legend\">4 — Biceps: chin-ups (supinated), ring curls, slow negatives</text>\n",
            "    <text x=\"18\" y=\"180\" class=\"legend\">5 — Trapezius & upper back: scapular pull-ups, shrugs (loaded carries), face pulls</text>\n",
            "    <text x=\"18\" y=\"210\" class=\"legend\">6 — Latissimus dorsi (lats): pull-ups, wide grip pull-ups, weighted pull-ups, muscle-up work</text>\n",
            "    <text x=\"18\" y=\"240\" class=\"legend\">8 — Rhomboids / mid-traps: inverted rows, ring rows, horizontal pulling emphasizing scapular retraction</text>\n",
            "    <text x=\"18\" y=\"270\" class=\"legend\">9 — Erector spinae (lower back): single-leg RDL, back extensions, supermans</text>\n",
            "\n",
            "    <text x=\"18\" y=\"310\" class=\"legend\">10 — Serratus anterior (side ribcage): push-up plus, scapular protraction on rings</text>\n",
            "    <text x=\"18\" y=\"340\" class=\"legend\">11 — Obliques (side abs): side plank dips, hanging oblique raises, windshield wipers</text>\n",
            "    <text x=\"18\" y=\"370\" class=\"legend\">12 — Rectus abdominis (front abs): hollow body holds, planks, hanging leg raises</text>\n",
            "    <text x=\"18\" y=\"400\" class=\"legend\">13 — Transverse abdominis / deep core: Pallof press, bracing drills, dead-bug</text>\n",
            "\n",
            "    <text x=\"18\" y=\"440\" class=\"legend\">14 — Gluteus maximus (glutes): single-leg hip thrusts, glute bridges, deep squats</text>\n",
            "    <text x=\"18\" y=\"470\" class=\"legend\">15 — Quadriceps (quads): squats, Bulgarian split squats, pistol progressions</text>\n",
            "    <text x=\"18\" y=\"500\" class=\"legend\">16 — Hamstrings & calves: Nordic hamstring curls, single-leg RDL, calf raises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"540\" class=\"legend-title\">Programming notes</text>\n",
            "    <text x=\"18\" y=\"570\" class=\"legend\">- Pair push and pull volume to avoid imbalance.</text>\n",
            "    <text x=\"18\" y=\"595\" class=\"legend\">- Train major groups 2–3×/week for hypertrophy. Mix heavy (3–6 reps) and moderate (6–12) sets.</text>\n",
            "    <text x=\"18\" y=\"620\" class=\"legend\">- Core work: mix anti-extension (plank/hollow), anti-rotation (Pallof/side-plank) and dynamic hanging leg raises.</text>\n",
            "    <text x=\"18\" y=\"645\" class=\"legend\">- Include scapular/shoulder prehab (band face pulls, scapular pull-ups) for healthy pressing and pulling.</text>\n",
            "\n",
            "    <text x=\"18\" y=\"700\" class=\"legend\">Tip: Save or print this SVG and use it while choosing exercises—tap the numbered targets to find suitable progressions.</text>\n",
            "  </g>\n",
            "\n",
            "  <text x=\"50\" y=\"1360\" class=\"panel\">If you want a different style (more anatomical accuracy, color-coded muscles, or a printable PDF), tell me which format and I’ll generate it.</text>\n",
            "</svg>\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"Yes make an SVG.\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Found 18 messages, dropping oldest 2 messages.\n",
            "Spent a total of 18447 tokens\n",
            "Response:\n",
            "  Below is a ready-to-run Python script that opens the SVG you pasted earlier. It tries to use pywebview (a lightweight native web-view wrapper) so a simple window pops up with the SVG rendered. If pywebview is not available it falls back to opening the SVG in your default web browser.\n",
            "\n",
            "1) Install pywebview (optional, recommended for an in-app window):\n",
            "   pip install pywebview\n",
            "\n",
            "2) Save the script below as view_svg.py and run:\n",
            "   python view_svg.py\n",
            "\n",
            "Script (save exactly, run with Python 3.7+):\n",
            "\n",
            "\n",
            "```python\n",
            "#!/usr/bin/env python3\n",
            "\"\"\"\n",
            "view_svg.py\n",
            "\n",
            "Open an embedded SVG in a window. Tries to use pywebview for an in-app window.\n",
            "Falls back to writing a temporary .svg file and opening it in the default browser.\n",
            "\n",
            "Usage:\n",
            "    python view_svg.py\n",
            "\"\"\"\n",
            "\n",
            "import os\n",
            "import sys\n",
            "import tempfile\n",
            "import webbrowser\n",
            "\n",
            "# Paste your SVG content here as a single triple-quoted string.\n",
            "# I used the SVG content you copied earlier.\n",
            "SVG_CONTENT = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1100\" height=\"1400\" viewBox=\"0 0 1100 1400\">\n",
            "  <style>\n",
            "    .title { font: bold 28px sans-serif; fill: #111; }\n",
            "    .panel { font: 18px sans-serif; fill: #222; }\n",
            "    .legend-title { font: bold 16px sans-serif; fill: #111; }\n",
            "    .legend { font: 14px sans-serif; fill: #111; }\n",
            "    .num { font: bold 12px sans-serif; fill: white; text-anchor: middle; dominant-baseline: central; }\n",
            "    .marker { fill: #d9534f; stroke: #b73736; stroke-width: 2; }\n",
            "    .figure-stroke { stroke: #333; stroke-width: 4; fill: none; stroke-linecap: round; stroke-linejoin: round; }\n",
            "    .figure-fill { fill: #f7f7f7; stroke: #555; stroke-width: 2; }\n",
            "    .box { fill: #fff; stroke: #ccc; stroke-width: 1; }\n",
            "  </style>\n",
            "\n",
            "  <rect x=\"10\" y=\"10\" width=\"1080\" height=\"1380\" class=\"box\" rx=\"10\" ry=\"10\"/>\n",
            "\n",
            "  <text x=\"50\" y=\"50\" class=\"title\">Calisthenics Muscle Map — Front & Back (numbered)</text>\n",
            "  <text x=\"50\" y=\"80\" class=\"panel\">Front view (left) and back view (right). Legend below maps numbers to muscle groups and example calisthenics exercises.</text>\n",
            "\n",
            "  <!-- LEFT: Front figure -->\n",
            "  <g transform=\"translate(50,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Front</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers front -->\n",
            "    <!-- 1 Pectoralis -->\n",
            "    <circle cx=\"140\" cy=\"140\" r=\"18\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"140\" class=\"num\">1</text>\n",
            "\n",
            "    <!-- 2 Anterior/medial deltoids (shoulder) -->\n",
            "    <circle cx=\"92\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"92\" y=\"110\" class=\"num\">2</text>\n",
            "\n",
            "    <!-- 3 Lateral/posterior deltoid (front-right shoulder area for labeling consistency) -->\n",
            "    <circle cx=\"188\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"110\" class=\"num\">3</text>\n",
            "\n",
            "    <!-- 4 Biceps (front upper arm left) -->\n",
            "    <circle cx=\"72\" cy=\"160\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"72\" y=\"160\" class=\"num\">4</text>\n",
            "\n",
            "    <!-- 10 Serratus (side of ribcage) -->\n",
            "    <circle cx=\"112\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"112\" y=\"170\" class=\"num\">10</text>\n",
            "\n",
            "    <!-- 11 Obliques (side abs) -->\n",
            "    <circle cx=\"170\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"170\" y=\"170\" class=\"num\">11</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (front abs) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (lower front) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 14 Glutes (left leg top) -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (right leg) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves (right lower leg) -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- RIGHT: Back figure -->\n",
            "  <g transform=\"translate(350,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Back</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers back -->\n",
            "    <!-- 5 Trapezius & upper back -->\n",
            "    <circle cx=\"140\" cy=\"100\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"100\" class=\"num\">5</text>\n",
            "\n",
            "    <!-- 6 Latissimus dorsi -->\n",
            "    <circle cx=\"188\" cy=\"150\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"150\" class=\"num\">6</text>\n",
            "\n",
            "    <!-- 8 Rhomboids / mid-traps -->\n",
            "    <circle cx=\"105\" cy=\"150\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"105\" y=\"150\" class=\"num\">8</text>\n",
            "\n",
            "    <!-- 9 Erector spinae (lower back) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">9</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (back side reference) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (lower front/back reference near pelvis) -->\n",
            "    <circle cx=\"140\" cy=\"260\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"260\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 14 Glutes -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (front leg reference on back view) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- Legend box -->\n",
            "  <g transform=\"translate(50,520)\">\n",
            "    <rect x=\"0\" y=\"0\" width=\"1000\" height=\"820\" rx=\"8\" ry=\"8\" fill=\"#ffffff\" stroke=\"#ddd\" />\n",
            "    <text x=\"18\" y=\"30\" class=\"legend-title\">Legend — number : muscle group — example calisthenics exercises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"60\" class=\"legend\">1 — Pectoralis major (chest): push-ups (incline → standard → decline), ring/parallel dips</text>\n",
            "    <text x=\"18\" y=\"90\" class=\"legend\">2 — Anterior / medial deltoids (front/mid shoulder): pike push-ups, handstand push-up progressions</text>\n",
            "    <text x=\"18\" y=\"120\" class=\"legend\">3 — Lateral / posterior deltoids (side/rear shoulder): band face pulls, rear-delt rows, Y/T/W</text>\n",
            "    <text x=\"18\" y=\"150\" class=\"legend\">4 — Biceps: chin-ups (supinated), ring curls, slow negatives</text>\n",
            "    <text x=\"18\" y=\"180\" class=\"legend\">5 — Trapezius & upper back: scapular pull-ups, shrugs (loaded carries), face pulls</text>\n",
            "    <text x=\"18\" y=\"210\" class=\"legend\">6 — Latissimus dorsi (lats): pull-ups, wide grip pull-ups, weighted pull-ups, muscle-up work</text>\n",
            "    <text x=\"18\" y=\"240\" class=\"legend\">8 — Rhomboids / mid-traps: inverted rows, ring rows, horizontal pulling emphasizing scapular retraction</text>\n",
            "    <text x=\"18\" y=\"270\" class=\"legend\">9 — Erector spinae (lower back): single-leg RDL, back extensions, supermans</text>\n",
            "\n",
            "    <text x=\"18\" y=\"310\" class=\"legend\">10 — Serratus anterior (side ribcage): push-up plus, scapular protraction on rings</text>\n",
            "    <text x=\"18\" y=\"340\" class=\"legend\">11 — Obliques (side abs): side plank dips, hanging oblique raises, windshield wipers</text>\n",
            "    <text x=\"18\" y=\"370\" class=\"legend\">12 — Rectus abdominis (front abs): hollow body holds, planks, hanging leg raises</text>\n",
            "    <text x=\"18\" y=\"400\" class=\"legend\">13 — Transverse abdominis / deep core: Pallof press, bracing drills, dead-bug</text>\n",
            "\n",
            "    <text x=\"18\" y=\"440\" class=\"legend\">14 — Gluteus maximus (glutes): single-leg hip thrusts, glute bridges, deep squats</text>\n",
            "    <text x=\"18\" y=\"470\" class=\"legend\">15 — Quadriceps (quads): squats, Bulgarian split squats, pistol progressions</text>\n",
            "    <text x=\"18\" y=\"500\" class=\"legend\">16 — Hamstrings & calves: Nordic hamstring curls, single-leg RDL, calf raises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"540\" class=\"legend-title\">Programming notes</text>\n",
            "    <text x=\"18\" y=\"570\" class=\"legend\">- Pair push and pull volume to avoid imbalance.</text>\n",
            "    <text x=\"18\" y=\"595\" class=\"legend\">- Train major groups 2–3×/week for hypertrophy. Mix heavy (3–6 reps) and moderate (6–12) sets.</text>\n",
            "    <text x=\"18\" y=\"620\" class=\"legend\">- Core work: mix anti-extension (plank/hollow), anti-rotation (Pallof/side-plank) and dynamic hanging leg raises.</text>\n",
            "    <text x=\"18\" y=\"645\" class=\"legend\">- Include scapular/shoulder prehab (band face pulls, scapular pull-ups) for healthy pressing and pulling.</text>\n",
            "\n",
            "    <text x=\"18\" y=\"700\" class=\"legend\">Tip: Save or print this SVG and use it while choosing exercises—tap the numbered targets to find suitable progressions.</text>\n",
            "  </g>\n",
            "\n",
            "  <text x=\"50\" y=\"1360\" class=\"panel\">If you want a different style (more anatomical accuracy, color-coded muscles, or a printable PDF), tell me which format and I’ll generate it.</text>\n",
            "</svg>\n",
            "\"\"\"\n",
            "\n",
            "def open_with_pywebview(html_content: str):\n",
            "    try:\n",
            "        import webview  # pywebview\n",
            "    except Exception as e:\n",
            "        raise RuntimeError(\"pywebview is not available\") from e\n",
            "\n",
            "    # Create a simple HTML wrapper so the browser knows it's SVG content and scales nicely\n",
            "    html_wrapper = f\"\"\"<!doctype html>\n",
            "<html>\n",
            "  <head>\n",
            "    <meta charset=\"utf-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "    <title>Calisthenics Muscle Map</title>\n",
            "    <style>html,body{{height:100%;margin:0;background:#f3f4f6}} svg{{display:block;margin:auto;max-width:100%;height:auto}}</style>\n",
            "  </head>\n",
            "  <body>\n",
            "    {html_content}\n",
            "  </body>\n",
            "</html>\"\"\"\n",
            "\n",
            "    # Create and show window\n",
            "    webview.create_window(\"Calisthenics Muscle Map\", html=html_wrapper, width=1000, height=800)\n",
            "    webview.start()\n",
            "\n",
            "def open_in_browser(svg_text: str):\n",
            "    # Write SVG to temporary file and open it in default browser\n",
            "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".svg\", prefix=\"calisthenics_\")\n",
            "    try:\n",
            "        tmp.write(svg_text.encode(\"utf-8\"))\n",
            "        tmp.flush()\n",
            "        tmp.close()\n",
            "        path = os.path.abspath(tmp.name)\n",
            "        url = \"file://\" + path\n",
            "        print(\"Opening SVG in default web browser:\", url)\n",
            "        webbrowser.open(url)\n",
            "    except Exception:\n",
            "        # Cleanup on error\n",
            "        try:\n",
            "            os.unlink(tmp.name)\n",
            "        except Exception:\n",
            "            pass\n",
            "        raise\n",
            "\n",
            "def main():\n",
            "    # Try pywebview first for an embedded window\n",
            "    try:\n",
            "        open_with_pywebview(SVG_CONTENT)\n",
            "    except Exception as e:\n",
            "        print(\"pywebview not available or failed to start:\", e)\n",
            "        print(\"Falling back to opening the SVG in your default web browser.\")\n",
            "        open_in_browser(SVG_CONTENT)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "Notes and troubleshooting\n",
            "- If you want the in-app window experience, install pywebview: pip install pywebview\n",
            "  - On some platforms pywebview may require additional runtime libraries (it uses a native web renderer).\n",
            "- If pywebview can't start you’ll still be able to view the SVG via your default browser (fallback).\n",
            "- If you prefer to open an existing SVG file instead of the embedded string, I can modify the script to accept a filename argument.\n",
            "\n",
            "Want me to add: clickable markers that open more info, or export to PNG/PDF from the script?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"I copied the svg html. Make a script in Python that can open an svg with the earlier given html. I should be able to run the python script and a window should pop up where I can view the svg html.\"},\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n \", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pywebview not available or failed to start: pywebview is not available\n",
            "Falling back to opening the SVG in your default web browser.\n",
            "Opening SVG in default web browser: file://C:\\Users\\rishi\\AppData\\Local\\Temp\\calisthenics_9f08d99j.svg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import webbrowser\n",
        "\n",
        "# Paste your SVG content here as a single triple-quoted string.\n",
        "# I used the SVG content you copied earlier.\n",
        "SVG_CONTENT = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1100\" height=\"1400\" viewBox=\"0 0 1100 1400\">\n",
        "  <style>\n",
        "    .title { font: bold 28px sans-serif; fill: #111; }\n",
        "    .panel { font: 18px sans-serif; fill: #222; }\n",
        "    .legend-title { font: bold 16px sans-serif; fill: #111; }\n",
        "    .legend { font: 14px sans-serif; fill: #111; }\n",
        "    .num { font: bold 12px sans-serif; fill: white; text-anchor: middle; dominant-baseline: central; }\n",
        "    .marker { fill: #d9534f; stroke: #b73736; stroke-width: 2; }\n",
        "    .figure-stroke { stroke: #333; stroke-width: 4; fill: none; stroke-linecap: round; stroke-linejoin: round; }\n",
        "    .figure-fill { fill: #f7f7f7; stroke: #555; stroke-width: 2; }\n",
        "    .box { fill: #fff; stroke: #ccc; stroke-width: 1; }\n",
        "  </style>\n",
        "\n",
        "  <rect x=\"10\" y=\"10\" width=\"1080\" height=\"1380\" class=\"box\" rx=\"10\" ry=\"10\"/>\n",
        "\n",
        "  <text x=\"50\" y=\"50\" class=\"title\">Calisthenics Muscle Map — Front & Back (numbered)</text>\n",
        "  <text x=\"50\" y=\"80\" class=\"panel\">Front view (left) and back view (right). Legend below maps numbers to muscle groups and example calisthenics exercises.</text>\n",
        "\n",
        "  <!-- LEFT: Front figure -->\n",
        "  <g transform=\"translate(50,120)\">\n",
        "    <text x=\"140\" y=\"-10\" class=\"panel\">Front</text>\n",
        "    <!-- head -->\n",
        "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
        "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
        "    <!-- torso -->\n",
        "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
        "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
        "    <!-- arms -->\n",
        "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
        "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
        "    <!-- legs -->\n",
        "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
        "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
        "\n",
        "    <!-- Markers front -->\n",
        "    <!-- 1 Pectoralis -->\n",
        "    <circle cx=\"140\" cy=\"140\" r=\"18\" class=\"marker\"/>\n",
        "    <text x=\"140\" y=\"140\" class=\"num\">1</text>\n",
        "\n",
        "    <!-- 2 Anterior/medial deltoids (shoulder) -->\n",
        "    <circle cx=\"92\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
        "    <text x=\"92\" y=\"110\" class=\"num\">2</text>\n",
        "\n",
        "    <!-- 3 Lateral/posterior deltoid (front-right shoulder area for labeling consistency) -->\n",
        "    <circle cx=\"188\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
        "    <text x=\"188\" y=\"110\" class=\"num\">3</text>\n",
        "\n",
        "    <!-- 4 Biceps (front upper arm left) -->\n",
        "    <circle cx=\"72\" cy=\"160\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"72\" y=\"160\" class=\"num\">4</text>\n",
        "\n",
        "    <!-- 10 Serratus (side of ribcage) -->\n",
        "    <circle cx=\"112\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"112\" y=\"170\" class=\"num\">10</text>\n",
        "\n",
        "    <!-- 11 Obliques (side abs) -->\n",
        "    <circle cx=\"170\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"170\" y=\"170\" class=\"num\">11</text>\n",
        "\n",
        "    <!-- 12 Rectus abdominis (front abs) -->\n",
        "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
        "    <text x=\"140\" y=\"200\" class=\"num\">12</text>\n",
        "\n",
        "    <!-- 13 Transverse abdominis / deep core (lower front) -->\n",
        "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
        "\n",
        "    <!-- 14 Glutes (left leg top) -->\n",
        "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
        "\n",
        "    <!-- 15 Quadriceps (right leg) -->\n",
        "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
        "\n",
        "    <!-- 16 Hamstrings/calves (right lower leg) -->\n",
        "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
        "  </g>\n",
        "\n",
        "  <!-- RIGHT: Back figure -->\n",
        "  <g transform=\"translate(350,120)\">\n",
        "    <text x=\"140\" y=\"-10\" class=\"panel\">Back</text>\n",
        "    <!-- head -->\n",
        "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
        "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
        "    <!-- torso -->\n",
        "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
        "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
        "    <!-- arms -->\n",
        "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
        "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
        "    <!-- legs -->\n",
        "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
        "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
        "\n",
        "    <!-- Markers back -->\n",
        "    <!-- 5 Trapezius & upper back -->\n",
        "    <circle cx=\"140\" cy=\"100\" r=\"16\" class=\"marker\"/>\n",
        "    <text x=\"140\" y=\"100\" class=\"num\">5</text>\n",
        "\n",
        "    <!-- 6 Latissimus dorsi -->\n",
        "    <circle cx=\"188\" cy=\"150\" r=\"16\" class=\"marker\"/>\n",
        "    <text x=\"188\" y=\"150\" class=\"num\">6</text>\n",
        "\n",
        "    <!-- 8 Rhomboids / mid-traps -->\n",
        "    <circle cx=\"105\" cy=\"150\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"105\" y=\"150\" class=\"num\">8</text>\n",
        "\n",
        "    <!-- 9 Erector spinae (lower back) -->\n",
        "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
        "    <text x=\"140\" y=\"200\" class=\"num\">9</text>\n",
        "\n",
        "    <!-- 13 Transverse abdominis / deep core (back side reference) -->\n",
        "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
        "\n",
        "    <!-- 12 Rectus abdominis (lower front/back reference near pelvis) -->\n",
        "    <circle cx=\"140\" cy=\"260\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"140\" y=\"260\" class=\"num\">12</text>\n",
        "\n",
        "    <!-- 14 Glutes -->\n",
        "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
        "\n",
        "    <!-- 15 Quadriceps (front leg reference on back view) -->\n",
        "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
        "\n",
        "    <!-- 16 Hamstrings/calves -->\n",
        "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
        "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
        "  </g>\n",
        "\n",
        "  <!-- Legend box -->\n",
        "  <g transform=\"translate(50,520)\">\n",
        "    <rect x=\"0\" y=\"0\" width=\"1000\" height=\"820\" rx=\"8\" ry=\"8\" fill=\"#ffffff\" stroke=\"#ddd\" />\n",
        "    <text x=\"18\" y=\"30\" class=\"legend-title\">Legend — number : muscle group — example calisthenics exercises</text>\n",
        "\n",
        "    <text x=\"18\" y=\"60\" class=\"legend\">1 — Pectoralis major (chest): push-ups (incline → standard → decline), ring/parallel dips</text>\n",
        "    <text x=\"18\" y=\"90\" class=\"legend\">2 — Anterior / medial deltoids (front/mid shoulder): pike push-ups, handstand push-up progressions</text>\n",
        "    <text x=\"18\" y=\"120\" class=\"legend\">3 — Lateral / posterior deltoids (side/rear shoulder): band face pulls, rear-delt rows, Y/T/W</text>\n",
        "    <text x=\"18\" y=\"150\" class=\"legend\">4 — Biceps: chin-ups (supinated), ring curls, slow negatives</text>\n",
        "    <text x=\"18\" y=\"180\" class=\"legend\">5 — Trapezius & upper back: scapular pull-ups, shrugs (loaded carries), face pulls</text>\n",
        "    <text x=\"18\" y=\"210\" class=\"legend\">6 — Latissimus dorsi (lats): pull-ups, wide grip pull-ups, weighted pull-ups, muscle-up work</text>\n",
        "    <text x=\"18\" y=\"240\" class=\"legend\">8 — Rhomboids / mid-traps: inverted rows, ring rows, horizontal pulling emphasizing scapular retraction</text>\n",
        "    <text x=\"18\" y=\"270\" class=\"legend\">9 — Erector spinae (lower back): single-leg RDL, back extensions, supermans</text>\n",
        "\n",
        "    <text x=\"18\" y=\"310\" class=\"legend\">10 — Serratus anterior (side ribcage): push-up plus, scapular protraction on rings</text>\n",
        "    <text x=\"18\" y=\"340\" class=\"legend\">11 — Obliques (side abs): side plank dips, hanging oblique raises, windshield wipers</text>\n",
        "    <text x=\"18\" y=\"370\" class=\"legend\">12 — Rectus abdominis (front abs): hollow body holds, planks, hanging leg raises</text>\n",
        "    <text x=\"18\" y=\"400\" class=\"legend\">13 — Transverse abdominis / deep core: Pallof press, bracing drills, dead-bug</text>\n",
        "\n",
        "    <text x=\"18\" y=\"440\" class=\"legend\">14 — Gluteus maximus (glutes): single-leg hip thrusts, glute bridges, deep squats</text>\n",
        "    <text x=\"18\" y=\"470\" class=\"legend\">15 — Quadriceps (quads): squats, Bulgarian split squats, pistol progressions</text>\n",
        "    <text x=\"18\" y=\"500\" class=\"legend\">16 — Hamstrings & calves: Nordic hamstring curls, single-leg RDL, calf raises</text>\n",
        "\n",
        "    <text x=\"18\" y=\"540\" class=\"legend-title\">Programming notes</text>\n",
        "    <text x=\"18\" y=\"570\" class=\"legend\">- Pair push and pull volume to avoid imbalance.</text>\n",
        "    <text x=\"18\" y=\"595\" class=\"legend\">- Train major groups 2–3×/week for hypertrophy. Mix heavy (3–6 reps) and moderate (6–12) sets.</text>\n",
        "    <text x=\"18\" y=\"620\" class=\"legend\">- Core work: mix anti-extension (plank/hollow), anti-rotation (Pallof/side-plank) and dynamic hanging leg raises.</text>\n",
        "    <text x=\"18\" y=\"645\" class=\"legend\">- Include scapular/shoulder prehab (band face pulls, scapular pull-ups) for healthy pressing and pulling.</text>\n",
        "\n",
        "    <text x=\"18\" y=\"700\" class=\"legend\">Tip: Save or print this SVG and use it while choosing exercises—tap the numbered targets to find suitable progressions.</text>\n",
        "  </g>\n",
        "\n",
        "  <text x=\"50\" y=\"1360\" class=\"panel\">If you want a different style (more anatomical accuracy, color-coded muscles, or a printable PDF), tell me which format and I’ll generate it.</text>\n",
        "</svg>\n",
        "\"\"\"\n",
        "\n",
        "def open_with_pywebview(html_content: str):\n",
        "    try:\n",
        "        import webview  # pywebview\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"pywebview is not available\") from e\n",
        "\n",
        "    # Create a simple HTML wrapper so the browser knows it's SVG content and scales nicely\n",
        "    html_wrapper = f\"\"\"<!doctype html>\n",
        "<html>\n",
        "  <head>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "    <title>Calisthenics Muscle Map</title>\n",
        "    <style>html,body{{height:100%;margin:0;background:#f3f4f6}} svg{{display:block;margin:auto;max-width:100%;height:auto}}</style>\n",
        "  </head>\n",
        "  <body>\n",
        "    {html_content}\n",
        "  </body>\n",
        "</html>\"\"\"\n",
        "\n",
        "    # Create and show window\n",
        "    webview.create_window(\"Calisthenics Muscle Map\", html=html_wrapper, width=1000, height=800)\n",
        "    webview.start()\n",
        "\n",
        "def open_in_browser(svg_text: str):\n",
        "    # Write SVG to temporary file and open it in default browser\n",
        "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".svg\", prefix=\"calisthenics_\")\n",
        "    try:\n",
        "        tmp.write(svg_text.encode(\"utf-8\"))\n",
        "        tmp.flush()\n",
        "        tmp.close()\n",
        "        path = os.path.abspath(tmp.name)\n",
        "        url = \"file://\" + path\n",
        "        print(\"Opening SVG in default web browser:\", url)\n",
        "        webbrowser.open(url)\n",
        "    except Exception:\n",
        "        # Cleanup on error\n",
        "        try:\n",
        "            os.unlink(tmp.name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    # Try pywebview first for an embedded window\n",
        "    try:\n",
        "        open_with_pywebview(SVG_CONTENT)\n",
        "    except Exception as e:\n",
        "        print(\"pywebview not available or failed to start:\", e)\n",
        "        print(\"Falling back to opening the SVG in your default web browser.\")\n",
        "        open_in_browser(SVG_CONTENT)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Found 18 messages, dropping oldest 2 messages.\n",
            "Spent a total of 21591 tokens\n",
            "Response:\n",
            " Two quick answers:\n",
            "\n",
            "1) The math\n",
            "- Your age = 25, your name \"Sagun\" has 5 letters.  \n",
            "- 25^5 = 9,765,625.\n",
            "\n",
            "2) The SVG XML error and how to fix it\n",
            "- The browser error \"xmlParseEntityRef: no name\" almost always means there is a raw ampersand (&) somewhere inside text in the SVG. In XML (and SVG) you must escape ampersands as &amp; (and also escape < and > if they appear in text nodes).\n",
            "- In the SVG you pasted there is an unescaped \"&\" (for example in the title \"Front & Back\"). That produces the parsing error.\n",
            "\n",
            "Two simple fixes:\n",
            "A) Manual: open the .svg file in a text editor and replace any \"&\" used in text with \"&amp;\" (e.g., change \"Front & Back\" → \"Front &amp; Back\").\n",
            "\n",
            "B) Safer automated fix in Python: sanitize ampersands that are not already part of an entity before writing the temp file. Below is an updated version of the viewing script that performs that sanitization and then opens the SVG in your browser (or via pywebview if available).\n",
            "\n",
            "Save this as view_svg.py and run python view_svg.py:\n",
            "\n",
            "```python\n",
            "#!/usr/bin/env python3\n",
            "\"\"\"\n",
            "view_svg.py\n",
            "\n",
            "Open the embedded SVG. This script sanitizes raw '&' characters in text\n",
            "so the SVG is valid XML and will not trigger xmlParseEntityRef errors.\n",
            "\"\"\"\n",
            "import os\n",
            "import tempfile\n",
            "import webbrowser\n",
            "import re\n",
            "\n",
            "SVG_CONTENT = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<!-- paste your SVG here (exact content you received) -->\n",
            "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1100\" height=\"1400\" viewBox=\"0 0 1100 1400\">\n",
            "  <!-- ... rest of svg ... -->\n",
            "  <text>Calisthenics Muscle Map — Front & Back (numbered)</text>\n",
            "</svg>\n",
            "\"\"\"\n",
            "\n",
            "def sanitize_ampersands(svg_text: str) -> str:\n",
            "    # Replace '&' that are NOT already the start of a valid entity such as &amp;, &lt;, &gt;, &quot;, &apos;,\n",
            "    # numeric entities like &#1234;, or hex numeric entities like &#x1A3F;\n",
            "    return re.sub(r'&(?!amp;|lt;|gt;|quot;|apos;|#\\d+;|#x[0-9A-Fa-f]+;)', '&amp;', svg_text)\n",
            "\n",
            "def open_in_browser(svg_text: str):\n",
            "    # Sanitize to avoid xmlParseEntityRef error\n",
            "    clean_svg = sanitize_ampersands(svg_text)\n",
            "\n",
            "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".svg\", prefix=\"calisthenics_\")\n",
            "    try:\n",
            "        tmp.write(clean_svg.encode(\"utf-8\"))\n",
            "        tmp.flush()\n",
            "        tmp.close()\n",
            "        path = os.path.abspath(tmp.name)\n",
            "        url = \"file://\" + path\n",
            "        print(\"Opening SVG in default web browser:\", url)\n",
            "        webbrowser.open(url)\n",
            "    except Exception as e:\n",
            "        # Cleanup on error\n",
            "        try:\n",
            "            os.unlink(tmp.name)\n",
            "        except Exception:\n",
            "            pass\n",
            "        raise\n",
            "\n",
            "def main():\n",
            "    try:\n",
            "        # Try to use pywebview if installed (optional)\n",
            "        import webview\n",
            "        # Use sanitized html wrapper so pywebview can display it.\n",
            "        safe_svg = sanitize_ampersands(SVG_CONTENT)\n",
            "        html = f\"<!doctype html><html><meta charset='utf-8'><body>{safe_svg}</body></html>\"\n",
            "        webview.create_window(\"Calisthenics Muscle Map\", html=html, width=1000, height=800)\n",
            "        webview.start()\n",
            "    except Exception:\n",
            "        print(\"pywebview not available or failed; opening in default browser instead.\")\n",
            "        open_in_browser(SVG_CONTENT)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "Notes:\n",
            "- Replace the placeholder SVG_CONTENT with your actual SVG text (the full SVG provided earlier).\n",
            "- The sanitize_ampersands function uses a regex to only replace ampersands that are not already valid XML entities, avoiding double-escaping.\n",
            "- You can still manually correct the file by changing \"&\" to \"&amp;\" where appropriate (title, legend, etc.).\n",
            "\n",
            "If you want, paste the exact lines around the reported error (line 17 from the temp file) and I’ll point out the exact substring that needs replacing or produce a corrected SVG for you.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    pipeline_with_history_and_summary,\n",
        "    {\"query\": \"What is my age to the power of the number of letters in my name? Also when my browser pulls up the temporary svg file I'm getting this error. How to fix: \"\n",
        "    \"This page contains the following errors:\"\n",
        "    \"error on line 17 at column 70: xmlParseEntityRef: no name\"},\n",
        "    # config = {\"configurable\": {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}}\n",
        "    config = {\"session_id\": \"example_with_summary_and_window_history\", \"llm\": llm, \"k\": 16}\n",
        ")\n",
        "print(\"Response:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: Conversation so far:\n",
            "\n",
            "- User opened casually (“Good morning dude. What's popping today?”). Assistant replied upbeat, offered several options (short morning plan, brainstorming, drafting/polishing messages, 3‑song playlist, quick recipe/meal plan, help with code/study/summaries, joke/creative piece) and asked which option the user wants or to describe what they’re up to.\n",
            "\n",
            "- User then asked: “Explain what calesthenics is and how it can be used to improve physical fitness. My name is Sagun and I am 25 years old.”\n",
            "\n",
            "- Assistant replied (addressing Sagun, and noting the spelling is usually “calisthenics”) with a detailed, practical overview covering:\n",
            "  - Definition and focus (bodyweight training, movement patterns)\n",
            "  - Benefits (strength, coordination, mobility, low cost, scalability)\n",
            "  - Key training principles (progressive overload, specificity, consistency, technique, recovery)\n",
            "  - Core exercise categories and examples (push, pull, legs, core, full‑body/skill)\n",
            "  - How to structure workouts by goal (strength, hypertrophy, endurance, skill work)\n",
            "  - A simple beginner 3x/week full‑body plan including warm‑up, exercises, sets/reps, conditioning finisher and cooldown\n",
            "  - Progression examples for push‑ups, pull‑ups, squats\n",
            "  - Tracking tips and example benchmarks\n",
            "  - Nutrition and recovery basics (protein, calorie balance, sleep, hydration)\n",
            "  - Safety tips (warm‑ups, wrist mobility, using negatives/bands, avoid sharp pain)\n",
            "  - A closing offer to tailor a 4‑week plan if Sagun provides current fitness level, any injuries, goals, and how many days/week he wants to train.\n",
            "\n",
            "Next actions: user can pick one of the assistant’s original offered options or respond to the assistant’s follow‑up by giving fitness level, injuries (if any), goals, and preferred training frequency so the assistant can create a tailored 4‑week plan.\n",
            "Human: Okay focus on calestinics for muscle hypertrophy and stength training.\n",
            "AI: Nice — focusing on calisthenics for muscle hypertrophy + strength is a great combo. Below is a practical, actionable guide for you, Sagun, including principles, exercise choices, progressions, and a sample 4-week program you can start with. If you want it tailored to your exact level or schedule, tell me how many days/week you can train and your current ability (e.g., strict pull-ups, push-up count, pistol squats).\n",
            "\n",
            "Key principles\n",
            "- Specificity: train heavy, low-rep work for strength and moderate-rep work for hypertrophy. Do both in the same week.\n",
            "- Progressive overload: increase difficulty by adding reps, better variations, slower tempo, pauses, or adding external load (vest/chain/dumbbell).\n",
            "- Frequency: hit each major muscle group 2–3x/week for optimal hypertrophy and strength.\n",
            "- Volume: aim for ~10–20+ effective sets per muscle group per week for hypertrophy (start lower and ramp up).\n",
            "- Intensity & rest: strength sets (3–6 reps) — rest 2–4 minutes; hypertrophy sets (6–12 reps, up to 15–20 for some calisthenic moves) — rest 60–90s.\n",
            "- Technique: prioritize clean reps and full range of motion. Use negatives/isometrics rather than sloppy reps.\n",
            "- Recovery: calories, protein (~1.6–2.2 g/kg), 7–9 hours sleep, and deloads every 4–8 weeks if needed.\n",
            "\n",
            "Equipment useful to accelerate progress\n",
            "- Pull-up bar (or rings)\n",
            "- Dip bars/rings\n",
            "- Weighted vest or belt for plates/dumbbells\n",
            "- Resistance bands for assistance\n",
            "- Parallettes (for wrist/push work)\n",
            "- Optional: kettlebell/dumbbell for goblet variance\n",
            "\n",
            "Core exercise categories & progressions (quick)\n",
            "- Horizontal push: incline push-ups → regular → decline → archer → one-arm negatives → weighted push-ups\n",
            "- Vertical push: pike push-ups → elevated pike → partial handstand → freestanding handstand push-up → weighted HSPU\n",
            "- Horizontal pull: inverted/Australian rows → ring rows → strict pull-ups → weighted pull-ups → typewriter/archer pulls\n",
            "- Vertical pull: chin-ups → pull-ups → weighted pull-ups → muscle-up progressions\n",
            "- Legs: bodyweight squat → Bulgarian split → shrimp/pistol progressions → assisted pistol → weighted pistol\n",
            "- Core: hollow body → L-sit → hanging knee raise → hanging leg raise → toes-to-bar → windshield wipers\n",
            "\n",
            "Advanced strength tools (particularly useful in calisthenics)\n",
            "- Eccentric-only sets (slow negatives, 3–6s)\n",
            "- Paused reps / isometrics (mid-range holds)\n",
            "- Weighted calisthenics (vests / belts)\n",
            "- Cluster sets and low-volume max-effort sets for strength\n",
            "- Tempo manipulation (slow eccentrics, explosive concentric)\n",
            "\n",
            "Programming templates\n",
            "Option A — 4 days/week (mix strength + hypertrophy)\n",
            "- Day 1 — Upper Strength (heavy)\n",
            "  - Heavy pull-up work: 4–6 sets × 3–6 reps (weighted if possible)\n",
            "  - Heavy dip/push movement: 4–6 sets × 3–6 reps (weighted dips or weighted push-ups)\n",
            "  - Accessory rows: 3 sets × 6–10\n",
            "  - Core heavy isometric: 3 × 10–20s front lever progression or L-sit hold\n",
            "- Day 2 — Lower + Conditioning\n",
            "  - Pistol/split-squat variation: 4–5 sets × 4–8 per leg\n",
            "  - Hamstring/glute single-leg RDL or Nordic hamstring: 3–4 × 6–10\n",
            "  - Calf work: 3 × 10–20\n",
            "  - Conditioning finisher 8–12 min EMOM or interval\n",
            "- Day 3 — Upper Hypertrophy\n",
            "  - Push: 4 × 8–12 (diamond/arched/decline push-ups or ring dips)\n",
            "  - Pull: 4 × 6–12 (strict pull-ups/rows; use bands to hit rep targets)\n",
            "  - Accessory shoulders: 3 × 8–12 (pike push-ups, face pulls/band work)\n",
            "  - Core dynamic: 3 × 10–15 hanging leg raises\n",
            "- Day 4 — Full Lower Hypertrophy + Skill\n",
            "  - Squat variations or Bulgarian split: 4 × 8–12\n",
            "  - Explosive: jump squats or step-ups 3 × 6–10\n",
            "  - Core skill: 10–15 min handstand or planche progressions\n",
            "  - Mobility/cooldown\n",
            "\n",
            "Option B — 3 days/week full-body (if short on time)\n",
            "- Each session: 1 heavy push (3–6 reps), 1 heavy pull (3–6), 1 leg compound (6–10), 1 hypertrophy accessory set (8–15), core.\n",
            "- Track weekly volume and increase difficulty each week.\n",
            "\n",
            "Sample 4-week microcycle (4 days/week) — conservative starter for strength + hypertrophy\n",
            "Weeks 1–2 (establish base)\n",
            "- Upper Strength: Weighted pull-ups 5×4–6; Weighted dips 5×4–6; Ring rows 3×8; L-sit 3×15s\n",
            "- Lower Strength: Bulgarian split 4×5–8; Nordic/hamstring RDL 3×6–8; Calves 3×12\n",
            "- Upper Hypertrophy: Push-ups (decline/weighted if needed) 4×8–12; Pull-ups or rows 4×8–12; Pike push-ups 3×8–12; Hanging leg raises 3×10–15\n",
            "- Lower Hypertrophy + Skill: Pistols assisted 4×6–10; Jump squats 3×6–10; Core skill 10 minutes\n",
            "Progression: add 1 rep per set each week or move to harder variation; if you can do full target reps easily, add load or harder progression.\n",
            "\n",
            "Weeks 3–4 (increase intensity & density)\n",
            "- Increase weight on strength days or shift to tougher variations.\n",
            "- For hypertrophy days, add an extra set per muscle group or shorten rest slightly.\n",
            "- Introduce one eccentric-focused set for your toughest lift (e.g., 3–5s negatives × 3–5 reps).\n",
            "\n",
            "How to progress practically (week-to-week)\n",
            "- If you hit top of rep range for all sets across a session, move to next variation or add ~2.5–5 kg via vest/weight.\n",
            "- If you barely hit lower end, keep same variation until you add reps across multiple sessions.\n",
            "- Track RPE; aim for sets near 7–9 RPE on hypertrophy work and 8–10 RPE on strength work.\n",
            "\n",
            "Tempo & set design examples\n",
            "- Strength main sets: explosive concentric, controlled eccentric (2–3s), 3–6 reps, 3–6 sets, long rests.\n",
            "- Hypertrophy sets: 3–4s eccentric, 1s pause at stretch, controlled concentric, 8–12 reps, 3–4 sets.\n",
            "- Eccentric overload: 4–6 reps, 5–6s negative, assistance on concentric if needed.\n",
            "- Isometric holds: 3–5 sets × 10–30s in a taxing position (e.g., tuck front lever, ring support).\n",
            "\n",
            "Nutrition & recovery (short)\n",
            "- Protein: 1.6–2.2 g/kg bodyweight (e.g., if you’re 75 kg, ~120–165 g/day).\n",
            "- Calories: slight surplus (+250–500 kcal/day) for hypertrophy; maintain for strength if you want minimal fat gain.\n",
            "- Sleep: 7–9 hours and manage stress.\n",
            "- Hydration and mobility: daily shoulder & hip mobility; warm up scapular and thoracic.\n",
            "\n",
            "Tracking & tests (biweekly)\n",
            "- Log exercise, sets, reps, variation, RPE, and bodyweight.\n",
            "- Strength benchmarks: aim for progressive milestones (e.g., 5–8 strict pull-ups, 10–20 push-ups, a clean weighted dip with +10–20% bodyweight).\n",
            "- Measure progress via strength increases, body composition changes, and consistency.\n",
            "\n",
            "Common mistakes to avoid\n",
            "- Doing endless high-rep sets of easy variations (low stimulus). Progress to harder variations.\n",
            "- Skipping eccentric and isometric work — they’re huge for strength and hypertrophy.\n",
            "- Not tracking or increasing difficulty over time.\n",
            "- Poor warm-up and scapular control leading to shoulder issues.\n",
            "\n",
            "Example session (Upper Strength day) — concrete\n",
            "- Warm-up: 5–8 min cardio + band pull-aparts + scapular pull-ups\n",
            "- Weighted pull-ups: 5 sets × 4–6 reps (2–4 min rest)\n",
            "- Weighted dips (or weighted push-ups): 5 × 4–6 (2–4 min rest)\n",
            "- Ring rows: 3 × 8–10 (90s rest)\n",
            "- Pike push-ups / HSPU progressions: 3 × 6–10\n",
            "- L-sit: 3 × 15–25s\n",
            "- Cooldown/stretch\n",
            "\n",
            "Want it tailored?\n",
            "Tell me:\n",
            "- Your current level (e.g., max reps for pull-ups and push-ups; pistol abilities)\n",
            "- Any injuries\n",
            "- How many days/week you want to train\n",
            "I’ll make a 4–8 week program scaled exactly for you, Sagun.\n",
            "Human: Okay breaksown the chemical processes involved in muscle hypertrophy and strength training.\n",
            "AI: Nice — Sagun. Below is a focused breakdown of the main chemical and molecular processes that drive muscle hypertrophy and strength gains. I’ll explain what happens at the cellular/molecular level during and after resistance exercise, the key signaling pathways, the role of hormones and nutrients, and how catabolic signals and neural factors fit in. I’ll finish with practical implications you can use for training/nutrition.\n",
            "\n",
            "1) The three mechanical triggers\n",
            "- Mechanical tension: heavy load/stretch on muscle fibers creates structural strain that starts mechanotransduction signaling (the dominant stimulus for hypertrophy).\n",
            "- Muscle damage: microtears and disruption of contractile proteins trigger inflammation and repair/remodeling.\n",
            "- Metabolic stress: accumulation of metabolites (lactate, H+, inorganic phosphate, ADP) from intense work promotes anabolic signaling and cell swelling.\n",
            "\n",
            "2) Mechanotransduction — how force becomes a biochemical signal\n",
            "- Integrins, focal adhesion kinase (FAK), and other costamere proteins sense stretch and load. Activation of FAK and associated complexes recruits downstream kinases (e.g., MAPKs) that contribute to growth signaling.\n",
            "- Titin and other sarcomeric proteins can act as stretch sensors, altering kinase activity and gene expression.\n",
            "- Stretch-activated ion channels and increased intracellular Ca2+ also transduce mechanical signals into biochemical cascades.\n",
            "\n",
            "3) The central anabolic hub: PI3K → Akt → mTORC1\n",
            "- Resistance exercise + amino acids (especially leucine) activate the PI3K → Akt (PKB) pathway.\n",
            "- Akt activates mTORC1 (mechanistic target of rapamycin complex 1), the key regulator of protein synthesis.\n",
            "- mTORC1 phosphorylates S6 kinase 1 (S6K1) and 4E-BP1, increasing ribosomal activity and cap-dependent translation — raising synthesis of contractile proteins (myosin, actin) and ribosomal proteins.\n",
            "- Amino acids are sensed by Rag GTPases/Ragulator on lysosomes; leucine is a strong activator of this route and synergizes with Akt signaling.\n",
            "- Net effect: increased muscle protein synthesis (MPS) relative to breakdown => hypertrophy over time.\n",
            "\n",
            "4) Role of satellite cells and myonuclei\n",
            "- Satellite cells (muscle stem cells; Pax7+ cells) are activated by mechanical stress, nitric oxide (NO), HGF, and inflammation.\n",
            "- Activated satellite cells proliferate and fuse to muscle fibers, donating myonuclei so the fiber can support more transcription and protein synthesis (myonuclear domain theory).\n",
            "- Local IGF-1 splice variants (including “mechano growth factor”/MGF) help activate satellite cells and promote hypertrophy.\n",
            "\n",
            "5) Protein breakdown and remodeling pathways\n",
            "- Resistance exercise acutely increases both MPS and muscle protein breakdown (MPB). Net hypertrophy requires MPS > MPB over time.\n",
            "- Two main proteolytic systems: ubiquitin-proteasome (E3 ligases like MuRF1 and Atrogin-1/MAFbx) and autophagy-lysosome pathway (LC3, Beclin1, etc).\n",
            "- Chronic resistance training tends to downregulate catabolic E3 ligases and shift balance toward synthesis, while disuse upregulates them.\n",
            "\n",
            "6) Hormonal modulators\n",
            "- Testosterone: anabolic steroid hormone that increases protein synthesis, satellite cell activity, and neuromuscular function.\n",
            "- Growth hormone (GH) and IGF-1: GH stimulates IGF-1 release (systemic and local). IGF-1 is anabolic via PI3K/Akt/mTOR and satellite cell activation.\n",
            "- Insulin: anti-catabolic; potentiates amino acid uptake and mTOR signaling when combined with amino acids.\n",
            "- Cortisol: catabolic when chronically elevated — increases proteolysis and blunts hypertrophy.\n",
            "- Acute spikes in testosterone, GH, cortisol immediately after exercise are normal; long-term levels and nutrient context are more important than single spikes.\n",
            "\n",
            "7) Energy metabolism & local metabolites\n",
            "- ATP depletion and increased AMP activate AMPK. AMPK signals low energy and inhibits mTORC1 (via TSC2/raptor), opposing hypertrophy if energetic stress is chronic.\n",
            "- Immediate energy systems: phosphocreatine buffers ATP during short maximal efforts; glycolysis produces ATP + lactate for higher-rep sets — metabolic stress helps hypertrophy.\n",
            "- Reactive oxygen species (ROS) and nitric oxide (NO) produced during exercise act as signaling molecules: low/moderate ROS can stimulate adaptive signaling; excessive ROS can damage tissue.\n",
            "\n",
            "8) Negative regulators\n",
            "- Myostatin (GDF-8): a TGF-β family member that inhibits muscle growth via SMAD2/3 signaling, reducing Akt/mTOR activity and satellite cell proliferation.\n",
            "- SMAD signaling and other TGF-β pathways limit hypertrophy; genetic or pharmacologic inhibition of myostatin increases muscle mass.\n",
            "\n",
            "9) Neural adaptations (chemical/neurophysiological basis of strength gains)\n",
            "- Early strength gains (weeks) are mainly neural: improved motor unit recruitment, firing rate, synchronization, reduced inhibitory reflexes.\n",
            "- At the neuromuscular junction, increased acetylcholine release and receptor adaptations improve excitation-contraction coupling efficiency.\n",
            "- Changes in central drive involve neurotransmitter systems (glutamate, GABA, monoamines) and neurotrophic factors (BDNF, NT-3) that influence motor neuron plasticity.\n",
            "\n",
            "10) Inflammation and immune role in repair\n",
            "- Immune cells (neutrophils, macrophages) remove debris and secrete cytokines/growth factors (IL-6, IGF-1, TNF-α in early phases) that recruit satellite cells and stimulate repair.\n",
            "- M1 macrophages are pro-inflammatory (cleanup), M2 macrophages are pro-regenerative — the timing and balance matter for effective remodeling.\n",
            "\n",
            "11) Time course of molecular events\n",
            "- Seconds–minutes: Ca2+ release, ATP turnover, activation of mechanosensors and kinases (FAK, MAPK), early phosphorylation events.\n",
            "- Hours: Akt/mTORC1 activation, increased translation initiation, local IGF-1/MGF expression, satellite cell activation begins.\n",
            "- 24–48+ hours: Elevated MPS (peaks within ~24h and can remain elevated ~24–48h depending on training status and nutrition), inflammation resolution, satellite cell proliferation and beginning of fusion.\n",
            "- Weeks–months: cumulative net protein accretion, fiber cross-sectional area increase, increased myonuclei, tendon and extracellular matrix remodeling.\n",
            "\n",
            "12) Nutrient signaling — why protein and timing matter\n",
            "- Essential amino acids and especially leucine strongly stimulate mTORC1 and MPS. Leucine activates mTOR signaling and promotes translation initiation.\n",
            "- Insulin (from carbs/meal) enhances amino acid uptake and synergizes with amino acids to suppress MPB and support MPS.\n",
            "- Energy deficit or persistent AMPK activation (e.g., very low calories, chronically high endurance work) blunts anabolic signaling.\n",
            "\n",
            "13) Practical implications (how to apply the chemistry)\n",
            "- Train with progressive mechanical tension (heavy loads or harder bodyweight variants) to maximally engage mechanotransduction and mTOR pathways.\n",
            "- Include both low-rep heavy sets (strength) and moderate-rep sets (hypertrophy/volume) — both stimulate overlapping molecular pathways.\n",
            "- Get sufficient protein (1.6–2.2 g/kg/day) and include ~20–40 g high-quality protein with ~2–3 g leucine per meal to optimally stimulate MPS post-workout.\n",
            "- Avoid chronic extreme caloric deficit or excessive endurance volume if the priority is hypertrophy (activates AMPK and inhibits mTOR).\n",
            "- Allow recovery: MPS is elevated for ~24–48 hours so frequency can be 2–3x/week per muscle group for optimal cumulative stimulus.\n",
            "- Minimize chronic stress/cortisol and ensure sleep to support anabolic hormonal milieu and recovery.\n",
            "\n",
            "Summary (one-line)\n",
            "Resistance exercise triggers mechanosensors, metabolic and inflammatory signals that activate PI3K/Akt/mTOR and satellite-cell pathways to increase protein synthesis and myonuclear content; hormones, nutrients (especially leucine), energy status (AMPK), and inhibitory factors (myostatin) modulate whether net muscle accretion and strength gains occur.\n",
            "\n",
            "If you want, I can:\n",
            "- Draw this out as a simple flowchart of signals (mechanical → sensors → mTOR → MPS; satellite cell path; inhibitors), or\n",
            "- Make a short practical checklist for training and nutrition that targets these molecular mechanisms specifically for hypertrophy and strength. Which would help more, Sagun?\n",
            "Human: This is great. I've recently been struggling with 'love handles' or excess fat around the sides and front of my abdomen and also relatively skinny arms. Can you tailor a stength + hypertrophy based workout focused this area?\n",
            "AI: Nice — good goals, Sagun. I’ll give you a practical, science-backed plan: a training template you can use to build arm muscle (biceps/triceps/forearms) while attacking overall fat (including love handles) through full‑body resistance work + conditioning and nutrition. Quick reality checks first, then a detailed 8‑week-style program you can start right away.\n",
            "\n",
            "Short realities\n",
            "- You can’t “spot reduce” fat from love handles. Fat loss requires an overall calorie deficit and full‑body work. Local core/oblique training will improve strength and shape the area as fat comes off, but won’t directly burn only that fat.\n",
            "- You can build arm size while losing fat, but progress is slower than doing a dedicated bulk. Best approach for both: high protein, progressive resistance training, and a modest calorie deficit (or body recomposition if you’re relatively new to progressive overload).\n",
            "\n",
            "Big-picture strategy\n",
            "- Train full body with emphasis on progressive overload and increased weekly volume for arms.\n",
            "- Add structured core + oblique work 3x/week to strengthen and define the waistline as fat decreases.\n",
            "- Add 2 conditioning sessions/week (HIIT or mixed steady-state) to create extra calorie burn and improve metabolic rate.\n",
            "- Nutrition: modest calorie deficit (~-200 to -400 kcal/day) or maintenance for recomposition; protein 1.6–2.2 g/kg/day; prioritize nutrient timing around workouts.\n",
            "\n",
            "Program overview (4 days/week — good balance for strength, hypertrophy and recovery)\n",
            "- Day 1 — Upper Strength (heavy; compound push/pull)\n",
            "- Day 2 — Lower + Conditioning\n",
            "- Day 3 — Upper Hypertrophy (Arm specialization + core/oblique)\n",
            "- Day 4 — Full-body Hypertrophy + Core/Conditioning\n",
            "- Optional: 1 active recovery or mobility day and/or light LISS if energy permits\n",
            "\n",
            "Weekly arm volume target\n",
            "- Biceps: 10–15 sets/week\n",
            "- Triceps: 10–18 sets/week\n",
            "(Adjust depending on recovery; beginners need less to start.)\n",
            "\n",
            "Progression tools\n",
            "- Add load (weighted vest / dip belt) for pull-ups/chin-ups/dips.\n",
            "- Move to harder calisthenic variations (archer, typewriter, one-arm progressions).\n",
            "- Slow eccentrics (3–6s), paused reps, and extra sets.\n",
            "- Increase weekly sets/reps gradually (5–10% weekly volume increase until you hit recovery limits).\n",
            "\n",
            "Equipment suggestions (optimal)\n",
            "- Pull-up bar or rings\n",
            "- Parallel bars / dip bars or rings\n",
            "- Weighted vest or dipping belt (or dumbbells)\n",
            "- Resistance bands\n",
            "- Parallettes or low box for incline variations\n",
            "\n",
            "Sample sessions (concrete)\n",
            "\n",
            "Day 1 — Upper Strength (focus heavy pulling & pushing)\n",
            "- Warm-up: 5–8 min cardio + band pull‑aparts + scapular pull-ups\n",
            "- Weighted chin-ups (supinated for biceps bias): 5 sets × 3–6 reps (2–3 min rest)\n",
            "- Weighted dips: 5 × 3–6 (2–3 min rest)\n",
            "- Ring rows (lean angle moderate): 3 × 6–10 (90–120 s)\n",
            "- Pike push-ups / HSPU progressions: 3 × 6–8\n",
            "- Farmer carry or heavy static hold: 2 × 30–60 s\n",
            "- Core: Pallof press or side plank: 3 × 10–15 reps/side\n",
            "\n",
            "Day 2 — Lower + Conditioning\n",
            "- Warm-up: dynamic hips, ankles\n",
            "- Bulgarian split squats (weighted if possible): 4 × 6–10 per leg\n",
            "- Nordic hamstring curls or single‑leg RDL: 3 × 6–10\n",
            "- Jump squats or explosive step-ups: 3 × 6–8\n",
            "- Calf raises: 3 × 10–20\n",
            "- Conditioning: 10–15 min HIIT (20s all‑out / 40s rest) OR 25–35 min moderate steady-state\n",
            "\n",
            "Day 3 — Upper Hypertrophy (Arm specialization & obliques)\n",
            "- Warm-up: band face pulls, wrist mobility\n",
            "- Close‑grip (diamond) push-ups or ring dips: 4 × 8–12 (triceps emphasis)\n",
            "- Weighted chin-ups OR supinated ring rows: 4 × 6–10 (biceps)\n",
            "- Archer push-ups / decline push-ups: 3 × 8–12\n",
            "- Australian rows underhand or ring curls (bodyweight curl variation): 3 × 8–12\n",
            "- Triceps extensions on rings / banded triceps pushdown: 3 × 10–15\n",
            "- Hammer-grip pull or towel holds for forearms: 3 × 8–12 (or 30–45 s holds)\n",
            "- Core/Obliques circuit (2–3 rounds, little rest):\n",
            "  - Side plank dips x 10–15/side\n",
            "  - Hanging oblique knee raises (or hanging windshield wipers progression) x 8–12/side\n",
            "  - Russian twists (weighted) x 15–20\n",
            "\n",
            "Day 4 — Full-body Hypertrophy + Core\n",
            "- Warm-up\n",
            "- Pull-ups (moderate): 4 × 6–12 (use bands to hit rep ranges)\n",
            "- Dips or push-up variation: 4 × 8–12\n",
            "- Pistol progressions or split squats: 4 × 6–10 per leg\n",
            "- Ring face pulls / YTWL (shoulder health): 3 × 12–15\n",
            "- Core finisher: L-sit holds 3 × 10–30 s + plank 3 × 45–60 s\n",
            "- Optional short conditioning: 8–10 min EMOM (burpees, kettlebell swings, or jump rope)\n",
            "\n",
            "8‑week progression plan\n",
            "Weeks 1–2: Build base — focus on technique, choose variations that let you hit prescribed rep ranges. Start on lower end of volume.\n",
            "Weeks 3–5: Increase volume for arms (add 1 set per arm exercise weekly or add 2–3 reps/set). Add 1–2 s eccentric slowing for hypertrophy sets.\n",
            "Weeks 6–8: Add intensity — move to harder variations or add weight. Keep 1 deload week if you feel run down (drop volume 30–40%).\n",
            "\n",
            "Sample arm-specific microprogression\n",
            "- Week 1: Weighted chin-ups 4×4 (if too hard, use band assistance)\n",
            "- Week 2: 5×4\n",
            "- Week 3: 4×5\n",
            "- Week 4: 5×5 or add +2.5–5 kg\n",
            "Aim to progressively increase reps or weight across weeks.\n",
            "\n",
            "Nutrition & cardio specifics for love handles reduction + arm gain\n",
            "- Goal: modest deficit (~-200 to -400 kcal/day) for fat loss with minimal muscle loss. If your primary goal becomes arm size, consider a small surplus after you reach acceptable body-fat.\n",
            "- Protein: 1.6–2.2 g/kg/day (e.g., if you weigh 75 kg, aim 120–165 g/day).\n",
            "- Carbs: around workouts (pre and post) to support performance and recovery.\n",
            "- Fats: ~20–30% of total calories.\n",
            "- Cardio: 2x/week HIIT (10–15 min) or 2–3x/week LISS (30–45 min) depending on preference. HIIT helps preserve muscle and is time-efficient.\n",
            "- Sleep 7–9 hours and manage stress — chronic cortisol makes love handle loss harder.\n",
            "\n",
            "Core & oblique exercise notes (form tips)\n",
            "- Side planks: keep hips stacked, dip slowly for tension. Add weight if too easy.\n",
            "- Hanging oblique raises: twist the ribcage and drive one knee/leg towards the opposite elbow; control the descent.\n",
            "- Windshield wipers: start with tucked knees, progress to straight legs.\n",
            "- Pallof presses: anti-rotation strength is excellent for core stability and waist aesthetics.\n",
            "\n",
            "Tracking & metrics\n",
            "- Track body weight, waist and hip circumference every 7–14 days.\n",
            "- Log workouts (exercise, sets, reps, weight/progression).\n",
            "- Photos every 2–4 weeks to judge changes in love handles — circumference and fat loss can lag strength gains.\n",
            "\n",
            "Example 6-month priority paths (choose one)\n",
            "- Priority A: Lose fat first — deficit + strength training for 8–16 weeks, then small surplus to build arm mass.\n",
            "- Priority B: Recomposition — slight deficit/maintenance + high protein + heavy progressive training to gain some arm muscle while slowly losing fat (works well if you’re new to structured overload).\n",
            "- Priority C: Bulk arms now — small surplus to maximize hypertrophy, then cut later to remove love-handle fat. Faster arm growth but you’ll temporarily gain fat.\n",
            "\n",
            "Quick no-equipment alternatives for arm hypertrophy\n",
            "- Biceps: towel/chin holds on doorframe, supinated close-grip inverted rows under a sturdy table, slow negatives on pull-up bar (eccentric).\n",
            "- Triceps: elevated push-ups, close-grip push-ups, slow decline push-ups, triceps extensions with bands or bodyweight using a chair.\n",
            "\n",
            "Want this tailored exactly to you?\n",
            "Tell me:\n",
            "- How many days/week you can commit\n",
            "- Your current best numbers (max strict pull-ups/chin-ups, push-ups in a set, pistol ability)\n",
            "- Equipment you have (weighted vest, pull-up bar, rings, bands, dumbbells)\n",
            "- Your priority (fat loss first, arm growth first, or recomposition)\n",
            "\n",
            "I’ll turn this into a detailed 8‑week plan with exact progressions and weekly set/reps for your level. Ready to lock it in, Sagun?\n",
            "Human: Earlier when you were talking about calisthenics you mentioned muscle groups. Can you go into a little bit of detail about that.\n",
            "AI: Nice — good follow-up, Sagun. Below is a practical breakdown of the main muscle groups you’ll be training with calisthenics, what each group does, key calisthenics exercises that hit them, useful progression options, coaching cues, and programming notes (rep ranges, volume, common errors, and mobility needs). Use this as a reference when you pick exercises and build sessions.\n",
            "\n",
            "Overview — movement patterns (quick)\n",
            "- Push (horizontal & vertical) — chest, triceps, anterior deltoids\n",
            "- Pull (horizontal & vertical) — lats, rhomboids, biceps, posterior deltoids\n",
            "- Hinge/single-leg — hamstrings, glutes\n",
            "- Squat — quads, glutes, adductors\n",
            "- Core/anti-rotation & anti-extension — rectus abdominis, obliques, transverse abdominis, erector spinae\n",
            "- Shoulder stabilizers/scapular control — serratus anterior, traps, rotator cuff\n",
            "\n",
            "1) Chest (pectoralis major & minor)\n",
            "- Primary function: horizontal adduction of the arm (bringing arm across body), contributes to shoulder flexion and internal rotation.\n",
            "- Calisthenics exercises: incline push-ups → standard push-ups → decline/feet-elevated push-ups → archer push-ups → ring/parallel bar dips → weighted dips.\n",
            "- Progressions: increase decline angle, add tempo (slow eccentric), move to unilateral (archer → one-arm push-up progressions), add weight.\n",
            "- Cues: full shoulder protraction on concentric, keep body rigid (plank line), elbows at ~45° for shoulder-friendly mechanics (unless targeting triceps).\n",
            "- Programming: for hypertrophy 3–5 sets of 6–12; for strength 4–6 sets of 3–6 with added load.\n",
            "- Common errors: flaring elbows excessively, sagging hips, poor scapular control → shoulder pain.\n",
            "\n",
            "2) Shoulders (deltoids + rotator cuff)\n",
            "- Primary function: arm abduction, flexion, extension and stabilization of the glenohumeral joint.\n",
            "- Calisthenics exercises: pike push-ups → elevated pike → partial handstand → strict handstand push-up (HSPU) → weighted HSPU. Lateral/ rear delt work: band face pulls, Y/T/W on rings/parallettes.\n",
            "- Progressions: increase elevation to get vertical load, move toward freestanding handstand work, add weight or deficit.\n",
            "- Cues: keep spine neutral, tuck chin slightly, drive through the shoulders and mid-foot when inverted.\n",
            "- Programming: shoulder-heavy days 3–5 sets of 4–10 depending on goal; accessory high-rep posterior work 2–3×12–20 for posture.\n",
            "- Mobility: thoracic extension, scapular upward rotation, lat flexibility for full ROM.\n",
            "- Pitfall: neglecting rotator cuff and posterior chain → imbalance and injury.\n",
            "\n",
            "3) Back — vertical pull (lats) & horizontal pull (rhomboids/traps)\n",
            "- Primary function: scapular retraction/depression, shoulder extension, posture and pulling strength.\n",
            "- Calisthenics exercises:\n",
            "  - Vertical pull: band-assisted pull-ups → strict pull-ups/chin-ups → weighted pull-ups → muscle-up progressions.\n",
            "  - Horizontal pull: inverted/Australian rows → ring rows → archer rows → single-arm rows on rings.\n",
            "- Progressions: decrease assistance, change grip (supinated for biceps bias), increase difficulty/angle, add weight.\n",
            "- Cues: initiate with scapular pull, lead with chest to the bar/rings, retract shoulders at the top, full controlled eccentric.\n",
            "- Programming: aim for 10–20 weekly sets for the back (split across vertical & horizontal); strength work 3–6 reps with weight; hypertrophy 6–12 reps/bodyweight variations.\n",
            "- Common errors: using too much momentum/kipping (unless specifically training muscle-ups), poor scapular control.\n",
            "\n",
            "4) Arms — biceps, triceps, forearms\n",
            "- Biceps function: elbow flexion and supination.\n",
            "  - Exercises: chin-ups (supinated), ring curls, slow eccentric pull-ups, towel/chin holds, bodyweight curls on rings.\n",
            "- Triceps function: elbow extension, shoulder stabilization.\n",
            "  - Exercises: dips (parallel bars/rings), close-grip push-ups, triceps extensions on rings, band pushdowns.\n",
            "- Forearms: gripping strength, wrist stability.\n",
            "  - Exercises: towel hangs, dead hangs, farmer carries, finger/forearm holds, slow eccentrics.\n",
            "- Progressions: weighted chin-ups/dips, one-arm assisted variations, increased time under tension.\n",
            "- Programming: for hypertrophy target 10–18 sets/week for triceps, 8–15 for biceps; use 6–12 rep ranges for growth, include some heavy low-rep sets.\n",
            "- Cues: full ROM, control the eccentric, don’t rely on shoulder/back to cheat curls.\n",
            "\n",
            "5) Legs — quads, hamstrings, glutes, calves\n",
            "- Quads (knee extension): bodyweight squat → goblet squat (if available) → Bulgarian split squat → pistol squat.\n",
            "- Hamstrings/glutes (hip extension & knee flexion): hip thrust variants (glute bridges), single-leg RDL, Nordic curls, glute-ham raises (if equipment).\n",
            "- Calves: standing calf raises, single-leg calf raises, slow eccentrics.\n",
            "- Progressions: add unilateral focus, increase ROM and time under tension, add weight (vest/dumbbell).\n",
            "- Cues: knees track toes, sit back for posterior chain emphasis, keep balance in single-leg work.\n",
            "- Programming: legs tolerate higher volume — 10–20+ sets/week depending on goals; strength sets 3–6 reps with added load (single-leg strength), hypertrophy 8–15 reps.\n",
            "- Common mistakes: neglecting hamstrings (creates imbalance), limited ankle mobility harming squat depth.\n",
            "\n",
            "6) Core & obliques — rectus abdominis, transverse abdominis, internal/external obliques, erector spinae\n",
            "- Function: anti-extension, anti-rotation, trunk flexion, and stabilizing pelvis/torso during movement.\n",
            "- Calisthenics exercises:\n",
            "  - Anti-extension: planks, ab wheel progressions, hollow body hold.\n",
            "  - Anti-rotation/oblique: Pallof press, side plank, hanging oblique raises, Russian twists.\n",
            "  - Dynamic hanging: hanging knee raises → hanging leg raises → toes-to-bar → windshield wipers.\n",
            "- Progressions: increase hold time/weight, move from tucked to straight-leg positions, add rotation and slow eccentrics.\n",
            "- Cues: brace as if preparing for a punch, avoid lumbar hyperextension, control breathing.\n",
            "- Programming: core work 2–4x/week; stronger core supports heavier calisthenics skill work (handstands, levers). 3–5 sets of 8–20 or 3×30–90s holds depending on exercise.\n",
            "- Note: core exercises shape the muscle but won’t remove fat locally — pair with overall fat loss plan.\n",
            "\n",
            "7) Scapular stabilizers & serratus anterior\n",
            "- Function: stabilize scapula for efficient pushing/pulling and healthy shoulders.\n",
            "- Exercises: scapular pull-ups/push-ups, banded protraction work, wall slides, ring scapular holds, push-up plus.\n",
            "- Programming: include low-volume activation every session (2–4 sets of 8–15) to reduce injury risk and improve pressing/pulling.\n",
            "\n",
            "Integration & programming tips\n",
            "- Antagonist balance: match push and pull volume to avoid shoulder imbalances (e.g., for every pushing set aim for ~equal pulling volume).\n",
            "- Frequency: hit major muscle groups 2–3x/week for hypertrophy and strength using full-body or upper/lower split.\n",
            "- Volume guidance: beginners 8–12 weekly sets/muscle; intermediates 12–20; advanced may need 20+ depending on recovery.\n",
            "- Strength vs hypertrophy: mix low-rep heavy sets (3–6) and moderate-rep volume sets (6–12). For calisthenics add difficulty by harder progressions, slow eccentrics, and weighted calisthenics.\n",
            "- Skill integration: allocate specific short blocks (5–15 minutes) for skill work like handstands, muscle-up transitions, front lever progressions — these improve neural control and transfer to strength.\n",
            "\n",
            "Mobility & injury prevention\n",
            "- Key mobility areas: thoracic extension (for overhead work), scapula and shoulder mobility, hip flexor and ankle mobility (for squats, pistols).\n",
            "- Prehab: activate rotator cuff and scapular stabilizers; do band face pulls, external rotation, and shoulder dislocates.\n",
            "- Warm-ups: dynamic movement patterns, progressive loading, and specific muscle activation before heavy sets.\n",
            "\n",
            "Coaching cues & how to pick the right progression\n",
            "- If you can do 3–6 reps of an exercise with good form, it’s a strength-level variation — add difficulty.\n",
            "- If you can do 8–15 reps with good control, use it for hypertrophy until you reach top of range, then progress.\n",
            "- If form breaks down, regress (easier angle, assistance, or fewer reps).\n",
            "- Use slow eccentrics (3–5s) to increase difficulty without adding weight.\n",
            "\n",
            "Quick example split mapping exercises to muscle groups (for reference)\n",
            "- Push day: push-ups/dips (chest, triceps, shoulders) + pike/HSPU progressions (shoulders) + triceps accessory.\n",
            "- Pull day: pull-ups/rows (lats, rhomboids, biceps) + curls/hold work for arms + scapular work.\n",
            "- Leg day: pistols/Bulgarian split (quads/glutes) + Nordic or single-leg RDL (hamstrings) + calves.\n",
            "- Core daily or 3x/week: planks, L-sits, hanging leg raises, Pallof presses.\n",
            "\n",
            "If you want, I can:\n",
            "- Create a 4‑week exercise selection sheet from beginner → advanced with one progression ladder per muscle group, or\n",
            "- Build a sample 4-day split tailored to your current numbers and equipment (tell me your max pull-ups, push-ups, and what equipment you have).\n",
            "\n",
            "Which would you prefer, Sagun?\n",
            "Human: It's still a bit hard to visualize. Can you make a diagram that shows the different muscle groups?\n",
            "AI: Good—I’ll make this visual and practical. Below are two simple diagrams (front and back) you can scan quickly, followed by a legend that names each numbered muscle group and lists the best calisthenics exercises that target it. If you want a cleaner graphic file (SVG/PNG) I can generate an SVG you can download and open in a browser — tell me and I’ll create it.\n",
            "\n",
            "Front view (numbered)\n",
            "      [H]\n",
            "       O\n",
            "      /|\\\n",
            "  [2]-/ | \\-[3]\n",
            "     /  |  \\\n",
            "    [1] | [4]\n",
            "     |  |  |\n",
            "    [10]| [11]\n",
            "     |  |  |\n",
            "    [12] [13]\n",
            "     |      |\n",
            "    / \\    / \\\n",
            "   /   \\  /   \\\n",
            "  [14] [15] [16]\n",
            "\n",
            "Back view (numbered)\n",
            "       O\n",
            "      /|\\\n",
            "  [2]-/ | \\-[3]\n",
            "     /  |  \\\n",
            "    [5] | [6]\n",
            "     |  |  |\n",
            "   [9]  |  [8]\n",
            "     |  |  |\n",
            "    [12] [13]\n",
            "     |      |\n",
            "    / \\    / \\\n",
            "   /   \\  /   \\\n",
            "  [14] [15] [16]\n",
            "\n",
            "Legend (numbers → muscle group + top calisthenics exercises)\n",
            "- [1] Pectoralis major (chest)\n",
            "  - Push-ups (incline → standard → decline), ring/parallel dips, archer push-up, weighted push-ups\n",
            "- [2] Anterior / medial deltoids (front & middle shoulders)\n",
            "  - Pike push-ups, elevated pike, partial/full handstand push-ups, decline/incline push-ups\n",
            "- [3] Lateral / posterior deltoids / upper arm (rear shoulder area)\n",
            "  - Band face pulls, ring rear-delt rows, Y/T/W movements, horizontal rows with high elbow\n",
            "- [4] Biceps (front of upper arm)\n",
            "  - Chin-ups (supinated), ring curls, slow negative pull-ups, underhand inverted rows, towel/chin holds\n",
            "- [5] Trapezius & upper back (upper traps, neck-to-shoulder)\n",
            "  - Shrugs with loaded carries, scapular pull-ups, face pulls, ring rows\n",
            "- [6] Latissimus dorsi (lats / sides of back)\n",
            "  - Pull-ups, wide grip pull-ups, ring rows, weighted pull-ups, muscle-up progressions\n",
            "- [8] Rhomboids / mid-traps (middle back)\n",
            "  - Inverted rows, ring rows, horizontal pulling movements emphasizing scapular retraction\n",
            "- [9] Erector spinae (lower back)\n",
            "  - Back extensions (if available), supermans, long-hold planks, slow Romanian deadlift variants (single-leg RDL)\n",
            "- [10] Serratus anterior (side of ribcage under armpit)\n",
            "  - Push-up plus, scapular push-ups, ring support protraction, overhead protraction work\n",
            "- [11] Obliques (side abs)\n",
            "  - Side planks, hanging oblique raises, windshield wipers, Russian twists\n",
            "- [12] Rectus abdominis (front abs)\n",
            "  - Hollow body holds, planks, hanging leg raises, ab wheel or rollout progressions\n",
            "- [13] Transverse abdominis / deep core\n",
            "  - Pallof press, bracing drills, vacuum-style bracing, dead bug progressions\n",
            "- [14] Gluteus maximus (butt)\n",
            "  - Hip thrust/glute bridge progressions, Nordic hamstrings (assist), single-leg hip drives, deep squats\n",
            "- [15] Quadriceps (front of thigh)\n",
            "  - Squats, pistol/single-leg squats, Bulgarian split squats, jump squats\n",
            "- [16] Hamstrings & calves (back of thigh & lower leg)\n",
            "  - Nordic hamstring curls, single-leg RDL, glute-ham raises, calf raises and eccentric calf work\n",
            "\n",
            "Quick programming notes tied to the diagram\n",
            "- Push muscles (chest, triceps, anterior delts): pair push-ups/dips with shoulder work and triceps accessories.\n",
            "- Pull muscles (lats, rhomboids, biceps): balance vertical pulls (pull-ups) with horizontal pulls (rows).\n",
            "- Legs: emphasize unilateral work (pistols/Bulgarian split) for balanced hypertrophy using calisthenics.\n",
            "- Core: mix anti-extension (planks/hollow), anti-rotation (Pallof/side-planks) and dynamic hanging work (leg raises/wipers).\n",
            "- Shoulder health: include serratus + scapular work (push-up plus, scapular pull-ups) every session.\n",
            "\n",
            "Want a downloadable image?\n",
            "- I can produce a neat SVG diagram (labelled front & back) you can open in any browser or save as PNG. Say “Yes — make an SVG” and I’ll generate it with these labels and exercises.\n",
            "Human: Yes make an SVG.\n",
            "AI: Here’s an SVG you can open in a browser or save as a .svg file. It shows a simplified front (left) and back (right) figure with numbered markers and a legend mapping each number to the muscle group plus 1–2 example calisthenics exercises.\n",
            "\n",
            "To use:\n",
            "- Copy everything below into a text file and save it as muscles.svg\n",
            "- Open with a browser (Chrome/Firefox) or an image viewer that supports SVG\n",
            "\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1100\" height=\"1400\" viewBox=\"0 0 1100 1400\">\n",
            "  <style>\n",
            "    .title { font: bold 28px sans-serif; fill: #111; }\n",
            "    .panel { font: 18px sans-serif; fill: #222; }\n",
            "    .legend-title { font: bold 16px sans-serif; fill: #111; }\n",
            "    .legend { font: 14px sans-serif; fill: #111; }\n",
            "    .num { font: bold 12px sans-serif; fill: white; text-anchor: middle; dominant-baseline: central; }\n",
            "    .marker { fill: #d9534f; stroke: #b73736; stroke-width: 2; }\n",
            "    .figure-stroke { stroke: #333; stroke-width: 4; fill: none; stroke-linecap: round; stroke-linejoin: round; }\n",
            "    .figure-fill { fill: #f7f7f7; stroke: #555; stroke-width: 2; }\n",
            "    .box { fill: #fff; stroke: #ccc; stroke-width: 1; }\n",
            "  </style>\n",
            "\n",
            "  <rect x=\"10\" y=\"10\" width=\"1080\" height=\"1380\" class=\"box\" rx=\"10\" ry=\"10\"/>\n",
            "\n",
            "  <text x=\"50\" y=\"50\" class=\"title\">Calisthenics Muscle Map — Front & Back (numbered)</text>\n",
            "  <text x=\"50\" y=\"80\" class=\"panel\">Front view (left) and back view (right). Legend below maps numbers to muscle groups and example calisthenics exercises.</text>\n",
            "\n",
            "  <!-- LEFT: Front figure -->\n",
            "  <g transform=\"translate(50,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Front</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers front -->\n",
            "    <!-- 1 Pectoralis -->\n",
            "    <circle cx=\"140\" cy=\"140\" r=\"18\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"140\" class=\"num\">1</text>\n",
            "\n",
            "    <!-- 2 Anterior/medial deltoids (shoulder) -->\n",
            "    <circle cx=\"92\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"92\" y=\"110\" class=\"num\">2</text>\n",
            "\n",
            "    <!-- 3 Lateral/posterior deltoid (front-right shoulder area for labeling consistency) -->\n",
            "    <circle cx=\"188\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"110\" class=\"num\">3</text>\n",
            "\n",
            "    <!-- 4 Biceps (front upper arm left) -->\n",
            "    <circle cx=\"72\" cy=\"160\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"72\" y=\"160\" class=\"num\">4</text>\n",
            "\n",
            "    <!-- 10 Serratus (side of ribcage) -->\n",
            "    <circle cx=\"112\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"112\" y=\"170\" class=\"num\">10</text>\n",
            "\n",
            "    <!-- 11 Obliques (side abs) -->\n",
            "    <circle cx=\"170\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"170\" y=\"170\" class=\"num\">11</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (front abs) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (lower front) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 14 Glutes (left leg top) -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (right leg) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves (right lower leg) -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- RIGHT: Back figure -->\n",
            "  <g transform=\"translate(350,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Back</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers back -->\n",
            "    <!-- 5 Trapezius & upper back -->\n",
            "    <circle cx=\"140\" cy=\"100\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"100\" class=\"num\">5</text>\n",
            "\n",
            "    <!-- 6 Latissimus dorsi -->\n",
            "    <circle cx=\"188\" cy=\"150\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"150\" class=\"num\">6</text>\n",
            "\n",
            "    <!-- 8 Rhomboids / mid-traps -->\n",
            "    <circle cx=\"105\" cy=\"150\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"105\" y=\"150\" class=\"num\">8</text>\n",
            "\n",
            "    <!-- 9 Erector spinae (lower back) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">9</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (back side reference) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (lower front/back reference near pelvis) -->\n",
            "    <circle cx=\"140\" cy=\"260\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"260\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 14 Glutes -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (front leg reference on back view) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- Legend box -->\n",
            "  <g transform=\"translate(50,520)\">\n",
            "    <rect x=\"0\" y=\"0\" width=\"1000\" height=\"820\" rx=\"8\" ry=\"8\" fill=\"#ffffff\" stroke=\"#ddd\" />\n",
            "    <text x=\"18\" y=\"30\" class=\"legend-title\">Legend — number : muscle group — example calisthenics exercises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"60\" class=\"legend\">1 — Pectoralis major (chest): push-ups (incline → standard → decline), ring/parallel dips</text>\n",
            "    <text x=\"18\" y=\"90\" class=\"legend\">2 — Anterior / medial deltoids (front/mid shoulder): pike push-ups, handstand push-up progressions</text>\n",
            "    <text x=\"18\" y=\"120\" class=\"legend\">3 — Lateral / posterior deltoids (side/rear shoulder): band face pulls, rear-delt rows, Y/T/W</text>\n",
            "    <text x=\"18\" y=\"150\" class=\"legend\">4 — Biceps: chin-ups (supinated), ring curls, slow negatives</text>\n",
            "    <text x=\"18\" y=\"180\" class=\"legend\">5 — Trapezius & upper back: scapular pull-ups, shrugs (loaded carries), face pulls</text>\n",
            "    <text x=\"18\" y=\"210\" class=\"legend\">6 — Latissimus dorsi (lats): pull-ups, wide grip pull-ups, weighted pull-ups, muscle-up work</text>\n",
            "    <text x=\"18\" y=\"240\" class=\"legend\">8 — Rhomboids / mid-traps: inverted rows, ring rows, horizontal pulling emphasizing scapular retraction</text>\n",
            "    <text x=\"18\" y=\"270\" class=\"legend\">9 — Erector spinae (lower back): single-leg RDL, back extensions, supermans</text>\n",
            "\n",
            "    <text x=\"18\" y=\"310\" class=\"legend\">10 — Serratus anterior (side ribcage): push-up plus, scapular protraction on rings</text>\n",
            "    <text x=\"18\" y=\"340\" class=\"legend\">11 — Obliques (side abs): side plank dips, hanging oblique raises, windshield wipers</text>\n",
            "    <text x=\"18\" y=\"370\" class=\"legend\">12 — Rectus abdominis (front abs): hollow body holds, planks, hanging leg raises</text>\n",
            "    <text x=\"18\" y=\"400\" class=\"legend\">13 — Transverse abdominis / deep core: Pallof press, bracing drills, dead-bug</text>\n",
            "\n",
            "    <text x=\"18\" y=\"440\" class=\"legend\">14 — Gluteus maximus (glutes): single-leg hip thrusts, glute bridges, deep squats</text>\n",
            "    <text x=\"18\" y=\"470\" class=\"legend\">15 — Quadriceps (quads): squats, Bulgarian split squats, pistol progressions</text>\n",
            "    <text x=\"18\" y=\"500\" class=\"legend\">16 — Hamstrings & calves: Nordic hamstring curls, single-leg RDL, calf raises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"540\" class=\"legend-title\">Programming notes</text>\n",
            "    <text x=\"18\" y=\"570\" class=\"legend\">- Pair push and pull volume to avoid imbalance.</text>\n",
            "    <text x=\"18\" y=\"595\" class=\"legend\">- Train major groups 2–3×/week for hypertrophy. Mix heavy (3–6 reps) and moderate (6–12) sets.</text>\n",
            "    <text x=\"18\" y=\"620\" class=\"legend\">- Core work: mix anti-extension (plank/hollow), anti-rotation (Pallof/side-plank) and dynamic hanging leg raises.</text>\n",
            "    <text x=\"18\" y=\"645\" class=\"legend\">- Include scapular/shoulder prehab (band face pulls, scapular pull-ups) for healthy pressing and pulling.</text>\n",
            "\n",
            "    <text x=\"18\" y=\"700\" class=\"legend\">Tip: Save or print this SVG and use it while choosing exercises—tap the numbered targets to find suitable progressions.</text>\n",
            "  </g>\n",
            "\n",
            "  <text x=\"50\" y=\"1360\" class=\"panel\">If you want a different style (more anatomical accuracy, color-coded muscles, or a printable PDF), tell me which format and I’ll generate it.</text>\n",
            "</svg>\n",
            "Human: I copied the svg html. Make a script in Python that can open an svg with the earlier given html. I should be able to run the python script and a window should pop up where I can view the svg html.\n",
            "AI: Below is a ready-to-run Python script that opens the SVG you pasted earlier. It tries to use pywebview (a lightweight native web-view wrapper) so a simple window pops up with the SVG rendered. If pywebview is not available it falls back to opening the SVG in your default web browser.\n",
            "\n",
            "1) Install pywebview (optional, recommended for an in-app window):\n",
            "   pip install pywebview\n",
            "\n",
            "2) Save the script below as view_svg.py and run:\n",
            "   python view_svg.py\n",
            "\n",
            "Script (save exactly, run with Python 3.7+):\n",
            "\n",
            "\n",
            "```python\n",
            "#!/usr/bin/env python3\n",
            "\"\"\"\n",
            "view_svg.py\n",
            "\n",
            "Open an embedded SVG in a window. Tries to use pywebview for an in-app window.\n",
            "Falls back to writing a temporary .svg file and opening it in the default browser.\n",
            "\n",
            "Usage:\n",
            "    python view_svg.py\n",
            "\"\"\"\n",
            "\n",
            "import os\n",
            "import sys\n",
            "import tempfile\n",
            "import webbrowser\n",
            "\n",
            "# Paste your SVG content here as a single triple-quoted string.\n",
            "# I used the SVG content you copied earlier.\n",
            "SVG_CONTENT = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1100\" height=\"1400\" viewBox=\"0 0 1100 1400\">\n",
            "  <style>\n",
            "    .title { font: bold 28px sans-serif; fill: #111; }\n",
            "    .panel { font: 18px sans-serif; fill: #222; }\n",
            "    .legend-title { font: bold 16px sans-serif; fill: #111; }\n",
            "    .legend { font: 14px sans-serif; fill: #111; }\n",
            "    .num { font: bold 12px sans-serif; fill: white; text-anchor: middle; dominant-baseline: central; }\n",
            "    .marker { fill: #d9534f; stroke: #b73736; stroke-width: 2; }\n",
            "    .figure-stroke { stroke: #333; stroke-width: 4; fill: none; stroke-linecap: round; stroke-linejoin: round; }\n",
            "    .figure-fill { fill: #f7f7f7; stroke: #555; stroke-width: 2; }\n",
            "    .box { fill: #fff; stroke: #ccc; stroke-width: 1; }\n",
            "  </style>\n",
            "\n",
            "  <rect x=\"10\" y=\"10\" width=\"1080\" height=\"1380\" class=\"box\" rx=\"10\" ry=\"10\"/>\n",
            "\n",
            "  <text x=\"50\" y=\"50\" class=\"title\">Calisthenics Muscle Map — Front & Back (numbered)</text>\n",
            "  <text x=\"50\" y=\"80\" class=\"panel\">Front view (left) and back view (right). Legend below maps numbers to muscle groups and example calisthenics exercises.</text>\n",
            "\n",
            "  <!-- LEFT: Front figure -->\n",
            "  <g transform=\"translate(50,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Front</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers front -->\n",
            "    <!-- 1 Pectoralis -->\n",
            "    <circle cx=\"140\" cy=\"140\" r=\"18\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"140\" class=\"num\">1</text>\n",
            "\n",
            "    <!-- 2 Anterior/medial deltoids (shoulder) -->\n",
            "    <circle cx=\"92\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"92\" y=\"110\" class=\"num\">2</text>\n",
            "\n",
            "    <!-- 3 Lateral/posterior deltoid (front-right shoulder area for labeling consistency) -->\n",
            "    <circle cx=\"188\" cy=\"110\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"110\" class=\"num\">3</text>\n",
            "\n",
            "    <!-- 4 Biceps (front upper arm left) -->\n",
            "    <circle cx=\"72\" cy=\"160\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"72\" y=\"160\" class=\"num\">4</text>\n",
            "\n",
            "    <!-- 10 Serratus (side of ribcage) -->\n",
            "    <circle cx=\"112\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"112\" y=\"170\" class=\"num\">10</text>\n",
            "\n",
            "    <!-- 11 Obliques (side abs) -->\n",
            "    <circle cx=\"170\" cy=\"170\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"170\" y=\"170\" class=\"num\">11</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (front abs) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (lower front) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 14 Glutes (left leg top) -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (right leg) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves (right lower leg) -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- RIGHT: Back figure -->\n",
            "  <g transform=\"translate(350,120)\">\n",
            "    <text x=\"140\" y=\"-10\" class=\"panel\">Back</text>\n",
            "    <!-- head -->\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-fill\"/>\n",
            "    <circle cx=\"140\" cy=\"60\" r=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- torso -->\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-fill\"/>\n",
            "    <rect x=\"100\" y=\"90\" width=\"80\" height=\"160\" rx=\"30\" ry=\"30\" class=\"figure-stroke\"/>\n",
            "    <!-- arms -->\n",
            "    <path d=\"M100 110 L40 180\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M180 110 L240 180\" class=\"figure-stroke\"/>\n",
            "    <!-- legs -->\n",
            "    <path d=\"M120 250 L100 360\" class=\"figure-stroke\"/>\n",
            "    <path d=\"M160 250 L180 360\" class=\"figure-stroke\"/>\n",
            "\n",
            "    <!-- Markers back -->\n",
            "    <!-- 5 Trapezius & upper back -->\n",
            "    <circle cx=\"140\" cy=\"100\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"100\" class=\"num\">5</text>\n",
            "\n",
            "    <!-- 6 Latissimus dorsi -->\n",
            "    <circle cx=\"188\" cy=\"150\" r=\"16\" class=\"marker\"/>\n",
            "    <text x=\"188\" y=\"150\" class=\"num\">6</text>\n",
            "\n",
            "    <!-- 8 Rhomboids / mid-traps -->\n",
            "    <circle cx=\"105\" cy=\"150\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"105\" y=\"150\" class=\"num\">8</text>\n",
            "\n",
            "    <!-- 9 Erector spinae (lower back) -->\n",
            "    <circle cx=\"140\" cy=\"200\" r=\"14\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"200\" class=\"num\">9</text>\n",
            "\n",
            "    <!-- 13 Transverse abdominis / deep core (back side reference) -->\n",
            "    <circle cx=\"140\" cy=\"230\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"230\" class=\"num\">13</text>\n",
            "\n",
            "    <!-- 12 Rectus abdominis (lower front/back reference near pelvis) -->\n",
            "    <circle cx=\"140\" cy=\"260\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"140\" y=\"260\" class=\"num\">12</text>\n",
            "\n",
            "    <!-- 14 Glutes -->\n",
            "    <circle cx=\"120\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"120\" y=\"300\" class=\"num\">14</text>\n",
            "\n",
            "    <!-- 15 Quadriceps (front leg reference on back view) -->\n",
            "    <circle cx=\"160\" cy=\"300\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"160\" y=\"300\" class=\"num\">15</text>\n",
            "\n",
            "    <!-- 16 Hamstrings/calves -->\n",
            "    <circle cx=\"175\" cy=\"350\" r=\"12\" class=\"marker\"/>\n",
            "    <text x=\"175\" y=\"350\" class=\"num\">16</text>\n",
            "  </g>\n",
            "\n",
            "  <!-- Legend box -->\n",
            "  <g transform=\"translate(50,520)\">\n",
            "    <rect x=\"0\" y=\"0\" width=\"1000\" height=\"820\" rx=\"8\" ry=\"8\" fill=\"#ffffff\" stroke=\"#ddd\" />\n",
            "    <text x=\"18\" y=\"30\" class=\"legend-title\">Legend — number : muscle group — example calisthenics exercises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"60\" class=\"legend\">1 — Pectoralis major (chest): push-ups (incline → standard → decline), ring/parallel dips</text>\n",
            "    <text x=\"18\" y=\"90\" class=\"legend\">2 — Anterior / medial deltoids (front/mid shoulder): pike push-ups, handstand push-up progressions</text>\n",
            "    <text x=\"18\" y=\"120\" class=\"legend\">3 — Lateral / posterior deltoids (side/rear shoulder): band face pulls, rear-delt rows, Y/T/W</text>\n",
            "    <text x=\"18\" y=\"150\" class=\"legend\">4 — Biceps: chin-ups (supinated), ring curls, slow negatives</text>\n",
            "    <text x=\"18\" y=\"180\" class=\"legend\">5 — Trapezius & upper back: scapular pull-ups, shrugs (loaded carries), face pulls</text>\n",
            "    <text x=\"18\" y=\"210\" class=\"legend\">6 — Latissimus dorsi (lats): pull-ups, wide grip pull-ups, weighted pull-ups, muscle-up work</text>\n",
            "    <text x=\"18\" y=\"240\" class=\"legend\">8 — Rhomboids / mid-traps: inverted rows, ring rows, horizontal pulling emphasizing scapular retraction</text>\n",
            "    <text x=\"18\" y=\"270\" class=\"legend\">9 — Erector spinae (lower back): single-leg RDL, back extensions, supermans</text>\n",
            "\n",
            "    <text x=\"18\" y=\"310\" class=\"legend\">10 — Serratus anterior (side ribcage): push-up plus, scapular protraction on rings</text>\n",
            "    <text x=\"18\" y=\"340\" class=\"legend\">11 — Obliques (side abs): side plank dips, hanging oblique raises, windshield wipers</text>\n",
            "    <text x=\"18\" y=\"370\" class=\"legend\">12 — Rectus abdominis (front abs): hollow body holds, planks, hanging leg raises</text>\n",
            "    <text x=\"18\" y=\"400\" class=\"legend\">13 — Transverse abdominis / deep core: Pallof press, bracing drills, dead-bug</text>\n",
            "\n",
            "    <text x=\"18\" y=\"440\" class=\"legend\">14 — Gluteus maximus (glutes): single-leg hip thrusts, glute bridges, deep squats</text>\n",
            "    <text x=\"18\" y=\"470\" class=\"legend\">15 — Quadriceps (quads): squats, Bulgarian split squats, pistol progressions</text>\n",
            "    <text x=\"18\" y=\"500\" class=\"legend\">16 — Hamstrings & calves: Nordic hamstring curls, single-leg RDL, calf raises</text>\n",
            "\n",
            "    <text x=\"18\" y=\"540\" class=\"legend-title\">Programming notes</text>\n",
            "    <text x=\"18\" y=\"570\" class=\"legend\">- Pair push and pull volume to avoid imbalance.</text>\n",
            "    <text x=\"18\" y=\"595\" class=\"legend\">- Train major groups 2–3×/week for hypertrophy. Mix heavy (3–6 reps) and moderate (6–12) sets.</text>\n",
            "    <text x=\"18\" y=\"620\" class=\"legend\">- Core work: mix anti-extension (plank/hollow), anti-rotation (Pallof/side-plank) and dynamic hanging leg raises.</text>\n",
            "    <text x=\"18\" y=\"645\" class=\"legend\">- Include scapular/shoulder prehab (band face pulls, scapular pull-ups) for healthy pressing and pulling.</text>\n",
            "\n",
            "    <text x=\"18\" y=\"700\" class=\"legend\">Tip: Save or print this SVG and use it while choosing exercises—tap the numbered targets to find suitable progressions.</text>\n",
            "  </g>\n",
            "\n",
            "  <text x=\"50\" y=\"1360\" class=\"panel\">If you want a different style (more anatomical accuracy, color-coded muscles, or a printable PDF), tell me which format and I’ll generate it.</text>\n",
            "</svg>\n",
            "\"\"\"\n",
            "\n",
            "def open_with_pywebview(html_content: str):\n",
            "    try:\n",
            "        import webview  # pywebview\n",
            "    except Exception as e:\n",
            "        raise RuntimeError(\"pywebview is not available\") from e\n",
            "\n",
            "    # Create a simple HTML wrapper so the browser knows it's SVG content and scales nicely\n",
            "    html_wrapper = f\"\"\"<!doctype html>\n",
            "<html>\n",
            "  <head>\n",
            "    <meta charset=\"utf-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "    <title>Calisthenics Muscle Map</title>\n",
            "    <style>html,body{{height:100%;margin:0;background:#f3f4f6}} svg{{display:block;margin:auto;max-width:100%;height:auto}}</style>\n",
            "  </head>\n",
            "  <body>\n",
            "    {html_content}\n",
            "  </body>\n",
            "</html>\"\"\"\n",
            "\n",
            "    # Create and show window\n",
            "    webview.create_window(\"Calisthenics Muscle Map\", html=html_wrapper, width=1000, height=800)\n",
            "    webview.start()\n",
            "\n",
            "def open_in_browser(svg_text: str):\n",
            "    # Write SVG to temporary file and open it in default browser\n",
            "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".svg\", prefix=\"calisthenics_\")\n",
            "    try:\n",
            "        tmp.write(svg_text.encode(\"utf-8\"))\n",
            "        tmp.flush()\n",
            "        tmp.close()\n",
            "        path = os.path.abspath(tmp.name)\n",
            "        url = \"file://\" + path\n",
            "        print(\"Opening SVG in default web browser:\", url)\n",
            "        webbrowser.open(url)\n",
            "    except Exception:\n",
            "        # Cleanup on error\n",
            "        try:\n",
            "            os.unlink(tmp.name)\n",
            "        except Exception:\n",
            "            pass\n",
            "        raise\n",
            "\n",
            "def main():\n",
            "    # Try pywebview first for an embedded window\n",
            "    try:\n",
            "        open_with_pywebview(SVG_CONTENT)\n",
            "    except Exception as e:\n",
            "        print(\"pywebview not available or failed to start:\", e)\n",
            "        print(\"Falling back to opening the SVG in your default web browser.\")\n",
            "        open_in_browser(SVG_CONTENT)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "Notes and troubleshooting\n",
            "- If you want the in-app window experience, install pywebview: pip install pywebview\n",
            "  - On some platforms pywebview may require additional runtime libraries (it uses a native web renderer).\n",
            "- If pywebview can't start you’ll still be able to view the SVG via your default browser (fallback).\n",
            "- If you prefer to open an existing SVG file instead of the embedded string, I can modify the script to accept a filename argument.\n",
            "\n",
            "Want me to add: clickable markers that open more info, or export to PNG/PDF from the script?\n",
            "Human: What is my age to the power of the number of letters in my name? Also when my browser pulls up the temporary svg file I'm getting this error. How to fix: This page contains the following errors:error on line 17 at column 70: xmlParseEntityRef: no name\n",
            "AI: Two quick answers:\n",
            "\n",
            "1) The math\n",
            "- Your age = 25, your name \"Sagun\" has 5 letters.  \n",
            "- 25^5 = 9,765,625.\n",
            "\n",
            "2) The SVG XML error and how to fix it\n",
            "- The browser error \"xmlParseEntityRef: no name\" almost always means there is a raw ampersand (&) somewhere inside text in the SVG. In XML (and SVG) you must escape ampersands as &amp; (and also escape < and > if they appear in text nodes).\n",
            "- In the SVG you pasted there is an unescaped \"&\" (for example in the title \"Front & Back\"). That produces the parsing error.\n",
            "\n",
            "Two simple fixes:\n",
            "A) Manual: open the .svg file in a text editor and replace any \"&\" used in text with \"&amp;\" (e.g., change \"Front & Back\" → \"Front &amp; Back\").\n",
            "\n",
            "B) Safer automated fix in Python: sanitize ampersands that are not already part of an entity before writing the temp file. Below is an updated version of the viewing script that performs that sanitization and then opens the SVG in your browser (or via pywebview if available).\n",
            "\n",
            "Save this as view_svg.py and run python view_svg.py:\n",
            "\n",
            "```python\n",
            "#!/usr/bin/env python3\n",
            "\"\"\"\n",
            "view_svg.py\n",
            "\n",
            "Open the embedded SVG. This script sanitizes raw '&' characters in text\n",
            "so the SVG is valid XML and will not trigger xmlParseEntityRef errors.\n",
            "\"\"\"\n",
            "import os\n",
            "import tempfile\n",
            "import webbrowser\n",
            "import re\n",
            "\n",
            "SVG_CONTENT = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<!-- paste your SVG here (exact content you received) -->\n",
            "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1100\" height=\"1400\" viewBox=\"0 0 1100 1400\">\n",
            "  <!-- ... rest of svg ... -->\n",
            "  <text>Calisthenics Muscle Map — Front & Back (numbered)</text>\n",
            "</svg>\n",
            "\"\"\"\n",
            "\n",
            "def sanitize_ampersands(svg_text: str) -> str:\n",
            "    # Replace '&' that are NOT already the start of a valid entity such as &amp;, &lt;, &gt;, &quot;, &apos;,\n",
            "    # numeric entities like &#1234;, or hex numeric entities like &#x1A3F;\n",
            "    return re.sub(r'&(?!amp;|lt;|gt;|quot;|apos;|#\\d+;|#x[0-9A-Fa-f]+;)', '&amp;', svg_text)\n",
            "\n",
            "def open_in_browser(svg_text: str):\n",
            "    # Sanitize to avoid xmlParseEntityRef error\n",
            "    clean_svg = sanitize_ampersands(svg_text)\n",
            "\n",
            "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".svg\", prefix=\"calisthenics_\")\n",
            "    try:\n",
            "        tmp.write(clean_svg.encode(\"utf-8\"))\n",
            "        tmp.flush()\n",
            "        tmp.close()\n",
            "        path = os.path.abspath(tmp.name)\n",
            "        url = \"file://\" + path\n",
            "        print(\"Opening SVG in default web browser:\", url)\n",
            "        webbrowser.open(url)\n",
            "    except Exception as e:\n",
            "        # Cleanup on error\n",
            "        try:\n",
            "            os.unlink(tmp.name)\n",
            "        except Exception:\n",
            "            pass\n",
            "        raise\n",
            "\n",
            "def main():\n",
            "    try:\n",
            "        # Try to use pywebview if installed (optional)\n",
            "        import webview\n",
            "        # Use sanitized html wrapper so pywebview can display it.\n",
            "        safe_svg = sanitize_ampersands(SVG_CONTENT)\n",
            "        html = f\"<!doctype html><html><meta charset='utf-8'><body>{safe_svg}</body></html>\"\n",
            "        webview.create_window(\"Calisthenics Muscle Map\", html=html, width=1000, height=800)\n",
            "        webview.start()\n",
            "    except Exception:\n",
            "        print(\"pywebview not available or failed; opening in default browser instead.\")\n",
            "        open_in_browser(SVG_CONTENT)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "Notes:\n",
            "- Replace the placeholder SVG_CONTENT with your actual SVG text (the full SVG provided earlier).\n",
            "- The sanitize_ampersands function uses a regex to only replace ampersands that are not already valid XML entities, avoiding double-escaping.\n",
            "- You can still manually correct the file by changing \"&\" to \"&amp;\" where appropriate (title, legend, etc.).\n",
            "\n",
            "If you want, paste the exact lines around the reported error (line 17 from the temp file) and I’ll point out the exact substring that needs replacing or produce a corrected SVG for you.\n"
          ]
        }
      ],
      "source": [
        "print(window_chat_map[\"example_with_summary_and_window_history\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the chat history above, you can see how the SystemMessage crafted encapsulated the jist of the conversation's earliest parts while leaving in the 16 most recent messages verbatim. This allowed the model to answer with memory persistence while keeping the total amount of tokens low."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 5953\n",
            "Summary memory conversation length: 1182\n",
            "Buffer window memory conversation length: 374\n",
            "Window and Summary memory conversation length: 17219\n"
          ]
        }
      ],
      "source": [
        "# Get window memory content\n",
        "window_and_summary__content = \"\\n\".join([msg.content for msg in window_chat_map[\"example_with_summary_and_window_history\"].messages])\n",
        "\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}\\n'\n",
        "    f'Buffer window memory conversation length: {len(tokenizer.encode(window_content))}'\n",
        "    f'\\nWindow and Summary memory conversation length: {len(tokenizer.encode(window_and_summary__content))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Window and Summary memory looks big because we haven't used the other ones for as long and with the same complexity of questions, but by the nature of its design, the space complexity of buffer memory will far exceed window and summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M4g--oayWvr"
      },
      "source": [
        "## What else can we do with memory?\n",
        "\n",
        "There are several cool things we can do with memory in langchain:\n",
        "* Implement our own custom memory modules (as we've done above)\n",
        "* Use multiple memory modules in the same chain\n",
        "* Combine agents with memory and other tools\n",
        "* Integrate knowledge graphs\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LangChain-Pinecone-io-thing (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
