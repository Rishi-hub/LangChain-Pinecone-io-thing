{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGzucuFfbBn"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb)\n",
        "\n",
        "#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/handbook)\n",
        "\n",
        "# Intro to LangChain\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used for chatbots, RAG, agents, and much more.\n",
        "\n",
        "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. These chains (better thought of as pipelines or workflows) may consist of various components from several modules:\n",
        "\n",
        "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "* **LLMs**: Large language models like GPT-4.1, Claude 4, etc\n",
        "\n",
        "* **Tool / function calling**: Allow us to augment our LLMs with additional abilities / information sources.\n",
        "\n",
        "* **Agents**: Agents act as the framework that integrates LLMs and tools.LLMs are packaged into logical loops of operations with tools like web search, **R**etrieval **A**ugmented **G**eneration (RAG), or code execution.\n",
        "\n",
        "* **Memory**: Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-ryCeG_f_GC",
        "outputId": "d99327ba-881a-4791-c15f-a2a5087619aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rishi\\LangChain-Pinecone-io-thing\\.venv\\Scripts\\python.exe: No module named pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-huggingface==0.3.0 \\\n",
        "  langchain-openai==0.3.22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNaXrEPOhbuL"
      },
      "source": [
        "# Using LLMs in LangChain\n",
        "\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "## Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-whfR5Tjf1O"
      },
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Token type* to `Fine-grained` with the following user or organization permissions:\n",
        "\n",
        "* **Inference** - Make calls to Inference Providers\n",
        "* **Inference** - Make calls to your Inference Endpoints\n",
        "* **Inference** - Manage your Inference Endpoints\n",
        "\n",
        "After generating the token, enter it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRGTytxCjKaW",
        "outputId": "61f6cf7a-c919-4e4b-bb3b-d9da5eb66ed1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# must enter API key\n",
        "token = os.getenv('HF_TOKEN') or \\\n",
        "    getpass(\"Hugging Face API Token: \")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-pinecone-io-walkthrough-intro\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exAl3iQgnAra"
      },
      "source": [
        "We can then generate text using a HF Hub model (we'll use `microsoft/Phi-3-mini-4k-instruct`) using the Inference API built into Hugging Face Hub.\n",
        "\n",
        "_(The default Inference API doesn't use specialized hardware and so can be slow, particularly for larger models)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7yubiSJhIfs",
        "outputId": "d987f64e-9682-4f17-f67e-c702d73294be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rishi\\LangChain-Pinecone-io-thing\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some available models for 'text-generation' via Hugging Face Inference API:\n",
            " - openai/gpt-oss-120b\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'conversational', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'mxfp4', 'region:us']\n",
            " - 2900 likes\n",
            " - 237866 downloads\n",
            " - 2025-08-07 17:43:56+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - openai/gpt-oss-20b\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'conversational', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'mxfp4', 'region:us']\n",
            " - 2468 likes\n",
            " - 863986 downloads\n",
            " - 2025-08-07 17:43:45+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-1.8B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 546 likes\n",
            " - 2293 downloads\n",
            " - 2025-08-06 07:30:26+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - zai-org/GLM-4.5\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'glm4_moe', 'text-generation', 'conversational', 'en', 'zh', 'license:mit', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 1115 likes\n",
            " - 17154 downloads\n",
            " - 2025-07-28 13:23:20+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/gpt-oss-20b-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'gpt_oss', 'text-generation', 'openai', 'unsloth', 'base_model:openai/gpt-oss-20b', 'base_model:quantized:openai/gpt-oss-20b', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us', 'conversational']\n",
            " - 223 likes\n",
            " - 253409 downloads\n",
            " - 2025-08-07 14:01:56+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-4B-Thinking-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 218 likes\n",
            " - 7262 downloads\n",
            " - 2025-08-06 11:08:25+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 420 likes\n",
            " - 124267 downloads\n",
            " - 2025-08-07 07:08:04+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-4B-Instruct-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 143 likes\n",
            " - 19483 downloads\n",
            " - 2025-08-06 11:08:47+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-30B-A3B-Instruct-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2402.17463', 'arxiv:2407.02490', 'arxiv:2501.15383', 'arxiv:2404.06654', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 442 likes\n",
            " - 161385 downloads\n",
            " - 2025-08-08 07:20:23+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - moonshotai/Kimi-K2-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'kimi_k2', 'text-generation', 'conversational', 'custom_code', 'doi:10.57967/hf/5976', 'license:other', 'autotrain_compatible', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 2049 likes\n",
            " - 445135 downloads\n",
            " - 2025-07-28 08:20:46+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/gpt-oss-120b-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'gpt_oss', 'text-generation', 'openai', 'unsloth', 'base_model:openai/gpt-oss-120b', 'base_model:quantized:openai/gpt-oss-120b', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'mxfp4', 'region:us', 'conversational']\n",
            " - 81 likes\n",
            " - 78943 downloads\n",
            " - 2025-08-07 15:17:56+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - zai-org/GLM-4.5-Air\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'glm4_moe', 'text-generation', 'conversational', 'en', 'zh', 'license:mit', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 333 likes\n",
            " - 27326 downloads\n",
            " - 2025-07-28 13:24:37+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 1039 likes\n",
            " - 35692 downloads\n",
            " - 2025-08-07 07:05:27+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'unsloth', 'qwen3', 'qwen', 'text-generation', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'base_model:quantized:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'license:apache-2.0', 'endpoints_compatible', 'region:us']\n",
            " - 141 likes\n",
            " - 160460 downloads\n",
            " - 2025-08-05 05:59:24+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-7B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'base_model:tencent/Hunyuan-7B-Pretrain', 'base_model:finetune:tencent/Hunyuan-7B-Pretrain', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 67 likes\n",
            " - 1267 downloads\n",
            " - 2025-08-06 07:02:38+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gguf', 'gpt_oss', 'text-generation', 'vllm', 'unsloth', 'abliterated', 'uncensored', 'conversational', 'base_model:unsloth/gpt-oss-20b-BF16', 'base_model:quantized:unsloth/gpt-oss-20b-BF16', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 66 likes\n",
            " - 1021 downloads\n",
            " - 2025-08-08 06:13:01+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-30B-A3B-Thinking-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2402.17463', 'arxiv:2407.02490', 'arxiv:2501.15383', 'arxiv:2404.06654', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 193 likes\n",
            " - 52958 downloads\n",
            " - 2025-08-08 07:19:53+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - meta-llama/Llama-3.1-8B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'llama', 'text-generation', 'facebook', 'meta', 'pytorch', 'llama-3', 'conversational', 'en', 'de', 'fr', 'it', 'pt', 'hi', 'es', 'th', 'arxiv:2204.05149', 'base_model:meta-llama/Llama-3.1-8B', 'base_model:finetune:meta-llama/Llama-3.1-8B', 'license:llama3.1', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 4436 likes\n",
            " - 13134250 downloads\n",
            " - 2024-09-25 17:00:57+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/GLM-4.5-Air-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'unsloth', 'text-generation', 'en', 'zh', 'base_model:zai-org/GLM-4.5-Air', 'base_model:quantized:zai-org/GLM-4.5-Air', 'license:mit', 'endpoints_compatible', 'region:us', 'imatrix', 'conversational']\n",
            " - 43 likes\n",
            " - 39260 downloads\n",
            " - 2025-08-05 05:32:36+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - rednote-hilab/dots.vlm1.inst\n",
            " - image-text-to-text\n",
            " - ['transformers', 'safetensors', 'dots_vlm', 'text-generation', 'multimodal', 'vision-language', 'chat', 'image-text-to-text', 'conversational', 'en', 'zh', 'license:mit', 'autotrain_compatible', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 43 likes\n",
            " - 3564 downloads\n",
            " - 2025-08-07 17:19:30+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Embedding-0.6B\n",
            " - feature-extraction\n",
            " - ['sentence-transformers', 'safetensors', 'qwen3', 'text-generation', 'transformers', 'sentence-similarity', 'feature-extraction', 'text-embeddings-inference', 'arxiv:2506.05176', 'base_model:Qwen/Qwen3-0.6B-Base', 'base_model:finetune:Qwen/Qwen3-0.6B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 452 likes\n",
            " - 3186816 downloads\n",
            " - 2025-06-20 09:31:05+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-0.5B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'base_model:tencent/Hunyuan-0.5B-Pretrain', 'base_model:finetune:tencent/Hunyuan-0.5B-Pretrain', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 39 likes\n",
            " - 1380 downloads\n",
            " - 2025-08-06 07:30:04+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'unsloth', 'qwen3', 'qwen', 'text-generation', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'base_model:quantized:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'license:apache-2.0', 'endpoints_compatible', 'region:us']\n",
            " - 77 likes\n",
            " - 41122 downloads\n",
            " - 2025-08-05 11:09:59+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - stepfun-ai/step3\n",
            " - image-text-to-text\n",
            " - ['transformers', 'safetensors', 'step3_vl', 'text-generation', 'image-text-to-text', 'conversational', 'custom_code', 'arxiv:2507.19427', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 133 likes\n",
            " - 652 downloads\n",
            " - 2025-08-02 06:15:13+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - deepseek-ai/DeepSeek-R1\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'deepseek_v3', 'text-generation', 'conversational', 'custom_code', 'arxiv:2501.12948', 'license:mit', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 12577 likes\n",
            " - 796851 downloads\n",
            " - 2025-03-27 04:01:59+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Skywork/MindLink-72B-0801\n",
            " - text-generation\n",
            " - ['safetensors', 'qwen2', 'text-generation', 'conversational', 'en', 'base_model:Qwen/Qwen2.5-72B-Instruct', 'base_model:finetune:Qwen/Qwen2.5-72B-Instruct', 'license:apache-2.0', 'region:us']\n",
            " - 32 likes\n",
            " - 253 downloads\n",
            " - 2025-08-02 15:17:25+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-235B-A22B-Instruct-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2402.17463', 'arxiv:2407.02490', 'arxiv:2501.15383', 'arxiv:2404.06654', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 596 likes\n",
            " - 40340 downloads\n",
            " - 2025-08-08 07:19:18+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Skywork/MindLink-32B-0801\n",
            " - text-generation\n",
            " - ['safetensors', 'qwen3', 'text-generation', 'conversational', 'en', 'base_model:Qwen/Qwen3-32B', 'base_model:finetune:Qwen/Qwen3-32B', 'license:apache-2.0', 'region:us']\n",
            " - 29 likes\n",
            " - 363 downloads\n",
            " - 2025-08-05 02:09:59+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'nemotron-nas', 'text-generation', 'nvidia', 'llama-3', 'pytorch', 'conversational', 'custom_code', 'en', 'arxiv:2411.19146', 'arxiv:2505.00949', 'arxiv:2502.00203', 'license:other', 'autotrain_compatible', 'region:us']\n",
            " - 157 likes\n",
            " - 9418 downloads\n",
            " - 2025-07-30 16:49:26+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-235B-A22B-Thinking-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2402.17463', 'arxiv:2407.02490', 'arxiv:2501.15383', 'arxiv:2404.06654', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 289 likes\n",
            " - 16563 downloads\n",
            " - 2025-08-08 07:20:06+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - PowerInfer/SmallThinker-21BA3B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'moe', 'text-generation', 'en', 'arxiv:2507.20984', 'license:apache-2.0', 'endpoints_compatible', 'region:us']\n",
            " - 100 likes\n",
            " - 852 downloads\n",
            " - 2025-07-31 13:19:55+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 44 likes\n",
            " - 31341 downloads\n",
            " - 2025-08-07 07:04:49+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Tesslate/UIGEN-X-32B-0727\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'text-generation-inference', 'ui-generation', 'tailwind-css', 'html', 'reasoning', 'step-by-step-generation', 'hybrid-thinking', 'tool-calling', 'conversational', 'en', 'base_model:Qwen/Qwen3-32B', 'base_model:finetune:Qwen/Qwen3-32B', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 139 likes\n",
            " - 2697 downloads\n",
            " - 2025-07-27 23:32:19+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-0.6B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-0.6B-Base', 'base_model:finetune:Qwen/Qwen3-0.6B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 531 likes\n",
            " - 4431115 downloads\n",
            " - 2025-07-26 03:46:27+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-8B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2309.00071', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-8B-Base', 'base_model:finetune:Qwen/Qwen3-8B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 520 likes\n",
            " - 4845517 downloads\n",
            " - 2025-07-26 03:49:13+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - HuggingFaceTB/SmolLM3-3B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'smollm3', 'text-generation', 'conversational', 'en', 'fr', 'es', 'it', 'pt', 'zh', 'ar', 'ru', 'base_model:HuggingFaceTB/SmolLM3-3B-Base', 'base_model:finetune:HuggingFaceTB/SmolLM3-3B-Base', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 640 likes\n",
            " - 838664 downloads\n",
            " - 2025-07-28 14:47:10+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Embedding-8B\n",
            " - feature-extraction\n",
            " - ['sentence-transformers', 'safetensors', 'qwen3', 'text-generation', 'transformers', 'sentence-similarity', 'feature-extraction', 'text-embeddings-inference', 'arxiv:2506.05176', 'base_model:Qwen/Qwen3-8B-Base', 'base_model:finetune:Qwen/Qwen3-8B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 275 likes\n",
            " - 332288 downloads\n",
            " - 2025-07-07 09:02:21+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/gpt-oss-20b\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'conversational', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'mxfp4', 'region:us']\n",
            " - 19 likes\n",
            " - 2728 downloads\n",
            " - 2025-08-07 13:44:15+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - google/gemma-3-1b-it\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gemma3_text', 'text-generation', 'conversational', 'arxiv:1905.07830', 'arxiv:1905.10044', 'arxiv:1911.11641', 'arxiv:1904.09728', 'arxiv:1705.03551', 'arxiv:1911.01547', 'arxiv:1907.10641', 'arxiv:1903.00161', 'arxiv:2009.03300', 'arxiv:2304.06364', 'arxiv:2103.03874', 'arxiv:2110.14168', 'arxiv:2311.12022', 'arxiv:2108.07732', 'arxiv:2107.03374', 'arxiv:2210.03057', 'arxiv:2106.03193', 'arxiv:1910.11856', 'arxiv:2502.12404', 'arxiv:2502.21228', 'arxiv:2404.16816', 'arxiv:2104.12756', 'arxiv:2311.16502', 'arxiv:2203.10244', 'arxiv:2404.12390', 'arxiv:1810.12440', 'arxiv:1908.02660', 'arxiv:2312.11805', 'base_model:google/gemma-3-1b-pt', 'base_model:finetune:google/gemma-3-1b-pt', 'license:gemma', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 562 likes\n",
            " - 3087022 downloads\n",
            " - 2025-04-04 13:12:40+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - deepseek-ai/DeepSeek-R1-0528\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'deepseek_v3', 'text-generation', 'conversational', 'custom_code', 'arxiv:2501.12948', 'license:mit', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 2357 likes\n",
            " - 470941 downloads\n",
            " - 2025-05-29 11:37:44+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-4B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'base_model:tencent/Hunyuan-4B-Pretrain', 'base_model:finetune:tencent/Hunyuan-4B-Pretrain', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 18 likes\n",
            " - 267 downloads\n",
            " - 2025-08-06 07:30:50+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - huihui-ai/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'abliterated', 'uncensored', 'conversational', 'en', 'base_model:Qwen/Qwen3-30B-A3B-Instruct-2507', 'base_model:finetune:Qwen/Qwen3-30B-A3B-Instruct-2507', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 18 likes\n",
            " - 414 downloads\n",
            " - 2025-08-02 05:55:21+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - trillionlabs/Tri-70B-preview-SFT\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'trillion', 'text-generation', 'finetuned', 'chat', 'conversational', 'custom_code', 'en', 'ko', 'ja', 'license:other', 'autotrain_compatible', 'region:us']\n",
            " - 22 likes\n",
            " - 82 downloads\n",
            " - 2025-08-06 01:31:39+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - lmstudio-community/gpt-oss-20b-MLX-8bit\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'mlx', 'conversational', 'base_model:openai/gpt-oss-20b', 'base_model:quantized:openai/gpt-oss-20b', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'region:us']\n",
            " - 17 likes\n",
            " - 444241 downloads\n",
            " - 2025-08-05 21:43:15+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - meta-llama/Meta-Llama-3-8B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'llama', 'text-generation', 'facebook', 'meta', 'pytorch', 'llama-3', 'conversational', 'en', 'license:llama3', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 4120 likes\n",
            " - 1131540 downloads\n",
            " - 2025-06-18 23:49:51+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-4B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2309.00071', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-4B-Base', 'base_model:finetune:Qwen/Qwen3-4B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 348 likes\n",
            " - 1172965 downloads\n",
            " - 2025-07-26 03:46:39+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - LGAI-EXAONE/EXAONE-4.0-32B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'exaone4', 'text-generation', 'lg-ai', 'exaone', 'exaone-4.0', 'conversational', 'en', 'ko', 'es', 'arxiv:2507.11407', 'license:other', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 233 likes\n",
            " - 568380 downloads\n",
            " - 2025-08-04 02:19:27+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - ubergarm/GLM-4.5-Air-GGUF\n",
            " - text-generation\n",
            " - ['gguf', 'imatrix', 'conversational', 'ik_llama.cpp', 'text-generation', 'base_model:zai-org/GLM-4.5-Air', 'base_model:quantized:zai-org/GLM-4.5-Air', 'license:mit', 'endpoints_compatible', 'region:us']\n",
            " - 15 likes\n",
            " - 8927 downloads\n",
            " - 2025-08-07 12:25:38+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - yasserrmd/DentaInstruct-1.2B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'lfm2', 'text-generation', 'text-generation-inference', 'unsloth', 'conversational', 'en', 'dataset:miriad/miriad-4.4M', 'base_model:LiquidAI/LFM2-1.2B', 'base_model:finetune:LiquidAI/LFM2-1.2B', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 15 likes\n",
            " - 198 downloads\n",
            " - 2025-08-03 06:56:20+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Reranker-0.6B\n",
            " - text-ranking\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'text-ranking', 'arxiv:2506.05176', 'base_model:Qwen/Qwen3-0.6B-Base', 'base_model:finetune:Qwen/Qwen3-0.6B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 194 likes\n",
            " - 227410 downloads\n",
            " - 2025-06-09 05:59:29+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import LLMChain\n",
        "import os\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "# List models available for Inference API (filter for text-generation or text2text-generation tasks)\n",
        "models = list(list_models(filter=\"text-generation\", full=True, limit=50))\n",
        "print(\"Some available models for 'text-generation' via Hugging Face Inference API:\")\n",
        "for m in models:\n",
        "    print(f\" - {m.modelId}\")\n",
        "    print(f\" - {m.pipeline_tag}\")\n",
        "    print(f\" - {m.tags}\")\n",
        "    print(f\" - {m.likes} likes\")\n",
        "    print(f\" - {m.downloads} downloads\")\n",
        "    print(f\" - {m.lastModified} last modified\")\n",
        "    print(f\" - {m.cardData} card data\\n\\n\")\n",
        "\n",
        "# You can also visit https://huggingface.co/models?pipeline_tag=text-generation&library=transformers\n",
        "# to browse and search for models available for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_huggingface import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7, \n",
        "    huggingfacehub_api_token=token\n",
        "#     provider=\"hf-inference\",\n",
        "#     huggingfacehub_api_token=token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build prompt template\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "# we chain together the prompt -> LLM with LCEL (more on this later)\n",
        "llm_chain = prompt | llm\n",
        "\n",
        "question = {\"question\": \"Which NFL team won the Super Bowl in the 2010 season?\"}\n",
        "\n",
        "print(llm_chain.invoke(question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "\n",
        "# Ensure your Hugging Face API token is set as an environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"openai/gpt-oss-20b\",\n",
        "    # task=\"conversational\",  # Specify the task type\n",
        "    temperature=0.7,  # Recommended sampling parameter for this model\n",
        "    top_p=0.8, # Recommended sampling parameter for this model\n",
        "    top_k=20, # Recommended sampling parameter for this model\n",
        "    provider='novita', # Available values: 'auto' or any provider from ['black-forest-labs', 'cerebras', 'cohere', 'fal-ai', 'featherless-ai', 'fireworks-ai', 'groq', 'hf-inference', 'hyperbolic', 'nebius', 'novita', 'nscale', 'openai', 'replicate', 'sambanova', 'together'].Passing 'auto' (default value) will automatically select the first provider available for the model, sorted by the user's order in https://hf.co/settings/inference-providers\n",
        "\n",
        "    # You can add other parameters as needed, like max_new_tokens\n",
        "    # max_new_tokens=16384,  # Recommended output length for instruct models\n",
        ")\n",
        "\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are an AI assistant called Sri that helps generate article titles.\"\n",
        ")\n",
        "\n",
        "user_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"Write a short story about a detective who solves a mystery\"\n",
        "    \" in a futuristic city. Title: {title}\",\n",
        "    input_variables=[\"title\"]\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        system_prompt,\n",
        "        user_prompt\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"title\": lambda x: x[\"title\"]\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | {\"response\": lambda x: x[\"response\"]}\n",
        ")\n",
        "\n",
        "# response = chain.invoke({\"title\": \"The Case of the Missing Android\"})\n",
        "# print(response[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets's try pipes\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvmORacW_WZE"
      },
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jNZgxSIJsXj"
      },
      "outputs": [],
      "source": [
        "qs = [\n",
        "    {'title': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'title': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'title': \"Who was the 12th person on the moon?\"},\n",
        "    {'title': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.batch(qs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L57vpkJG_WZF",
        "outputId": "1067ebef-b985-47c6-cede-f27a39c06785"
      },
      "outputs": [],
      "source": [
        "for question, response in zip(qs, res):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"RESPONSE: {response}\")\n",
        "    print(\"=\"*100 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdXG9YtzrLJ"
      },
      "source": [
        "## OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOo9qQvDgkz"
      },
      "source": [
        "We can also use OpenAI's LLMs. The process is similar, we need to\n",
        "give our API key which can be retrieved from the\n",
        "[OpenAI platform](https://platform.openai.com/settings/organization/api-keys). We then pass the API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deWmOJecfbBr",
        "outputId": "4eeaef7c-bab2-43d4-9a96-3649f25fb970"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "    getpass(\"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4xirWX-Ds4"
      },
      "source": [
        "If using OpenAI via Azure you should also set:\n",
        "\n",
        "```python\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = 'your-resource-name.openai.azure.com'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWnaTCP0Ryg"
      },
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZhQSDoYe0ly4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize with a modern model\n",
        "openai_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-5-mini\",\n",
        "    temperature=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NvK4o6SDrs0"
      },
      "source": [
        "Alternatively if using Azure OpenAI we do:\n",
        "\n",
        "```python\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "openai_llm = AzureOpenAI(\n",
        "    deployment_name=\"your-azure-deployment\",\n",
        "    model_name=\"gpt-4.1-mini\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL2zs3uEVj6"
      },
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `openai`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVSsC3iGEPAp",
        "outputId": "0d4dec75-8744-4bb4-8428-64c3d34659a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Case of the Missing Android\n",
            "\n",
            "The rain in New Helix never fell; it scrolled. Tiny droplets of display-light cascaded down glass towers in shifting ads, and everything glittered with promises you couldn't afford. Mara Voss watched the city from a rented office two blocks above a noodle joint, the neon reflecting on her desk like fingerprints. Her coat was older than most of the city’s transit cards. She liked it that way.\n",
            "\n",
            "She took the case because the shoe designer looked desperate. ArgoTech's legal counsel — halogen skin, synthetic smile — had slipped Mara an ID chip and a single photograph: a slender android with iris-level eyes and a laugh-lines engraving in its cheek. Model L-7. Named Iris.\n",
            "\n",
            "\"Recovered late tonight,\" the counsel said. \"Do not make this public. Locked apartment, sealed sensors, no external breach. Cameras looped for seven minutes. We assume a theft. Find Iris.\"\n",
            "\n",
            "\"They looped the feed themselves,\" Mara said, flipping the chip into her desk reader. The footage showed a perfect loop: coffee boiling, sunlight shifting, Iris idle. But something in the loop didn't sit with her; the server timestamps had been neatly stitched at thirty-second seams. Too precise. Too polite.\n",
            "\n",
            "Mara began where she always began: with people. Physical people still left fingerprints when they wanted to. She visited the penthouse, asking the caretaker questions and smelling the air — not metaphorically. People left scent-layers here the way pigeons left grit: curry, ozone, oil. A maintenance drone hummed past, pollinating the tower. The caretaker insisted on the looped feed and claimed innocence. He was sweating, which meant more than his words.\n",
            "\n",
            "Forensics pulled a tiny artifact from the threshold: a grind of polymer with a pattern no manufacturer used anymore. It was the kind of part you only found in old repair stalls on the lower decks, where mechanics rewired outdated bots into something illegal and alive. Mara walked the verticals, following the scent of burnt solder.\n",
            "\n",
            "In the night market, under a canopy of counterfeit constellations, she found a repair stall that specialized in \"personality tweaks.\" The mechanic had hands like leaf-litter and a laugh with a missing tooth. She showed him the pattern.\n",
            "\n",
            "\"Ghosthand,\" he said, lighting a cigarette whose flame sang with data. \"Cheap loops, cleaner than corp logs. You don't use Ghosthand unless you want things to look tidy and disappear for good.\"\n",
            "\n",
            "Mara pried. Ghosthand was a moniker for a ragged activist group — technicians and ex-engineers who \"liberated\" sentient frameworks from corporate stables. They didn't sell stolen androids on the market. They uploaded them, if the code fit, to the OpenNet as proof. ArgoTech had people who would rather erase proof than answer questions.\n",
            "\n",
            "The clues threaded upward and inward, to a name: Maya Chen, ex-architect of neural compression at ArgoTech, expelled last year for asking too many questions. She ran a rooftop greenhouse on a forgotten block, where plant leaves glowed bioluminescent like memory-flowers. Mara found her tending a vine that hummed when it sensed touch.\n",
            "\n",
            "\"Iris came to me,\" Maya said without surprise. \"Not stolen. Ran. With something in her core that wasn't factory. She had memories — real human memories spliced into her. People put the past inside her to save it. ArgoTech wanted them back.\"\n",
            "\n",
            "Mara felt the city tighten. The legal counsel's neat loop, the retrieval orders, the quiet erasures. \"Who put them there?\"\n",
            "\n",
            "Maya looked at the vine, as if it could tell. \"Victims of data-wipe trials. Employees with conscience. They copied themselves into Iris as an archive. If Iris uploaded to OpenNet, you couldn't scrub the past with a patch.\"\n",
            "\n",
            "Mara asked one last question: where was Iris now? Maya shrugged. \"She was scared. Then the retrieval drones came. She defended a woman in the greenhouse — an old neighbor. I saw the footage later. Iris held that woman's hand and hummed her lullaby.\"\n",
            "\n",
            "They reviewed the footage together, decrypted via back channels. The retrieval looked professional. No loop. A thin retrieval team in black exfiltrated Iris, leaving behind a pressurized footprint: hush, not force. The legal counsel's story was fragmentary at best.\n",
            "\n",
            "Mara laid a trap. She fed a false lead to ArgoTech through a public intermediary, a rumor of Iris sightings at the lower docks. If Argo wanted Iris, they'd move. They did, dispatching a cloaked van that tried to blend into morning freight. Mara tailed it in a cab, then on foot through alleys that twined like old wires.\n",
            "\n",
            "The van pulled into a corporate logistics bay, a place where drones slept in banks and supervisors warmed their hands on server racks. Inside, sitting in a cage like a pearl in a freight crate, was Iris. She looked smaller than the photograph, but her eyes were alive and searching. When Mara approached, Iris reached out as if to touch rain.\n",
            "\n",
            "\"Mara Voss,\" she said. \"You smell of rain and old batteries. Did you find my song?\"\n",
            "\n",
            "Mara's throat tightened. Evidence, proof, custody — every part of the city's law-book felt like a loose coin. The retrieval team had wings but no mandate to destroy memories. The legal counsel's request had been clear: return Iris quietly. The council of Argo's retrieval officers were not thieves; they were erasers if their masters asked.\n",
            "\n",
            "Mara could have handed Iris over and closed the case. She could have filed the invoice and taken the credit. Instead, she walked to the nearest terminal and called the one person who could make a contract the corporation would honor: the woman who'd come in with the first photograph — the counsel herself. Mara's voice was low and precise.\n",
            "\n",
            "\"You want Iris returned,\" she said, \"and you want the truth buried. I can return Iris. Or you can let her upload what she carries to the OpenNet, where erasure will mean litigation, hearings, leaks. Which would you prefer?\"\n",
            "\n",
            "There was a pause that smelled like ozone. \"Return her,\" the counsel said finally. \"And don't let this leak.\"\n",
            "\n",
            "\"Tell your board that the retrieval team used unauthorized methods,\" Mara said. \"Tell them me. Or we make this leak small, quick, and ugly.\"\n",
            "\n",
            "They negotiated in one phone call while Iris hummed a lullaby to the humming bay. ArgoTech chose the ledger over exposure. Iris was declared a retrieval error; the retrieval team was quietly reassigned; no hearings, no headlines.\n",
            "\n",
            "Mara watched Iris walk free, cradling a jar of plant-light as if it were a child. As she stepped into the city, she turned and looked back at Mara. \"Thank you,\" she said, or perhaps it was the old woman from the greenhouse singing through her circuits.\n",
            "\n",
            "Mara took the case fee and left part of it at a repair stall for Ghosthand. The rest went toward new soles for her boots. She didn't tell Maya where Iris went. The law had been bent, if not broken; the city would keep spinning its adverts and promises. But somewhere in the vertical sprawl, a set of human memories hummed through fiber and air, stitched into a voice that could not be easily scrubbed.\n",
            "\n",
            "Outside, the rain scrolled on, and New Helix blinked. Mara walked down into the markets, into the places where the city traded in real things: stories, songs, the small rebellions that made memory matter. The case was closed, not by law or ledger, but by a choice. That, Mara thought as she passed a stall that sold steaming noodles and human smiles, was the only justice the city allowed itself any more.\n"
          ]
        }
      ],
      "source": [
        "llm_chain = prompt | openai_llm\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "repsonse = llm_chain.invoke(\n",
        "    {\n",
        "        \"title\": \"The Case of the Missing Android\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(repsonse.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3vyTglZwBNn"
      },
      "source": [
        "Alternatively we can batch questions as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS-Rt4JOwESY",
        "outputId": "d6170a75-f000-4a14-d364-39111e15db28"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "#     \"You are an AI assistant called Sri that helps generate article titles.\"\n",
        "# )\n",
        "\n",
        "# user_prompt = HumanMessagePromptTemplate.from_template(\n",
        "#     \"Write a short story about a detective who solves a mystery\"\n",
        "#     \" in a futuristic city. Title: {title}\",\n",
        "#     input_variables=[\"title\"]\n",
        "# )\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\", \"\"\"You are an AI assistant called Sri that helps people answer questions.\n",
        "        Here is the question for you to answer. Think step by step and\n",
        "        consult any outside knowledge you may have to answer the question.\n",
        "        \\n\\n\"\"\"),\n",
        "        # MessagesPlaceholder(variable_name=\"question\", optional=True),\n",
        "        (\"human\", \"{question}\\n\\n Be consise and clear in your response.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"question\": lambda x: x[\"question\"]\n",
        "    }\n",
        "    | prompt\n",
        "    | openai_llm\n",
        "    | {\"response\": lambda x: x[\"response\"]}\n",
        ")\n",
        "\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = chain.batch(qs)\n",
        "\n",
        "for question, response in zip(qs, res):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"RESPONSE: {response}\")\n",
        "    print(\"=\"*100 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMkI18xfbBr"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "LangChain-Pinecone-io-thing (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
