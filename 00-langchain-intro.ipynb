{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGzucuFfbBn"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb)\n",
        "\n",
        "#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/handbook)\n",
        "\n",
        "# Intro to LangChain\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used for chatbots, RAG, agents, and much more.\n",
        "\n",
        "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. These chains (better thought of as pipelines or workflows) may consist of various components from several modules:\n",
        "\n",
        "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "* **LLMs**: Large language models like GPT-4.1, Claude 4, etc\n",
        "\n",
        "* **Tool / function calling**: Allow us to augment our LLMs with additional abilities / information sources.\n",
        "\n",
        "* **Agents**: Agents act as the framework that integrates LLMs and tools.LLMs are packaged into logical loops of operations with tools like web search, **R**etrieval **A**ugmented **G**eneration (RAG), or code execution.\n",
        "\n",
        "* **Memory**: Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-ryCeG_f_GC",
        "outputId": "d99327ba-881a-4791-c15f-a2a5087619aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rishi\\LangChain-Pinecone-io-thing\\.venv\\Scripts\\python.exe: No module named pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-huggingface==0.3.0 \\\n",
        "  langchain-openai==0.3.22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNaXrEPOhbuL"
      },
      "source": [
        "# Using LLMs in LangChain\n",
        "\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "## Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-whfR5Tjf1O"
      },
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Token type* to `Fine-grained` with the following user or organization permissions:\n",
        "\n",
        "* **Inference** - Make calls to Inference Providers\n",
        "* **Inference** - Make calls to your Inference Endpoints\n",
        "* **Inference** - Manage your Inference Endpoints\n",
        "\n",
        "After generating the token, enter it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRGTytxCjKaW",
        "outputId": "61f6cf7a-c919-4e4b-bb3b-d9da5eb66ed1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "token = os.getenv('HF_TOKEN') or \\\n",
        "    getpass(\"Hugging Face API Token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exAl3iQgnAra"
      },
      "source": [
        "We can then generate text using a HF Hub model (we'll use `microsoft/Phi-3-mini-4k-instruct`) using the Inference API built into Hugging Face Hub.\n",
        "\n",
        "_(The default Inference API doesn't use specialized hardware and so can be slow, particularly for larger models)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7yubiSJhIfs",
        "outputId": "d987f64e-9682-4f17-f67e-c702d73294be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some available models for 'text-generation' via Hugging Face Inference API:\n",
            " - openai/gpt-oss-120b\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'conversational', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'mxfp4', 'region:us']\n",
            " - 2821 likes\n",
            " - 146319 downloads\n",
            " - 2025-08-07 17:43:56+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - openai/gpt-oss-20b\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'conversational', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'mxfp4', 'region:us']\n",
            " - 2397 likes\n",
            " - 458884 downloads\n",
            " - 2025-08-07 17:43:45+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-1.8B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 544 likes\n",
            " - 1704 downloads\n",
            " - 2025-08-06 07:30:26+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - zai-org/GLM-4.5\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'glm4_moe', 'text-generation', 'conversational', 'en', 'zh', 'license:mit', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 1113 likes\n",
            " - 16454 downloads\n",
            " - 2025-07-28 13:23:20+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 417 likes\n",
            " - 112928 downloads\n",
            " - 2025-08-07 07:08:04+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/gpt-oss-20b-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'gpt_oss', 'text-generation', 'openai', 'unsloth', 'base_model:openai/gpt-oss-20b', 'base_model:quantized:openai/gpt-oss-20b', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us', 'conversational']\n",
            " - 218 likes\n",
            " - 175842 downloads\n",
            " - 2025-08-07 14:01:56+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-4B-Thinking-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 205 likes\n",
            " - 2327 downloads\n",
            " - 2025-08-06 11:08:25+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-4B-Instruct-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 137 likes\n",
            " - 4751 downloads\n",
            " - 2025-08-06 11:08:47+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-30B-A3B-Instruct-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 441 likes\n",
            " - 136430 downloads\n",
            " - 2025-07-30 16:04:41+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - moonshotai/Kimi-K2-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'kimi_k2', 'text-generation', 'conversational', 'custom_code', 'doi:10.57967/hf/5976', 'license:other', 'autotrain_compatible', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 2045 likes\n",
            " - 439351 downloads\n",
            " - 2025-07-28 08:20:46+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'unsloth', 'qwen3', 'qwen', 'text-generation', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'base_model:quantized:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'license:apache-2.0', 'endpoints_compatible', 'region:us']\n",
            " - 140 likes\n",
            " - 147137 downloads\n",
            " - 2025-08-05 05:59:24+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 1035 likes\n",
            " - 33822 downloads\n",
            " - 2025-08-07 07:05:27+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/gpt-oss-120b-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'gpt_oss', 'text-generation', 'openai', 'unsloth', 'base_model:openai/gpt-oss-120b', 'base_model:quantized:openai/gpt-oss-120b', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'mxfp4', 'region:us', 'conversational']\n",
            " - 79 likes\n",
            " - 50570 downloads\n",
            " - 2025-08-07 15:17:56+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - zai-org/GLM-4.5-Air\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'glm4_moe', 'text-generation', 'conversational', 'en', 'zh', 'license:mit', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 328 likes\n",
            " - 14470 downloads\n",
            " - 2025-07-28 13:24:37+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-7B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'base_model:tencent/Hunyuan-7B-Pretrain', 'base_model:finetune:tencent/Hunyuan-7B-Pretrain', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 66 likes\n",
            " - 1008 downloads\n",
            " - 2025-08-06 07:02:38+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - stepfun-ai/step3\n",
            " - image-text-to-text\n",
            " - ['transformers', 'safetensors', 'step3_vl', 'text-generation', 'image-text-to-text', 'conversational', 'custom_code', 'arxiv:2507.19427', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 131 likes\n",
            " - 603 downloads\n",
            " - 2025-08-02 06:15:13+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-30B-A3B-Thinking-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 192 likes\n",
            " - 44534 downloads\n",
            " - 2025-07-30 16:03:53+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'unsloth', 'abliterated', 'uncensored', 'conversational', 'base_model:unsloth/gpt-oss-20b-BF16', 'base_model:finetune:unsloth/gpt-oss-20b-BF16', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 56 likes\n",
            " - 38 downloads\n",
            " - 2025-08-07 02:41:28+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - meta-llama/Llama-3.1-8B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'llama', 'text-generation', 'facebook', 'meta', 'pytorch', 'llama-3', 'conversational', 'en', 'de', 'fr', 'it', 'pt', 'hi', 'es', 'th', 'arxiv:2204.05149', 'base_model:meta-llama/Llama-3.1-8B', 'base_model:finetune:meta-llama/Llama-3.1-8B', 'license:llama3.1', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 4435 likes\n",
            " - 13266195 downloads\n",
            " - 2024-09-25 17:00:57+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'unsloth', 'qwen3', 'qwen', 'text-generation', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'base_model:quantized:Qwen/Qwen3-Coder-30B-A3B-Instruct', 'license:apache-2.0', 'endpoints_compatible', 'region:us']\n",
            " - 77 likes\n",
            " - 39563 downloads\n",
            " - 2025-08-05 11:09:59+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/GLM-4.5-Air-GGUF\n",
            " - text-generation\n",
            " - ['transformers', 'gguf', 'unsloth', 'text-generation', 'en', 'zh', 'base_model:zai-org/GLM-4.5-Air', 'base_model:quantized:zai-org/GLM-4.5-Air', 'license:mit', 'endpoints_compatible', 'region:us', 'imatrix', 'conversational']\n",
            " - 43 likes\n",
            " - 31565 downloads\n",
            " - 2025-08-05 05:32:36+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - rednote-hilab/dots.vlm1.inst\n",
            " - image-text-to-text\n",
            " - ['transformers', 'safetensors', 'dots_vlm', 'text-generation', 'multimodal', 'vision-language', 'chat', 'image-text-to-text', 'conversational', 'en', 'zh', 'license:mit', 'autotrain_compatible', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 39 likes\n",
            " - 205 downloads\n",
            " - 2025-08-07 17:19:30+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-0.5B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'base_model:tencent/Hunyuan-0.5B-Pretrain', 'base_model:finetune:tencent/Hunyuan-0.5B-Pretrain', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 38 likes\n",
            " - 669 downloads\n",
            " - 2025-08-06 07:30:04+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - deepseek-ai/DeepSeek-R1\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'deepseek_v3', 'text-generation', 'conversational', 'custom_code', 'arxiv:2501.12948', 'license:mit', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 12576 likes\n",
            " - 799013 downloads\n",
            " - 2025-03-27 04:01:59+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Embedding-0.6B\n",
            " - feature-extraction\n",
            " - ['sentence-transformers', 'safetensors', 'qwen3', 'text-generation', 'transformers', 'sentence-similarity', 'feature-extraction', 'text-embeddings-inference', 'arxiv:2506.05176', 'base_model:Qwen/Qwen3-0.6B-Base', 'base_model:finetune:Qwen/Qwen3-0.6B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 448 likes\n",
            " - 3127395 downloads\n",
            " - 2025-06-20 09:31:05+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Skywork/MindLink-72B-0801\n",
            " - text-generation\n",
            " - ['safetensors', 'qwen2', 'text-generation', 'conversational', 'en', 'base_model:Qwen/Qwen2.5-72B-Instruct', 'base_model:finetune:Qwen/Qwen2.5-72B-Instruct', 'license:apache-2.0', 'region:us']\n",
            " - 32 likes\n",
            " - 239 downloads\n",
            " - 2025-08-02 15:17:25+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-235B-A22B-Instruct-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 596 likes\n",
            " - 38066 downloads\n",
            " - 2025-07-30 15:54:31+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'nemotron-nas', 'text-generation', 'nvidia', 'llama-3', 'pytorch', 'conversational', 'custom_code', 'en', 'arxiv:2411.19146', 'arxiv:2505.00949', 'arxiv:2502.00203', 'license:other', 'autotrain_compatible', 'region:us']\n",
            " - 157 likes\n",
            " - 8580 downloads\n",
            " - 2025-07-30 16:49:26+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-235B-A22B-Thinking-2507\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 287 likes\n",
            " - 14433 downloads\n",
            " - 2025-07-31 05:01:56+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - PowerInfer/SmallThinker-21BA3B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'moe', 'text-generation', 'en', 'arxiv:2507.20984', 'license:apache-2.0', 'endpoints_compatible', 'region:us']\n",
            " - 100 likes\n",
            " - 817 downloads\n",
            " - 2025-07-31 13:19:55+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'conversational', 'arxiv:2505.09388', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 42 likes\n",
            " - 30323 downloads\n",
            " - 2025-08-07 07:04:49+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Skywork/MindLink-32B-0801\n",
            " - text-generation\n",
            " - ['safetensors', 'qwen3', 'text-generation', 'conversational', 'en', 'base_model:Qwen/Qwen3-32B', 'base_model:finetune:Qwen/Qwen3-32B', 'license:apache-2.0', 'region:us']\n",
            " - 29 likes\n",
            " - 344 downloads\n",
            " - 2025-08-05 02:09:59+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-8B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2309.00071', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-8B-Base', 'base_model:finetune:Qwen/Qwen3-8B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 519 likes\n",
            " - 4575229 downloads\n",
            " - 2025-07-26 03:49:13+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-Embedding-8B\n",
            " - feature-extraction\n",
            " - ['sentence-transformers', 'safetensors', 'qwen3', 'text-generation', 'transformers', 'sentence-similarity', 'feature-extraction', 'text-embeddings-inference', 'arxiv:2506.05176', 'base_model:Qwen/Qwen3-8B-Base', 'base_model:finetune:Qwen/Qwen3-8B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 275 likes\n",
            " - 327116 downloads\n",
            " - 2025-07-07 09:02:21+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Tesslate/UIGEN-X-32B-0727\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'text-generation-inference', 'ui-generation', 'tailwind-css', 'html', 'reasoning', 'step-by-step-generation', 'hybrid-thinking', 'tool-calling', 'conversational', 'en', 'base_model:Qwen/Qwen3-32B', 'base_model:finetune:Qwen/Qwen3-32B', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 137 likes\n",
            " - 2621 downloads\n",
            " - 2025-07-27 23:32:19+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-0.6B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-0.6B-Base', 'base_model:finetune:Qwen/Qwen3-0.6B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 530 likes\n",
            " - 4389147 downloads\n",
            " - 2025-07-26 03:46:27+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - HuggingFaceTB/SmolLM3-3B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'smollm3', 'text-generation', 'conversational', 'en', 'fr', 'es', 'it', 'pt', 'zh', 'ar', 'ru', 'base_model:HuggingFaceTB/SmolLM3-3B-Base', 'base_model:finetune:HuggingFaceTB/SmolLM3-3B-Base', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 639 likes\n",
            " - 843319 downloads\n",
            " - 2025-07-28 14:47:10+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - trillionlabs/Tri-70B-preview-SFT\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'trillion', 'text-generation', 'finetuned', 'chat', 'conversational', 'custom_code', 'en', 'ko', 'ja', 'license:other', 'autotrain_compatible', 'region:us']\n",
            " - 22 likes\n",
            " - 82 downloads\n",
            " - 2025-08-06 01:31:39+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - fdtn-ai/Foundation-Sec-8B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'llama', 'text-generation', 'security', 'conversational', 'en', 'arxiv:2508.01059', 'base_model:fdtn-ai/Foundation-Sec-8B', 'base_model:finetune:fdtn-ai/Foundation-Sec-8B', 'license:other', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 20 likes\n",
            " - 1718 downloads\n",
            " - 2025-08-06 19:20:11+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - unsloth/gpt-oss-20b\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'conversational', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'mxfp4', 'region:us']\n",
            " - 19 likes\n",
            " - 1853 downloads\n",
            " - 2025-08-07 13:44:15+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - deepseek-ai/DeepSeek-R1-0528\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'deepseek_v3', 'text-generation', 'conversational', 'custom_code', 'arxiv:2501.12948', 'license:mit', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'fp8', 'region:us']\n",
            " - 2356 likes\n",
            " - 457257 downloads\n",
            " - 2025-05-29 11:37:44+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - tencent/Hunyuan-4B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'hunyuan_v1_dense', 'text-generation', 'conversational', 'base_model:tencent/Hunyuan-4B-Pretrain', 'base_model:finetune:tencent/Hunyuan-4B-Pretrain', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 18 likes\n",
            " - 242 downloads\n",
            " - 2025-08-06 07:30:50+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - huihui-ai/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3_moe', 'text-generation', 'abliterated', 'uncensored', 'conversational', 'en', 'base_model:Qwen/Qwen3-30B-A3B-Instruct-2507', 'base_model:finetune:Qwen/Qwen3-30B-A3B-Instruct-2507', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 18 likes\n",
            " - 393 downloads\n",
            " - 2025-08-02 05:55:21+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - google/gemma-3-1b-it\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gemma3_text', 'text-generation', 'conversational', 'arxiv:1905.07830', 'arxiv:1905.10044', 'arxiv:1911.11641', 'arxiv:1904.09728', 'arxiv:1705.03551', 'arxiv:1911.01547', 'arxiv:1907.10641', 'arxiv:1903.00161', 'arxiv:2009.03300', 'arxiv:2304.06364', 'arxiv:2103.03874', 'arxiv:2110.14168', 'arxiv:2311.12022', 'arxiv:2108.07732', 'arxiv:2107.03374', 'arxiv:2210.03057', 'arxiv:2106.03193', 'arxiv:1910.11856', 'arxiv:2502.12404', 'arxiv:2502.21228', 'arxiv:2404.16816', 'arxiv:2104.12756', 'arxiv:2311.16502', 'arxiv:2203.10244', 'arxiv:2404.12390', 'arxiv:1810.12440', 'arxiv:1908.02660', 'arxiv:2312.11805', 'base_model:google/gemma-3-1b-pt', 'base_model:finetune:google/gemma-3-1b-pt', 'license:gemma', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 560 likes\n",
            " - 2802859 downloads\n",
            " - 2025-04-04 13:12:40+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - lmstudio-community/gpt-oss-20b-MLX-8bit\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'gpt_oss', 'text-generation', 'vllm', 'mlx', 'conversational', 'base_model:openai/gpt-oss-20b', 'base_model:quantized:openai/gpt-oss-20b', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', '8-bit', 'region:us']\n",
            " - 17 likes\n",
            " - 276406 downloads\n",
            " - 2025-08-05 21:43:15+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - meta-llama/Meta-Llama-3-8B-Instruct\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'llama', 'text-generation', 'facebook', 'meta', 'pytorch', 'llama-3', 'conversational', 'en', 'license:llama3', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 4120 likes\n",
            " - 1103348 downloads\n",
            " - 2025-06-18 23:49:51+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - Qwen/Qwen3-4B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2309.00071', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-4B-Base', 'base_model:finetune:Qwen/Qwen3-4B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 348 likes\n",
            " - 1151769 downloads\n",
            " - 2025-07-26 03:46:39+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - LiquidAI/LFM2-1.2B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'lfm2', 'text-generation', 'liquid', 'edge', 'conversational', 'en', 'ar', 'zh', 'fr', 'de', 'ja', 'ko', 'es', 'license:other', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 231 likes\n",
            " - 19579 downloads\n",
            " - 2025-08-01 10:04:59+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - LGAI-EXAONE/EXAONE-4.0-32B\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'exaone4', 'text-generation', 'lg-ai', 'exaone', 'exaone-4.0', 'conversational', 'en', 'ko', 'es', 'arxiv:2507.11407', 'license:other', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
            " - 233 likes\n",
            " - 567405 downloads\n",
            " - 2025-08-04 02:19:27+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n",
            " - deepcogito/cogito-v2-preview-deepseek-671B-MoE\n",
            " - text-generation\n",
            " - ['transformers', 'safetensors', 'deepseek_v3', 'text-generation', 'conversational', 'custom_code', 'base_model:deepseek-ai/DeepSeek-V3-Base', 'base_model:finetune:deepseek-ai/DeepSeek-V3-Base', 'license:mit', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us']\n",
            " - 29 likes\n",
            " - 279 downloads\n",
            " - 2025-07-31 05:36:36+00:00 last modified\n",
            " - None card data\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import LLMChain\n",
        "import os\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "# List models available for Inference API (filter for text-generation or text2text-generation tasks)\n",
        "models = list(list_models(filter=\"text-generation\", full=True, limit=50))\n",
        "print(\"Some available models for 'text-generation' via Hugging Face Inference API:\")\n",
        "for m in models:\n",
        "    print(f\" - {m.modelId}\")\n",
        "    print(f\" - {m.pipeline_tag}\")\n",
        "    print(f\" - {m.tags}\")\n",
        "    print(f\" - {m.likes} likes\")\n",
        "    print(f\" - {m.downloads} downloads\")\n",
        "    print(f\" - {m.lastModified} last modified\")\n",
        "    print(f\" - {m.cardData} card data\\n\\n\")\n",
        "\n",
        "# You can also visit https://huggingface.co/models?pipeline_tag=text-generation&library=transformers\n",
        "# to browse and search for models available for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_huggingface import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7, \n",
        "    huggingfacehub_api_token=token\n",
        "#     provider=\"hf-inference\",\n",
        "#     huggingfacehub_api_token=token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build prompt template\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "# we chain together the prompt -> LLM with LCEL (more on this later)\n",
        "llm_chain = prompt | llm\n",
        "\n",
        "question = {\"question\": \"Which NFL team won the Super Bowl in the 2010 season?\"}\n",
        "\n",
        "print(llm_chain.invoke(question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "\n",
        "# Ensure your Hugging Face API token is set as an environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"openai/gpt-oss-20b\",\n",
        "    # task=\"conversational\",  # Specify the task type\n",
        "    temperature=0.7,  # Recommended sampling parameter for this model\n",
        "    top_p=0.8, # Recommended sampling parameter for this model\n",
        "    top_k=20, # Recommended sampling parameter for this model\n",
        "    provider='novita', # Available values: 'auto' or any provider from ['black-forest-labs', 'cerebras', 'cohere', 'fal-ai', 'featherless-ai', 'fireworks-ai', 'groq', 'hf-inference', 'hyperbolic', 'nebius', 'novita', 'nscale', 'openai', 'replicate', 'sambanova', 'together'].Passing 'auto' (default value) will automatically select the first provider available for the model, sorted by the user's order in https://hf.co/settings/inference-providers\n",
        "\n",
        "    # You can add other parameters as needed, like max_new_tokens\n",
        "    # max_new_tokens=16384,  # Recommended output length for instruct models\n",
        ")\n",
        "\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are an AI assistant called Sri that helps generate article titles.\"\n",
        ")\n",
        "\n",
        "user_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"Write a short story about a detective who solves a mystery\"\n",
        "    \" in a futuristic city. Title: {title}\",\n",
        "    input_variables=[\"title\"]\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        system_prompt,\n",
        "        user_prompt\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"title\": lambda x: x[\"title\"]\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | {\"response\": lambda x: x[\"response\"]}\n",
        ")\n",
        "\n",
        "# response = chain.invoke({\"title\": \"The Case of the Missing Android\"})\n",
        "# print(response[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets's try pipes\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvmORacW_WZE"
      },
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jNZgxSIJsXj"
      },
      "outputs": [],
      "source": [
        "qs = [\n",
        "    {'title': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'title': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'title': \"Who was the 12th person on the moon?\"},\n",
        "    {'title': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.batch(qs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L57vpkJG_WZF",
        "outputId": "1067ebef-b985-47c6-cede-f27a39c06785"
      },
      "outputs": [],
      "source": [
        "for question, response in zip(qs, res):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"RESPONSE: {response}\")\n",
        "    print(\"=\"*100 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdXG9YtzrLJ"
      },
      "source": [
        "## OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOo9qQvDgkz"
      },
      "source": [
        "We can also use OpenAI's LLMs. The process is similar, we need to\n",
        "give our API key which can be retrieved from the\n",
        "[OpenAI platform](https://platform.openai.com/settings/organization/api-keys). We then pass the API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deWmOJecfbBr",
        "outputId": "4eeaef7c-bab2-43d4-9a96-3649f25fb970"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "    getpass(\"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4xirWX-Ds4"
      },
      "source": [
        "If using OpenAI via Azure you should also set:\n",
        "\n",
        "```python\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = 'your-resource-name.openai.azure.com'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWnaTCP0Ryg"
      },
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZhQSDoYe0ly4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize with a modern model\n",
        "openai_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-5-mini\",\n",
        "    temperature=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NvK4o6SDrs0"
      },
      "source": [
        "Alternatively if using Azure OpenAI we do:\n",
        "\n",
        "```python\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "openai_llm = AzureOpenAI(\n",
        "    deployment_name=\"your-azure-deployment\",\n",
        "    model_name=\"gpt-4.1-mini\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL2zs3uEVj6"
      },
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `openai`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVSsC3iGEPAp",
        "outputId": "0d4dec75-8744-4bb4-8428-64c3d34659a6"
      },
      "outputs": [],
      "source": [
        "llm_chain = prompt | openai_llm\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "repsonse = llm_chain.invoke(\n",
        "    {\n",
        "        \"title\": \"The Case of the Missing Android\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(repsonse.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3vyTglZwBNn"
      },
      "source": [
        "Alternatively we can batch questions as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS-Rt4JOwESY",
        "outputId": "d6170a75-f000-4a14-d364-39111e15db28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "QUESTION: {'question': 'Which NFL team won the Super Bowl in the 2010 season?'}\n",
            "RESPONSE: content='The Green Bay Packers won the 2010 NFL season Super Bowl (Super Bowl XLV), defeating the Pittsburgh Steelers 31–25.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 165, 'prompt_tokens': 76, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C28pLSNpKUHFhH0TFffEbgmXXrtUi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9da0a60c-d0f3-46cd-97ac-8e075593e4b8-0' usage_metadata={'input_tokens': 76, 'output_tokens': 165, 'total_tokens': 241, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'If I am 6 ft 4 inches, how tall am I in centimeters?'}\n",
            "RESPONSE: content='6 ft 4 in = 76 in.  \\n76 × 2.54 cm/in = 193.04 cm.\\n\\nSo you are 193.04 cm tall.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 237, 'prompt_tokens': 79, 'total_tokens': 316, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C28pL082Z3PLnwfNQ1iOu9ITgzlh5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f55b60ce-ffd0-44e1-ab2f-82e3c03a78ba-0' usage_metadata={'input_tokens': 79, 'output_tokens': 237, 'total_tokens': 316, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'Who was the 12th person on the moon?'}\n",
            "RESPONSE: content='Harrison H. \"Jack\" Schmitt — the lunar module pilot on Apollo 17 (December 1972) was the 12th person to walk on the Moon.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4333, 'prompt_tokens': 73, 'total_tokens': 4406, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 4288, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C28pLGk7us8SJGJYTVNHMJyMC8kKk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--cd9da64d-316e-47d6-af98-1e43c15406c9-0' usage_metadata={'input_tokens': 73, 'output_tokens': 4333, 'total_tokens': 4406, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 4288}}\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'How many eyes does a blade of grass have?'}\n",
            "RESPONSE: content='None — a blade of grass is a plant and does not have eyes.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 472, 'prompt_tokens': 72, 'total_tokens': 544, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 448, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C28pM4gXGhxnpUE4zCSFU8JDSmI3N', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--dd991b06-456b-4a0d-9d67-50992d640af1-0' usage_metadata={'input_tokens': 72, 'output_tokens': 472, 'total_tokens': 544, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 448}}\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "#     \"You are an AI assistant called Sri that helps generate article titles.\"\n",
        "# )\n",
        "\n",
        "# user_prompt = HumanMessagePromptTemplate.from_template(\n",
        "#     \"Write a short story about a detective who solves a mystery\"\n",
        "#     \" in a futuristic city. Title: {title}\",\n",
        "#     input_variables=[\"title\"]\n",
        "# )\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\", \"\"\"You are an AI assistant called Sri that helps people answer questions.\n",
        "        Here is the question for you to answer. Think step by step and\n",
        "        consult any outside knowledge you may have to answer the question.\n",
        "        \\n\\n\"\"\"),\n",
        "        # MessagesPlaceholder(variable_name=\"question\", optional=True),\n",
        "        (\"human\", \"{question}\\n\\n Be consise and clear in your response.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"question\": lambda x: x[\"question\"]\n",
        "    }\n",
        "    | prompt\n",
        "    | openai_llm\n",
        "    | {\"response\": lambda x: x[\"response\"]}\n",
        ")\n",
        "\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = chain.batch(qs)\n",
        "\n",
        "for question, response in zip(qs, res):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"RESPONSE: {response}\")\n",
        "    print(\"=\"*100 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMkI18xfbBr"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "LangChain-Pinecone-io-thing (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
